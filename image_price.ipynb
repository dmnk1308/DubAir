{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "import os \n",
    "from helpers_image_classification import *\n",
    "from load_data import *\n",
    "import cv2\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 10000)\n",
    "from load_data import *\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import sklearn.metrics\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "from PIL import ImageStat, Image\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Prices and Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load paths and labels\n",
    "img_folder = \"data/images_resized\"\n",
    "img_df = pd.read_csv(\"data/img_paths.csv\")\n",
    "#img_df = img_df[img_df[\"label\"] == 0]\n",
    "img_df.reset_index(drop=True, inplace = True)\n",
    "\n",
    "# load label book\n",
    "label_cat = [\"bathroom\", \"bedroom\", \"dining\", \"hallway\", \"kitchen\", \"living\", \"others\"]\n",
    "label = np.arange(7)\n",
    "label_book = pd.DataFrame({\"label\": label_cat, \"categorical_label\": label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prices\n",
    "url_listing = \"http://data.insideairbnb.com/ireland/leinster/dublin/2021-11-07/data/listings.csv.gz\"\n",
    "listings = pd.read_csv(url_listing)\n",
    "urls = listings[\"listing_url\"]\n",
    "ids = listings[\"id\"]\n",
    "price = listings[\"price\"]\n",
    "price = price.str.replace(\"$\",\"\")\n",
    "price = price.str.replace(\",\",\"\")\n",
    "price = price.astype(float)\n",
    "listings[\"price\"] = price\n",
    "listings[\"log_price\"] = np.log(price)\n",
    "listings = listings[listings[\"price\"]<500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>id</th>\n",
       "      <th>img_no</th>\n",
       "      <th>label</th>\n",
       "      <th>log_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44077_0.png</td>\n",
       "      <td>44077</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.174387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44077_1.png</td>\n",
       "      <td>44077</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.174387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44077_2.png</td>\n",
       "      <td>44077</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.174387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44077_3.png</td>\n",
       "      <td>44077</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.174387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44077_4.png</td>\n",
       "      <td>44077</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.174387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      img_path     id  img_no  label  log_price\n",
       "0  44077_0.png  44077       0    4.0   4.174387\n",
       "1  44077_1.png  44077       1    5.0   4.174387\n",
       "2  44077_2.png  44077       2    1.0   4.174387\n",
       "3  44077_3.png  44077       3    1.0   4.174387\n",
       "4  44077_4.png  44077       4    1.0   4.174387"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save relevant columns\n",
    "price_df = listings[[\"log_price\", \"id\"]]\n",
    "# merge image and price data together to get the corresponding price for each image\n",
    "df = pd.merge(img_df, price_df, on = \"id\", how = \"left\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56704, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter = np.any(df.isna(), axis = 1)\n",
    "df = df[~filter]\n",
    "\n",
    "# drop \"others\" as we are only interested in the rooms\n",
    "filter = df[\"label\"] == 6.0\n",
    "df = df[~filter]\n",
    "\n",
    "# use only listings with images for 4 or categories \n",
    "bool = []\n",
    "for id in df[\"id\"]:\n",
    "    tmp = df[df[\"id\"] == id]\n",
    "    if len(np.unique(tmp[\"label\"])) >= 4:\n",
    "        bool.append(True)\n",
    "    else:\n",
    "        bool.append(False)\n",
    "df_new = df[bool]\n",
    "np.unique(df_new[\"id\"].values).shape\n",
    "df = df_new\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(df):\n",
    "    ''' Extracts the features from a Resnet50 '''\n",
    "    counter_overall_dummy = 0\n",
    "    counter_overall_img = 0\n",
    "\n",
    "    def input_pipeline(room = 0, df = df):\n",
    "        ids = []\n",
    "        features = []\n",
    "        \n",
    "        # FILTER DF\n",
    "        df_room = df[df[\"label\"] == room]\n",
    "        \n",
    "        # RESNET\n",
    "        resnet = tf.keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', pooling=\"avg\", input_shape = (None,None,3))\n",
    "        resnet_pre = keras.applications.resnet50.preprocess_input\n",
    "        resnet.trainable = False\n",
    "        counter_dummy = 0\n",
    "        counter_img = 0\n",
    "        for id in tqdm_notebook(np.unique(df[\"id\"])):\n",
    "            filter = df_room[\"id\"] == id  \n",
    "\n",
    "            try:\n",
    "                l = []\n",
    "                if filter.sum() == 0:\n",
    "                    dummy_image = np.zeros((1,256,256,3))\n",
    "                    dummy_image = resnet_pre(dummy_image)\n",
    "                    dummy_image = resnet(dummy_image)\n",
    "                    l.append(dummy_image)\n",
    "                    counter_dummy += 1                \n",
    "                else:\n",
    "                    path_id = df_room[\"img_path\"][filter]\n",
    "                    for p in path_id:\n",
    "                        img_tmp = plt.imread(\"data/images_resized/\"+p)\n",
    "                        img_tmp = np.expand_dims(img_tmp, axis = 0)\n",
    "                        img_tmp = resnet_pre(img_tmp)\n",
    "                        img_tmp = resnet(img_tmp)\n",
    "                        l.append(img_tmp)\n",
    "                        counter_img += 1\n",
    "                l = np.stack(l)\n",
    "                l = np.max(l, axis = 0)\n",
    "                features.append(l)\n",
    "                ids.append(id)\n",
    "            except:\n",
    "                dummy_image = np.zeros((1,256,256,3))\n",
    "                dummy_image = resnet_pre(dummy_image)\n",
    "                dummy_image = resnet(dummy_image)\n",
    "                l.append(dummy_image)\n",
    "                l = np.stack(l)\n",
    "                l = np.max(l, axis = 0)\n",
    "                features.append(l)\n",
    "                ids.append(id)\n",
    "                continue\n",
    "        features = np.squeeze(np.stack(features))\n",
    "        filter = np.nonzero(features.sum(axis = 0))[0]\n",
    "        features = features[:,filter]\n",
    "        print(len(filter), \" features are nonzero.\")\n",
    "        features = features.tolist()\n",
    "        print(counter_dummy, \"dummy images were added.\")\n",
    "        return features, ids, counter_dummy, counter_img\n",
    "\n",
    "    basis_df = df[[\"id\",\"log_price\"]]\n",
    "    basis_df = basis_df.drop_duplicates()\n",
    "    \n",
    "    features = []\n",
    "    ids = []\n",
    "    \n",
    "    for i in tqdm_notebook(np.unique(df[\"label\"])):\n",
    "        feat_cat, ids_cat, counter_dummy, counter_img = input_pipeline(i,df)\n",
    "        df_tmp = pd.DataFrame({\"features_\"+str(i): feat_cat, \"id\": ids_cat})\n",
    "        basis_df = pd.merge(basis_df, df_tmp, on = \"id\", how = \"left\")\n",
    "        counter_overall_dummy += counter_dummy\n",
    "        counter_overall_img += counter_img\n",
    "    print(counter_overall_dummy)\n",
    "    print(counter_overall_img)\n",
    "    return basis_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee88fbeb2274ae88b89edc42c4838d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 14:03:03.366285: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-02-22 14:03:03.366398: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51a96ab9fb84557ab0ad425ccdf9a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1416  features are nonzero.\n",
      "266 dummy images were added.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efcd3ba12ff84c65af0fc81b912737e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1560  features are nonzero.\n",
      "42 dummy images were added.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f02133f7394f30bb6d9530436e4608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302  features are nonzero.\n",
      "2316 dummy images were added.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2f44736ff18423786c5246a46a5613f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1415  features are nonzero.\n",
      "2428 dummy images were added.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86181c81211b478aa8939b90b55af0a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1488  features are nonzero.\n",
      "189 dummy images were added.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f0ffe6ff1949a881f66bc1a29b0b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1422  features are nonzero.\n",
      "342 dummy images were added.\n",
      "5583\n",
      "56704\n"
     ]
    }
   ],
   "source": [
    "final_df = data_generator(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input of model a tuples\n",
    "def transform(ds):\n",
    "    bath = tf.unstack(ds[\"bath\"])\n",
    "    bed = tf.unstack(ds[\"bed\"])\n",
    "    dining = tf.unstack(ds[\"dining\"])\n",
    "    hall = tf.unstack(ds[\"hall\"])\n",
    "    kitchen = tf.unstack(ds[\"kitchen\"])\n",
    "    living = tf.unstack(ds[\"living\"])\n",
    "   # others = tf.unstack(ds[\"others\"])\n",
    "\n",
    "    prices = ds[\"price\"]\n",
    "    \n",
    "    y = prices\n",
    "    return (bath, bed, dining, hall, kitchen, living), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_val, y_train, y_test, y_val = load_data(drop_id = False)\n",
    "\n",
    "train_ids = X_train[\"id\"]\n",
    "test_ids = X_test[\"id\"]\n",
    "val_ids = X_val[\"id\"]\n",
    "\n",
    "X_train = pd.merge(train_ids, final_df, on = \"id\", how = \"left\")\n",
    "X_train = X_train.drop([\"id\",\"log_price\"], axis = 1)\n",
    "X_train.columns = [\"bath\", \"bed\", \"dining\", \"hall\", \"kitchen\", \"living\"]\n",
    "filter = np.any(X_train.isna(), axis = 1)\n",
    "X_train = X_train[~filter]\n",
    "y_train = y_train[~filter]\n",
    "\n",
    "X_test = pd.merge(test_ids, final_df, on = \"id\", how = \"left\")\n",
    "X_test = X_test.drop([\"id\",\"log_price\"], axis = 1)\n",
    "X_test.columns = [\"bath\", \"bed\", \"dining\", \"hall\", \"kitchen\", \"living\"]\n",
    "filter = np.any(X_test.isna(), axis = 1)\n",
    "X_test = X_test[~filter]\n",
    "y_test = y_test[~filter]\n",
    "\n",
    "X_val = pd.merge(val_ids, final_df, on = \"id\", how = \"left\")\n",
    "X_val = X_val.drop([\"id\",\"log_price\"], axis = 1)\n",
    "X_val.columns = [\"bath\", \"bed\", \"dining\", \"hall\", \"kitchen\", \"living\"]\n",
    "filter = np.any(X_val.isna(), axis = 1)\n",
    "X_val = X_val[~filter]\n",
    "y_val = y_val[~filter]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35785"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count how many training images we got\n",
    "train_id_final = pd.merge(train_ids, final_df, on = \"id\", how = \"left\")[\"id\"]\n",
    "counter = 0\n",
    "for i in train_id_final:\n",
    "    counter += (df[\"id\"] == i).sum()\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define weight constraint that ensures incoming weights for a layer to be greater than 0 and sum up top 1 \n",
    "from tensorflow.keras import backend\n",
    "class weight_constr(tf.keras.constraints.Constraint):\n",
    "  ''' Constrains weight tensors to sum up to 1 and being greater than 0 '''\n",
    "\n",
    "  def __init__(self):\n",
    "    self.ref_value = 1\n",
    "\n",
    "  def __call__(self, w):\n",
    "    nonneg = w * tf.cast(tf.greater_equal(w, 0.), backend.floatx())\n",
    "    sum_w = tf.reduce_sum(nonneg)\n",
    "    nonneg_one = nonneg/sum_w\n",
    "    return nonneg_one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup model\n",
    "class Img_model(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, dropout = 0, l2 = 0, nodes1 = 512, nodes2 = 1):\n",
    "    super().__init__()\n",
    "    \n",
    "    # bathroom\n",
    "    self.bn1_bath = tf.keras.layers.BatchNormalization()\n",
    "    self.drop1_bath = tf.keras.layers.Dropout(dropout)\n",
    "    self.dense1_bath = tf.keras.layers.Dense(nodes1, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.bn2_bath = tf.keras.layers.BatchNormalization()\n",
    "    self.drop2_bath = tf.keras.layers.Dropout(dropout)\n",
    "    #self.dense2_bath = tf.keras.layers.Dense(nodes2, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.dense2_bath = tf.keras.layers.Dense(nodes2, kernel_regularizer = keras.regularizers.l2(l2))\n",
    " \n",
    "    # bedroom\n",
    "    self.bn1_bed = tf.keras.layers.BatchNormalization()\n",
    "    self.drop1_bed = tf.keras.layers.Dropout(dropout)\n",
    "    self.dense1_bed = tf.keras.layers.Dense(nodes1, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.bn2_bed = tf.keras.layers.BatchNormalization()\n",
    "    self.drop2_bed = tf.keras.layers.Dropout(dropout)\n",
    "    #self.dense2_bed = tf.keras.layers.Dense(nodes2, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.dense2_bed = tf.keras.layers.Dense(nodes2, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "       \n",
    "      # dining\n",
    "    self.bn1_dining = tf.keras.layers.BatchNormalization()\n",
    "    self.drop1_dining = tf.keras.layers.Dropout(dropout)\n",
    "    self.dense1_dining = tf.keras.layers.Dense(nodes1, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.bn2_dining = tf.keras.layers.BatchNormalization()\n",
    "    self.drop2_dining = tf.keras.layers.Dropout(dropout)\n",
    "    #self.dense2_dining = tf.keras.layers.Dense(nodes2, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.dense2_dining = tf.keras.layers.Dense(nodes2, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "   \n",
    "    # hall\n",
    "    self.bn1_hall = tf.keras.layers.BatchNormalization()\n",
    "    self.drop1_hall = tf.keras.layers.Dropout(dropout)\n",
    "    self.dense1_hall = tf.keras.layers.Dense(nodes1, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.bn2_hall = tf.keras.layers.BatchNormalization()\n",
    "    self.drop2_hall = tf.keras.layers.Dropout(dropout)\n",
    "    #self.dense2_hall = tf.keras.layers.Dense(nodes2, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.dense2_hall = tf.keras.layers.Dense(nodes2, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    \n",
    "      # kitchen\n",
    "    self.bn1_kitchen = tf.keras.layers.BatchNormalization()\n",
    "    self.drop1_kitchen = tf.keras.layers.Dropout(dropout)\n",
    "    self.dense1_kitchen = tf.keras.layers.Dense(nodes1, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.bn2_kitchen = tf.keras.layers.BatchNormalization()\n",
    "    self.drop2_kitchen = tf.keras.layers.Dropout(dropout)\n",
    "    #self.dense2_kitchen = tf.keras.layers.Dense(nodes2, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.dense2_kitchen = tf.keras.layers.Dense(nodes2, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "  \n",
    "      # livingroom\n",
    "    self.bn1_living = tf.keras.layers.BatchNormalization()\n",
    "    self.drop1_living = tf.keras.layers.Dropout(dropout)\n",
    "    self.dense1_living = tf.keras.layers.Dense(nodes1, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.bn2_living = tf.keras.layers.BatchNormalization()\n",
    "    self.drop2_living = tf.keras.layers.Dropout(dropout)\n",
    "    #self.dense2_living = tf.keras.layers.Dense(nodes2, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.dense2_living = tf.keras.layers.Dense(nodes2, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    \n",
    "      # others\n",
    "    self.bn1_others = tf.keras.layers.BatchNormalization()\n",
    "    self.drop1_others = tf.keras.layers.Dropout(dropout)\n",
    "    self.dense1_others = tf.keras.layers.Dense(nodes1, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.bn2_others = tf.keras.layers.BatchNormalization()\n",
    "    self.drop2_others = tf.keras.layers.Dropout(dropout)\n",
    "    #self.dense2_others = tf.keras.layers.Dense(nodes2, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.dense2_others = tf.keras.layers.Dense(nodes2, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "\n",
    "    # final prediction\n",
    "    self.bn_final = tf.keras.layers.BatchNormalization()\n",
    "    self.drop_final = tf.keras.layers.Dropout(dropout)\n",
    "    self.dense_final = tf.keras.layers.Dense(1, kernel_constraint =weight_constr())\n",
    "    \n",
    "  def call(self, inputs, training = None):\n",
    "    \n",
    "    # bathroom\n",
    "    x = self.bn1_bath(inputs[0])\n",
    "    x = self.drop1_bath(x)\n",
    "    x = self.dense1_bath(x)\n",
    "    x = self.bn2_bath(x)\n",
    "    x = self.drop2_bath(x)\n",
    "    bath_out = self.dense2_bath(x)\n",
    "    \n",
    "    \n",
    "    # bedroom\n",
    "    x = self.bn1_bed(inputs[1])\n",
    "    x = self.drop1_bed(x)\n",
    "    x = self.dense1_bed(x)\n",
    "    x = self.bn2_bed(x)\n",
    "    x = self.drop2_bed(x)\n",
    "    bed_out = self.dense2_bed(x)\n",
    "    \n",
    "    # diningroom\n",
    "    x = self.bn1_dining(inputs[2])\n",
    "    x = self.drop1_dining(x)\n",
    "    x = self.dense1_dining(x)\n",
    "    x = self.bn2_dining(x)\n",
    "    x = self.drop2_dining(x)\n",
    "    dining_out = self.dense2_dining(x)\n",
    "    \n",
    "    # hallroom\n",
    "    x = self.bn1_hall(inputs[3])\n",
    "    x = self.drop1_hall(x)\n",
    "    x = self.dense1_hall(x)\n",
    "    x = self.bn2_hall(x)\n",
    "    x = self.drop2_hall(x)\n",
    "    hall_out = self.dense2_hall(x)\n",
    "    \n",
    "    # kitchen\n",
    "    x = self.bn1_kitchen(inputs[4])\n",
    "    x = self.drop1_kitchen(x)\n",
    "    x = self.dense1_kitchen(x)\n",
    "    x = self.bn2_kitchen(x)\n",
    "    x = self.drop2_kitchen(x)\n",
    "    kitchen_out = self.dense2_kitchen(x)\n",
    "    \n",
    "    # livingroom\n",
    "    x = self.bn1_living(inputs[5])\n",
    "    x = self.drop1_living(x)\n",
    "    x = self.dense1_living(x)\n",
    "    x = self.bn2_living(x)\n",
    "    x = self.drop2_living(x)\n",
    "    living_out = self.dense2_living(x)\n",
    "\n",
    "    # join\n",
    "    out = tf.keras.layers.concatenate([bath_out, bed_out, dining_out, hall_out, kitchen_out, living_out])#, others_out])\n",
    "    out = self.bn_final(out)\n",
    "    out = self.drop_final(out)\n",
    "    return self.dense_final(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define r^2 metric\n",
    "def R_squared(y, y_pred):\n",
    "  residual = tf.reduce_sum(tf.square(tf.subtract(y, y_pred)))\n",
    "  total = tf.reduce_sum(tf.square(tf.subtract(y, tf.reduce_mean(y))))\n",
    "  r2 = tf.subtract(1.0, tf.math.divide(residual, total))\n",
    "  \n",
    "  return r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X_train_init, X_test_init, X_val_init, y_train_init, y_test_init, y_val_init = load_data(drop_id = False)\n",
    "\n",
    "# join train and validation data as we only need the indicies\n",
    "X = pd.concat((X_train_init,X_val_init)).reset_index(drop = True)\n",
    "y = pd.concat((y_train_init, y_val_init)).reset_index(drop=True)\n",
    "\n",
    "# define metrics to track\n",
    "para_do = []\n",
    "para_l2 = []\n",
    "para_nodes = []\n",
    "best_val_mse = []\n",
    "best_val_rsq = []\n",
    "best_train_mse = []\n",
    "best_train_rsq = []\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "for do in tqdm_notebook([0,0.2,0.5], desc = \"Dropout\"): # best 0.2, 0.001, 64 \n",
    "    for l2 in tqdm_notebook([0,0.01,0.001], desc = \"L2-Reg\"):\n",
    "        for nodes in tqdm_notebook([64, 512], desc = \"Nodes\"):\n",
    "            best_val_mse_tmp = []\n",
    "            best_val_rsq_tmp = []     \n",
    "            best_train_mse_tmp = []\n",
    "            best_train_rsq_tmp = []    \n",
    "            for train_idx, val_idx in tqdm_notebook(kf.split(X), total = 5):\n",
    "                \n",
    "                # data processing\n",
    "                X_train = X.loc[train_idx,:]\n",
    "                X_val = X.loc[val_idx,:]\n",
    "                X_test = X_test_init\n",
    "\n",
    "                y_train = y[train_idx]\n",
    "                y_val = y[val_idx]\n",
    "                y_test = y_test_init\n",
    "\n",
    "                train_ids = X_train[\"id\"]\n",
    "                test_ids = X_test_init[\"id\"]\n",
    "                val_ids = X_val[\"id\"]\n",
    "\n",
    "                X_train = pd.merge(train_ids, final_df, on = \"id\", how = \"left\")\n",
    "                X_train = X_train.drop([\"id\",\"log_price\"], axis = 1)\n",
    "                X_train.columns = [\"bath\", \"bed\", \"dining\", \"hall\", \"kitchen\", \"living\"]#, \"others\"]\n",
    "                filter = np.any(X_train.isna(), axis = 1).values\n",
    "                X_train = X_train[~filter]\n",
    "                y_train = y_train[~filter]\n",
    "\n",
    "                X_test = pd.merge(test_ids, final_df, on = \"id\", how = \"left\")\n",
    "                X_test = X_test.drop([\"id\",\"log_price\"], axis = 1)\n",
    "                X_test.columns = [\"bath\", \"bed\", \"dining\", \"hall\", \"kitchen\", \"living\"]#, \"others\"]\n",
    "                filter = np.any(X_test.isna(), axis = 1).values\n",
    "                X_test = X_test[~filter]\n",
    "                y_test = y_test[~filter]\n",
    "\n",
    "                X_val = pd.merge(val_ids, final_df, on = \"id\", how = \"left\")\n",
    "                X_val = X_val.drop([\"id\",\"log_price\"], axis = 1)\n",
    "                X_val.columns = [\"bath\", \"bed\", \"dining\", \"hall\", \"kitchen\", \"living\"]#, \"others\"]\n",
    "                filter = np.any(X_val.isna(), axis = 1).values\n",
    "                X_val = X_val[~filter]\n",
    "                y_val = y_val[~filter]\n",
    "                \n",
    "                batch_size = 128\n",
    "\n",
    "                # setup tf datasets\n",
    "                data_train = tf.data.Dataset.from_tensor_slices({\"bath\": np.squeeze(np.stack(X_train[\"bath\"])),\n",
    "                                                                \"bed\": np.squeeze(np.stack(X_train[\"bed\"])),\n",
    "                                                                \"dining\": np.squeeze(np.stack(X_train[\"dining\"])),\n",
    "                                                                \"hall\": np.squeeze(np.stack(X_train[\"hall\"])),\n",
    "                                                                \"kitchen\": np.squeeze(np.stack(X_train[\"kitchen\"])),\n",
    "                                                                \"living\": np.squeeze(np.stack(X_train[\"living\"])),\n",
    "                                                                \"price\": y_train})\n",
    "                data_train = data_train.cache()\n",
    "                data_train = data_train.shuffle(6000, seed = 13)\n",
    "                train_dataset = data_train.take(len(y_train))\n",
    "                train_dataset = train_dataset.map(transform)\n",
    "                train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "                data_test = tf.data.Dataset.from_tensor_slices({\"bath\": np.squeeze(np.stack(X_test[\"bath\"])),\n",
    "                                                                \"bed\": np.squeeze(np.stack(X_test[\"bed\"])),\n",
    "                                                                \"dining\": np.squeeze(np.stack(X_test[\"dining\"])),\n",
    "                                                                \"hall\": np.squeeze(np.stack(X_test[\"hall\"])),\n",
    "                                                                \"kitchen\": np.squeeze(np.stack(X_test[\"kitchen\"])),\n",
    "                                                                \"living\": np.squeeze(np.stack(X_test[\"living\"])),\n",
    "                                                                \"price\": y_test})\n",
    "                data_test = data_test.cache()\n",
    "                test_dataset = data_test.take(len(y_test))\n",
    "                test_dataset = test_dataset.map(transform)\n",
    "                test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "                data_val = tf.data.Dataset.from_tensor_slices({\"bath\": np.squeeze(np.stack(X_val[\"bath\"])),\n",
    "                                                                \"bed\": np.squeeze(np.stack(X_val[\"bed\"])),\n",
    "                                                                \"dining\": np.squeeze(np.stack(X_val[\"dining\"])),\n",
    "                                                                \"hall\": np.squeeze(np.stack(X_val[\"hall\"])),\n",
    "                                                                \"kitchen\": np.squeeze(np.stack(X_val[\"kitchen\"])),\n",
    "                                                                \"living\": np.squeeze(np.stack(X_val[\"living\"])),\n",
    "                                                                \"price\": y_val})\n",
    "                data_val = data_val.cache()\n",
    "                val_dataset = data_val.take(len(y_val))\n",
    "                val_dataset = val_dataset.map(transform)\n",
    "                val_dataset = val_dataset.batch(batch_size)\n",
    "                \n",
    "                # setup model and train\n",
    "                model = Img_model(dropout= do, nodes1 = nodes, nodes2 = 1, l2 = l2)\n",
    "                \n",
    "                lr = tf.keras.optimizers.schedules.ExponentialDecay(0.01, decay_steps=100, decay_rate=0.9, staircase=False)\n",
    "                \n",
    "                model.compile(optimizer=keras.optimizers.Adam(lr),\n",
    "                                loss= \"mse\", metrics= R_squared)\n",
    "                tf.random.set_seed(2)\n",
    "                history = model.fit(train_dataset, epochs = 50, validation_data = val_dataset, verbose = 0)\n",
    "                best_val_mse_tmp.append(np.nanmin(np.array(history.history[\"val_loss\"])))\n",
    "                best_val_rsq_tmp.append(np.nanmax(np.array(history.history[\"val_R_squared\"])))\n",
    "                best_train_mse_tmp.append(np.nanmin(np.array(history.history[\"loss\"])))\n",
    "                best_train_rsq_tmp.append(np.nanmax(np.array(history.history[\"R_squared\"])))\n",
    "                                        \n",
    "            # save mean of metrics across folds                           \n",
    "            best_val_mse.append(np.nanmean(np.array(best_val_mse_tmp)))\n",
    "            best_val_rsq.append(np.nanmean(np.array(best_val_rsq_tmp)))\n",
    "            best_train_mse.append(np.nanmean(np.array(best_train_mse_tmp)))\n",
    "            best_train_rsq.append(np.nanmean(np.array(best_train_rsq_tmp)))\n",
    "            \n",
    "            # save parameter combination\n",
    "            para_do.append(do)\n",
    "            para_l2.append(l2)\n",
    "            para_nodes.append(nodes)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results = pd.DataFrame({\"Dropout\": para_do, \"L2\": para_l2, \"Nodes\": para_nodes, \"Val_MSE\": best_val_mse, \"Val_Rsq\": best_val_rsq, \"Train_MSE\": best_train_mse, \"Train_Rsq\": best_train_rsq})\n",
    "grid_results = pd.read_csv(\"grid_results_priceimage.csv\")\n",
    "grid_results.drop(grid_results.columns[0], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrrrrrr}\n",
      "\\toprule\n",
      " Dropout &    L2 &  Nodes &  Val\\_MSE &   Val\\_Rsq &  Train\\_MSE &  Train\\_Rsq \\\\\n",
      "\\midrule\n",
      "     0.0 & 0.000 &     64 & 0.387561 &  0.066719 &   0.028726 &   0.928711 \\\\\n",
      "     0.0 & 0.000 &    512 & 0.374895 &  0.102516 &   0.022524 &   0.944012 \\\\\n",
      "     0.0 & 0.010 &     64 & 0.443970 &  0.109065 &   0.281611 &   0.468693 \\\\\n",
      "     0.0 & 0.010 &    512 & 0.463205 &  0.109977 &   0.387082 &   0.280092 \\\\\n",
      "     0.0 & 0.001 &     64 & 0.461879 &  0.084419 &   0.158667 &   0.768617 \\\\\n",
      "     0.0 & 0.001 &    512 & 0.487444 &  0.091255 &   0.172171 &   0.770218 \\\\\n",
      "     0.2 & 0.000 &     64 & 0.354459 &  0.147335 &   0.404355 &   0.012711 \\\\\n",
      "     0.2 & 0.000 &    512 & 0.348692 &  0.164735 &   0.345854 &   0.161032 \\\\\n",
      "     0.2 & 0.010 &     64 & 0.469838 &  0.095086 &   0.740294 &  -0.547127 \\\\\n",
      "     0.2 & 0.010 &    512 & 0.527357 &  0.073910 &   0.749915 &  -0.469468 \\\\\n",
      "     0.2 & 0.001 &     64 & 0.463425 &  0.141795 &   0.570263 &  -0.126860 \\\\\n",
      "     0.2 & 0.001 &    512 & 0.506924 &  0.135987 &   0.645072 &  -0.199441 \\\\\n",
      "     0.5 & 0.000 &     64 & 0.412565 &  0.003676 &   0.847911 &  -1.077794 \\\\\n",
      "     0.5 & 0.000 &    512 & 0.401188 &  0.035182 &   0.873145 &  -1.135030 \\\\\n",
      "     0.5 & 0.010 &     64 & 0.553903 & -0.062593 &   1.017722 &  -1.169570 \\\\\n",
      "     0.5 & 0.010 &    512 & 0.642384 & -0.028530 &   1.115496 &  -1.183225 \\\\\n",
      "     0.5 & 0.001 &     64 & 0.517259 &  0.003316 &   0.942442 &  -1.037349 \\\\\n",
      "     0.5 & 0.001 &    512 & 0.592464 &  0.012630 &   1.012375 &  -1.012021 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(grid_results.to_latex(index = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize grid search results\n",
    "grid_res = grid_results.groupby([\"Dropout\",\"L2\",\"Nodes\"]).sum()\n",
    "grid_res.columns = [\"Val MSE\", r\"Val. R^2\", \"Train MSE\", r\"Train R^2\"]\n",
    "grid_nice = grid_res[[r\"Val. R^2\", r\"Train R^2\"]].style.background_gradient(cmap ='Greens')\\\n",
    "        .set_properties(**{'font-size': '10px'})\n",
    "grid_nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a8e2b_row0_col0 {\n",
       "  background-color: #016e2d;\n",
       "  color: #f1f1f1;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row0_col1 {\n",
       "  background-color: #00451c;\n",
       "  color: #f1f1f1;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row1_col0 {\n",
       "  background-color: #006027;\n",
       "  color: #f1f1f1;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row1_col1, #T_a8e2b_row7_col0 {\n",
       "  background-color: #00441b;\n",
       "  color: #f1f1f1;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row2_col0 {\n",
       "  background-color: #45ad5f;\n",
       "  color: #f1f1f1;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row2_col1 {\n",
       "  background-color: #006c2c;\n",
       "  color: #f1f1f1;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row3_col0 {\n",
       "  background-color: #68be70;\n",
       "  color: #000000;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row3_col1 {\n",
       "  background-color: #03702e;\n",
       "  color: #f1f1f1;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row4_col0 {\n",
       "  background-color: #359e53;\n",
       "  color: #f1f1f1;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row4_col1 {\n",
       "  background-color: #1f8742;\n",
       "  color: #f1f1f1;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row5_col0 {\n",
       "  background-color: #46ae60;\n",
       "  color: #f1f1f1;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row5_col1 {\n",
       "  background-color: #37a055;\n",
       "  color: #f1f1f1;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row6_col0 {\n",
       "  background-color: #004a1e;\n",
       "  color: #f1f1f1;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row6_col1 {\n",
       "  background-color: #3ba458;\n",
       "  color: #f1f1f1;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row7_col1 {\n",
       "  background-color: #2e964d;\n",
       "  color: #f1f1f1;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row8_col0 {\n",
       "  background-color: #48ae60;\n",
       "  color: #f1f1f1;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row8_col1 {\n",
       "  background-color: #75c477;\n",
       "  color: #000000;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row9_col0 {\n",
       "  background-color: #81ca81;\n",
       "  color: #000000;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row9_col1 {\n",
       "  background-color: #8dd08a;\n",
       "  color: #000000;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row10_col0 {\n",
       "  background-color: #50b264;\n",
       "  color: #f1f1f1;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row10_col1 {\n",
       "  background-color: #abdda5;\n",
       "  color: #000000;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row11_col0 {\n",
       "  background-color: #9bd696;\n",
       "  color: #000000;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row11_col1 {\n",
       "  background-color: #aedea7;\n",
       "  color: #000000;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row12_col0 {\n",
       "  background-color: #19833e;\n",
       "  color: #f1f1f1;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row12_col1 {\n",
       "  background-color: #c9eac2;\n",
       "  color: #000000;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row13_col0 {\n",
       "  background-color: #0e7936;\n",
       "  color: #f1f1f1;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row13_col1 {\n",
       "  background-color: #ceecc8;\n",
       "  color: #000000;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row14_col0 {\n",
       "  background-color: #8ed08b;\n",
       "  color: #000000;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row14_col1 {\n",
       "  background-color: #ddf2d8;\n",
       "  color: #000000;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row15_col0 {\n",
       "  background-color: #dbf1d5;\n",
       "  color: #000000;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row15_col1 {\n",
       "  background-color: #e9f7e5;\n",
       "  color: #000000;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row16_col0 {\n",
       "  background-color: #b7e2b1;\n",
       "  color: #000000;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row16_col1 {\n",
       "  background-color: #ebf7e7;\n",
       "  color: #000000;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_a8e2b_row17_col0, #T_a8e2b_row17_col1 {\n",
       "  background-color: #f7fcf5;\n",
       "  color: #000000;\n",
       "  font-size: 10px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a8e2b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a8e2b_level0_col0\" class=\"col_heading level0 col0\" >Val MSE</th>\n",
       "      <th id=\"T_a8e2b_level0_col1\" class=\"col_heading level0 col1\" >Train MSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Dropout</th>\n",
       "      <th class=\"index_name level1\" >L2</th>\n",
       "      <th class=\"index_name level2\" >Nodes</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a8e2b_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"6\">0.000000</th>\n",
       "      <th id=\"T_a8e2b_level1_row0\" class=\"row_heading level1 row0\" rowspan=\"2\">0.000000</th>\n",
       "      <th id=\"T_a8e2b_level2_row0\" class=\"row_heading level2 row0\" >64</th>\n",
       "      <td id=\"T_a8e2b_row0_col0\" class=\"data row0 col0\" >0.387561</td>\n",
       "      <td id=\"T_a8e2b_row0_col1\" class=\"data row0 col1\" >0.028726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a8e2b_level2_row1\" class=\"row_heading level2 row1\" >512</th>\n",
       "      <td id=\"T_a8e2b_row1_col0\" class=\"data row1 col0\" >0.374895</td>\n",
       "      <td id=\"T_a8e2b_row1_col1\" class=\"data row1 col1\" >0.022524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a8e2b_level1_row2\" class=\"row_heading level1 row2\" rowspan=\"2\">0.001000</th>\n",
       "      <th id=\"T_a8e2b_level2_row2\" class=\"row_heading level2 row2\" >64</th>\n",
       "      <td id=\"T_a8e2b_row2_col0\" class=\"data row2 col0\" >0.461879</td>\n",
       "      <td id=\"T_a8e2b_row2_col1\" class=\"data row2 col1\" >0.158667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a8e2b_level2_row3\" class=\"row_heading level2 row3\" >512</th>\n",
       "      <td id=\"T_a8e2b_row3_col0\" class=\"data row3 col0\" >0.487444</td>\n",
       "      <td id=\"T_a8e2b_row3_col1\" class=\"data row3 col1\" >0.172171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a8e2b_level1_row4\" class=\"row_heading level1 row4\" rowspan=\"2\">0.010000</th>\n",
       "      <th id=\"T_a8e2b_level2_row4\" class=\"row_heading level2 row4\" >64</th>\n",
       "      <td id=\"T_a8e2b_row4_col0\" class=\"data row4 col0\" >0.443970</td>\n",
       "      <td id=\"T_a8e2b_row4_col1\" class=\"data row4 col1\" >0.281611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a8e2b_level2_row5\" class=\"row_heading level2 row5\" >512</th>\n",
       "      <td id=\"T_a8e2b_row5_col0\" class=\"data row5 col0\" >0.463205</td>\n",
       "      <td id=\"T_a8e2b_row5_col1\" class=\"data row5 col1\" >0.387082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a8e2b_level0_row6\" class=\"row_heading level0 row6\" rowspan=\"6\">0.200000</th>\n",
       "      <th id=\"T_a8e2b_level1_row6\" class=\"row_heading level1 row6\" rowspan=\"2\">0.000000</th>\n",
       "      <th id=\"T_a8e2b_level2_row6\" class=\"row_heading level2 row6\" >64</th>\n",
       "      <td id=\"T_a8e2b_row6_col0\" class=\"data row6 col0\" >0.354459</td>\n",
       "      <td id=\"T_a8e2b_row6_col1\" class=\"data row6 col1\" >0.404355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a8e2b_level2_row7\" class=\"row_heading level2 row7\" >512</th>\n",
       "      <td id=\"T_a8e2b_row7_col0\" class=\"data row7 col0\" >0.348692</td>\n",
       "      <td id=\"T_a8e2b_row7_col1\" class=\"data row7 col1\" >0.345854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a8e2b_level1_row8\" class=\"row_heading level1 row8\" rowspan=\"2\">0.001000</th>\n",
       "      <th id=\"T_a8e2b_level2_row8\" class=\"row_heading level2 row8\" >64</th>\n",
       "      <td id=\"T_a8e2b_row8_col0\" class=\"data row8 col0\" >0.463425</td>\n",
       "      <td id=\"T_a8e2b_row8_col1\" class=\"data row8 col1\" >0.570263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a8e2b_level2_row9\" class=\"row_heading level2 row9\" >512</th>\n",
       "      <td id=\"T_a8e2b_row9_col0\" class=\"data row9 col0\" >0.506924</td>\n",
       "      <td id=\"T_a8e2b_row9_col1\" class=\"data row9 col1\" >0.645072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a8e2b_level1_row10\" class=\"row_heading level1 row10\" rowspan=\"2\">0.010000</th>\n",
       "      <th id=\"T_a8e2b_level2_row10\" class=\"row_heading level2 row10\" >64</th>\n",
       "      <td id=\"T_a8e2b_row10_col0\" class=\"data row10 col0\" >0.469838</td>\n",
       "      <td id=\"T_a8e2b_row10_col1\" class=\"data row10 col1\" >0.740294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a8e2b_level2_row11\" class=\"row_heading level2 row11\" >512</th>\n",
       "      <td id=\"T_a8e2b_row11_col0\" class=\"data row11 col0\" >0.527357</td>\n",
       "      <td id=\"T_a8e2b_row11_col1\" class=\"data row11 col1\" >0.749915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a8e2b_level0_row12\" class=\"row_heading level0 row12\" rowspan=\"6\">0.500000</th>\n",
       "      <th id=\"T_a8e2b_level1_row12\" class=\"row_heading level1 row12\" rowspan=\"2\">0.000000</th>\n",
       "      <th id=\"T_a8e2b_level2_row12\" class=\"row_heading level2 row12\" >64</th>\n",
       "      <td id=\"T_a8e2b_row12_col0\" class=\"data row12 col0\" >0.412565</td>\n",
       "      <td id=\"T_a8e2b_row12_col1\" class=\"data row12 col1\" >0.847911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a8e2b_level2_row13\" class=\"row_heading level2 row13\" >512</th>\n",
       "      <td id=\"T_a8e2b_row13_col0\" class=\"data row13 col0\" >0.401188</td>\n",
       "      <td id=\"T_a8e2b_row13_col1\" class=\"data row13 col1\" >0.873145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a8e2b_level1_row14\" class=\"row_heading level1 row14\" rowspan=\"2\">0.001000</th>\n",
       "      <th id=\"T_a8e2b_level2_row14\" class=\"row_heading level2 row14\" >64</th>\n",
       "      <td id=\"T_a8e2b_row14_col0\" class=\"data row14 col0\" >0.517259</td>\n",
       "      <td id=\"T_a8e2b_row14_col1\" class=\"data row14 col1\" >0.942442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a8e2b_level2_row15\" class=\"row_heading level2 row15\" >512</th>\n",
       "      <td id=\"T_a8e2b_row15_col0\" class=\"data row15 col0\" >0.592464</td>\n",
       "      <td id=\"T_a8e2b_row15_col1\" class=\"data row15 col1\" >1.012375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a8e2b_level1_row16\" class=\"row_heading level1 row16\" rowspan=\"2\">0.010000</th>\n",
       "      <th id=\"T_a8e2b_level2_row16\" class=\"row_heading level2 row16\" >64</th>\n",
       "      <td id=\"T_a8e2b_row16_col0\" class=\"data row16 col0\" >0.553903</td>\n",
       "      <td id=\"T_a8e2b_row16_col1\" class=\"data row16 col1\" >1.017722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a8e2b_level2_row17\" class=\"row_heading level2 row17\" >512</th>\n",
       "      <td id=\"T_a8e2b_row17_col0\" class=\"data row17 col0\" >0.642384</td>\n",
       "      <td id=\"T_a8e2b_row17_col1\" class=\"data row17 col1\" >1.115496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x3b3a288e0>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_res = grid_results.groupby([\"Dropout\",\"L2\",\"Nodes\"]).sum()\n",
    "grid_res.columns = [\"Val MSE\", r\"Val. R^2\", \"Train MSE\", r\"Train R^2\"]\n",
    "grid_nice2 = grid_res[[\"Val MSE\", \"Train MSE\"]].style.background_gradient(cmap ='Greens_r')\\\n",
    "        .set_properties(**{'font-size': '10px'})\n",
    "grid_nice2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train again for best parameters to find best fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 15:30:38.424325: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-02-22 15:30:38.435320: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 15:30:40.353517: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - ETA: 0s - loss: 19.9002 - mae: 4.3768 - R_squared: -47.4141"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 15:30:47.402172: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 10s 337ms/step - loss: 19.9002 - mae: 4.3768 - R_squared: -47.4141 - val_loss: 3.0199 - val_mae: 1.6184 - val_R_squared: -6.7949\n",
      "Epoch 2/300\n",
      "22/22 [==============================] - 7s 301ms/step - loss: 16.2567 - mae: 3.9670 - R_squared: -38.2907 - val_loss: 4.4761 - val_mae: 2.0229 - val_R_squared: -10.5338\n",
      "Epoch 3/300\n",
      "22/22 [==============================] - 7s 304ms/step - loss: 13.1695 - mae: 3.5651 - R_squared: -31.2449 - val_loss: 4.5945 - val_mae: 2.0523 - val_R_squared: -10.8360\n",
      "Epoch 4/300\n",
      "22/22 [==============================] - 7s 315ms/step - loss: 10.6938 - mae: 3.2008 - R_squared: -24.7609 - val_loss: 4.5861 - val_mae: 2.0489 - val_R_squared: -10.8107\n",
      "Epoch 5/300\n",
      "22/22 [==============================] - 7s 307ms/step - loss: 8.6311 - mae: 2.8614 - R_squared: -19.8220 - val_loss: 4.5942 - val_mae: 2.0487 - val_R_squared: -10.8275\n",
      "Epoch 6/300\n",
      "22/22 [==============================] - 7s 297ms/step - loss: 7.0717 - mae: 2.5727 - R_squared: -16.1333 - val_loss: 3.7909 - val_mae: 1.8447 - val_R_squared: -8.7625\n",
      "Epoch 7/300\n",
      "22/22 [==============================] - 7s 293ms/step - loss: 5.6409 - mae: 2.2807 - R_squared: -12.4656 - val_loss: 3.2801 - val_mae: 1.7005 - val_R_squared: -7.4473\n",
      "Epoch 8/300\n",
      "22/22 [==============================] - 7s 296ms/step - loss: 4.5436 - mae: 2.0297 - R_squared: -9.9687 - val_loss: 2.8190 - val_mae: 1.5598 - val_R_squared: -6.2599\n",
      "Epoch 9/300\n",
      "22/22 [==============================] - 7s 298ms/step - loss: 3.7490 - mae: 1.8152 - R_squared: -8.0365 - val_loss: 2.6206 - val_mae: 1.4959 - val_R_squared: -5.7492\n",
      "Epoch 10/300\n",
      "22/22 [==============================] - 7s 299ms/step - loss: 3.0131 - mae: 1.6043 - R_squared: -6.1807 - val_loss: 2.1063 - val_mae: 1.3165 - val_R_squared: -4.4253\n",
      "Epoch 11/300\n",
      "22/22 [==============================] - 7s 306ms/step - loss: 2.4869 - mae: 1.4277 - R_squared: -5.0595 - val_loss: 1.7970 - val_mae: 1.1956 - val_R_squared: -3.6295\n",
      "Epoch 12/300\n",
      "22/22 [==============================] - 7s 296ms/step - loss: 2.0194 - mae: 1.2675 - R_squared: -3.8367 - val_loss: 1.6436 - val_mae: 1.1319 - val_R_squared: -3.2334\n",
      "Epoch 13/300\n",
      "22/22 [==============================] - 7s 301ms/step - loss: 1.6873 - mae: 1.1283 - R_squared: -3.0687 - val_loss: 1.0521 - val_mae: 0.8583 - val_R_squared: -1.7030\n",
      "Epoch 14/300\n",
      "22/22 [==============================] - 7s 302ms/step - loss: 1.3700 - mae: 0.9961 - R_squared: -2.3743 - val_loss: 1.0869 - val_mae: 0.8760 - val_R_squared: -1.7960\n",
      "Epoch 15/300\n",
      "22/22 [==============================] - 9s 397ms/step - loss: 1.1984 - mae: 0.9171 - R_squared: -1.8738 - val_loss: 0.9172 - val_mae: 0.7931 - val_R_squared: -1.3594\n",
      "Epoch 16/300\n",
      "22/22 [==============================] - 7s 325ms/step - loss: 1.0441 - mae: 0.8305 - R_squared: -1.5065 - val_loss: 0.8285 - val_mae: 0.7448 - val_R_squared: -1.1298\n",
      "Epoch 17/300\n",
      "22/22 [==============================] - 8s 343ms/step - loss: 0.9138 - mae: 0.7674 - R_squared: -1.2556 - val_loss: 0.8308 - val_mae: 0.7459 - val_R_squared: -1.1311\n",
      "Epoch 18/300\n",
      "22/22 [==============================] - 7s 334ms/step - loss: 0.8023 - mae: 0.7148 - R_squared: -0.9309 - val_loss: 0.6329 - val_mae: 0.6390 - val_R_squared: -0.6238\n",
      "Epoch 19/300\n",
      "22/22 [==============================] - 7s 332ms/step - loss: 0.7474 - mae: 0.6801 - R_squared: -0.8387 - val_loss: 0.5533 - val_mae: 0.5938 - val_R_squared: -0.4199\n",
      "Epoch 20/300\n",
      "22/22 [==============================] - 8s 336ms/step - loss: 0.6478 - mae: 0.6295 - R_squared: -0.5738 - val_loss: 0.5847 - val_mae: 0.6105 - val_R_squared: -0.5031\n",
      "Epoch 21/300\n",
      "22/22 [==============================] - 7s 334ms/step - loss: 0.6142 - mae: 0.6082 - R_squared: -0.4634 - val_loss: 0.5720 - val_mae: 0.6031 - val_R_squared: -0.4713\n",
      "Epoch 22/300\n",
      "22/22 [==============================] - 8s 339ms/step - loss: 0.5642 - mae: 0.5876 - R_squared: -0.3464 - val_loss: 0.5222 - val_mae: 0.5754 - val_R_squared: -0.3388\n",
      "Epoch 23/300\n",
      "22/22 [==============================] - 7s 337ms/step - loss: 0.5541 - mae: 0.5708 - R_squared: -0.3469 - val_loss: 0.4797 - val_mae: 0.5492 - val_R_squared: -0.2247\n",
      "Epoch 24/300\n",
      "22/22 [==============================] - 7s 331ms/step - loss: 0.5163 - mae: 0.5631 - R_squared: -0.2429 - val_loss: 0.4210 - val_mae: 0.5181 - val_R_squared: -0.0696\n",
      "Epoch 25/300\n",
      "22/22 [==============================] - 8s 346ms/step - loss: 0.4957 - mae: 0.5460 - R_squared: -0.1977 - val_loss: 0.3659 - val_mae: 0.4904 - val_R_squared: 0.0692\n",
      "Epoch 26/300\n",
      "22/22 [==============================] - 8s 351ms/step - loss: 0.4879 - mae: 0.5425 - R_squared: -0.1783 - val_loss: 0.3768 - val_mae: 0.4930 - val_R_squared: 0.0403\n",
      "Epoch 27/300\n",
      "22/22 [==============================] - 7s 333ms/step - loss: 0.4774 - mae: 0.5356 - R_squared: -0.1395 - val_loss: 0.3235 - val_mae: 0.4676 - val_R_squared: 0.1878\n",
      "Epoch 28/300\n",
      "22/22 [==============================] - 8s 346ms/step - loss: 0.4773 - mae: 0.5412 - R_squared: -0.1502 - val_loss: 0.3386 - val_mae: 0.4793 - val_R_squared: 0.1475\n",
      "Epoch 29/300\n",
      "22/22 [==============================] - 8s 354ms/step - loss: 0.4563 - mae: 0.5289 - R_squared: -0.1009 - val_loss: 0.3446 - val_mae: 0.4741 - val_R_squared: 0.1301\n",
      "Epoch 30/300\n",
      "22/22 [==============================] - 8s 338ms/step - loss: 0.4644 - mae: 0.5319 - R_squared: -0.1332 - val_loss: 0.3288 - val_mae: 0.4611 - val_R_squared: 0.1688\n",
      "Epoch 31/300\n",
      "22/22 [==============================] - 8s 347ms/step - loss: 0.4176 - mae: 0.5042 - R_squared: -0.0063 - val_loss: 0.3638 - val_mae: 0.4839 - val_R_squared: 0.0662\n",
      "Epoch 32/300\n",
      "22/22 [==============================] - 8s 360ms/step - loss: 0.4517 - mae: 0.5225 - R_squared: -0.0735 - val_loss: 0.3232 - val_mae: 0.4545 - val_R_squared: 0.1785\n",
      "Epoch 33/300\n",
      "22/22 [==============================] - 8s 364ms/step - loss: 0.4503 - mae: 0.5244 - R_squared: -0.0747 - val_loss: 0.3379 - val_mae: 0.4658 - val_R_squared: 0.1365\n",
      "Epoch 34/300\n",
      "22/22 [==============================] - 8s 360ms/step - loss: 0.4274 - mae: 0.5122 - R_squared: -0.0205 - val_loss: 0.3273 - val_mae: 0.4580 - val_R_squared: 0.1632\n",
      "Epoch 35/300\n",
      "22/22 [==============================] - 8s 371ms/step - loss: 0.4214 - mae: 0.5113 - R_squared: -0.0330 - val_loss: 0.3287 - val_mae: 0.4590 - val_R_squared: 0.1609\n",
      "Epoch 36/300\n",
      "22/22 [==============================] - 8s 366ms/step - loss: 0.4169 - mae: 0.4974 - R_squared: -0.0115 - val_loss: 0.3334 - val_mae: 0.4674 - val_R_squared: 0.1532\n",
      "Epoch 37/300\n",
      "22/22 [==============================] - 8s 363ms/step - loss: 0.4038 - mae: 0.4948 - R_squared: 0.0275 - val_loss: 0.3354 - val_mae: 0.4703 - val_R_squared: 0.1610\n",
      "Epoch 38/300\n",
      "22/22 [==============================] - 8s 359ms/step - loss: 0.3900 - mae: 0.4859 - R_squared: 0.0582 - val_loss: 0.3374 - val_mae: 0.4622 - val_R_squared: 0.1456\n",
      "Epoch 39/300\n",
      "22/22 [==============================] - 8s 366ms/step - loss: 0.4033 - mae: 0.4939 - R_squared: 0.0342 - val_loss: 0.3129 - val_mae: 0.4491 - val_R_squared: 0.2097\n",
      "Epoch 40/300\n",
      "22/22 [==============================] - 8s 368ms/step - loss: 0.3972 - mae: 0.4962 - R_squared: 0.0313 - val_loss: 0.3271 - val_mae: 0.4554 - val_R_squared: 0.1680\n",
      "Epoch 41/300\n",
      "22/22 [==============================] - 8s 366ms/step - loss: 0.3796 - mae: 0.4794 - R_squared: 0.0785 - val_loss: 0.3436 - val_mae: 0.4685 - val_R_squared: 0.1165\n",
      "Epoch 42/300\n",
      "22/22 [==============================] - 9s 384ms/step - loss: 0.4004 - mae: 0.4930 - R_squared: 0.0095 - val_loss: 0.3306 - val_mae: 0.4572 - val_R_squared: 0.1713\n",
      "Epoch 43/300\n",
      "22/22 [==============================] - 8s 370ms/step - loss: 0.4035 - mae: 0.4921 - R_squared: 0.0235 - val_loss: 0.3933 - val_mae: 0.5038 - val_R_squared: 0.0044\n",
      "Epoch 44/300\n",
      "22/22 [==============================] - 9s 386ms/step - loss: 0.3804 - mae: 0.4761 - R_squared: 0.0858 - val_loss: 0.3537 - val_mae: 0.4743 - val_R_squared: 0.1096\n",
      "Epoch 45/300\n",
      "22/22 [==============================] - 9s 394ms/step - loss: 0.3605 - mae: 0.4685 - R_squared: 0.1335 - val_loss: 0.3457 - val_mae: 0.4655 - val_R_squared: 0.1219\n",
      "Epoch 46/300\n",
      "22/22 [==============================] - 9s 395ms/step - loss: 0.3631 - mae: 0.4699 - R_squared: 0.1289 - val_loss: 0.3637 - val_mae: 0.4764 - val_R_squared: 0.0670\n",
      "Epoch 47/300\n",
      "22/22 [==============================] - 9s 387ms/step - loss: 0.3633 - mae: 0.4696 - R_squared: 0.1083 - val_loss: 0.3271 - val_mae: 0.4613 - val_R_squared: 0.1726\n",
      "Epoch 48/300\n",
      "22/22 [==============================] - 10s 459ms/step - loss: 0.3666 - mae: 0.4695 - R_squared: 0.1154 - val_loss: 0.3174 - val_mae: 0.4516 - val_R_squared: 0.1961\n",
      "Epoch 49/300\n",
      "22/22 [==============================] - 10s 441ms/step - loss: 0.3352 - mae: 0.4471 - R_squared: 0.2010 - val_loss: 0.3278 - val_mae: 0.4574 - val_R_squared: 0.1702\n",
      "Epoch 50/300\n",
      "22/22 [==============================] - 10s 459ms/step - loss: 0.3470 - mae: 0.4608 - R_squared: 0.1703 - val_loss: 0.3238 - val_mae: 0.4581 - val_R_squared: 0.1713\n",
      "Epoch 51/300\n",
      "22/22 [==============================] - 9s 388ms/step - loss: 0.3411 - mae: 0.4563 - R_squared: 0.1923 - val_loss: 0.3466 - val_mae: 0.4691 - val_R_squared: 0.1103\n",
      "Epoch 52/300\n",
      "22/22 [==============================] - 9s 408ms/step - loss: 0.3542 - mae: 0.4670 - R_squared: 0.1541 - val_loss: 0.3313 - val_mae: 0.4603 - val_R_squared: 0.1507\n",
      "Epoch 53/300\n",
      "22/22 [==============================] - 9s 383ms/step - loss: 0.3550 - mae: 0.4675 - R_squared: 0.1238 - val_loss: 0.3151 - val_mae: 0.4507 - val_R_squared: 0.2036\n",
      "Epoch 54/300\n",
      "22/22 [==============================] - 9s 427ms/step - loss: 0.3296 - mae: 0.4529 - R_squared: 0.2174 - val_loss: 0.3248 - val_mae: 0.4608 - val_R_squared: 0.1749\n",
      "Epoch 55/300\n",
      "22/22 [==============================] - 10s 449ms/step - loss: 0.3376 - mae: 0.4485 - R_squared: 0.1771 - val_loss: 0.3309 - val_mae: 0.4606 - val_R_squared: 0.1670\n",
      "Epoch 56/300\n",
      "22/22 [==============================] - 9s 403ms/step - loss: 0.3457 - mae: 0.4578 - R_squared: 0.1527 - val_loss: 0.3133 - val_mae: 0.4450 - val_R_squared: 0.2163\n",
      "Epoch 57/300\n",
      "22/22 [==============================] - 10s 440ms/step - loss: 0.3204 - mae: 0.4421 - R_squared: 0.2248 - val_loss: 0.3284 - val_mae: 0.4563 - val_R_squared: 0.1678\n",
      "Epoch 58/300\n",
      "22/22 [==============================] - 9s 418ms/step - loss: 0.3294 - mae: 0.4470 - R_squared: 0.2109 - val_loss: 0.3215 - val_mae: 0.4543 - val_R_squared: 0.1897\n",
      "Epoch 59/300\n",
      "22/22 [==============================] - 9s 415ms/step - loss: 0.3162 - mae: 0.4411 - R_squared: 0.2290 - val_loss: 0.3263 - val_mae: 0.4566 - val_R_squared: 0.1803\n",
      "Epoch 60/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3529 - mae: 0.4617 - R_squared: 0.1387"
     ]
    }
   ],
   "source": [
    "for i in np.arange(5):\n",
    "    tf.random.set_seed(123)\n",
    "    X_train, X_test, X_val, y_train, y_test, y_val = load_data_fold(fold = i+1,drop_id=False)\n",
    "    \n",
    "    train_ids = X_train[\"id\"]\n",
    "    test_ids = X_test[\"id\"]\n",
    "    val_ids = X_val[\"id\"]\n",
    "\n",
    "    X_train = pd.merge(train_ids, final_df, on = \"id\", how = \"left\")\n",
    "    X_train = X_train.drop([\"id\",\"log_price\"], axis = 1)\n",
    "    X_train.columns = [\"bath\", \"bed\", \"dining\", \"hall\", \"kitchen\", \"living\"]#, \"others\"]\n",
    "    filter = np.any(X_train.isna(), axis = 1).values\n",
    "    X_train = X_train[~filter]\n",
    "    y_train = y_train[~filter]\n",
    "\n",
    "    X_test = pd.merge(test_ids, final_df, on = \"id\", how = \"left\")\n",
    "    X_test = X_test.drop([\"id\",\"log_price\"], axis = 1)\n",
    "    X_test.columns = [\"bath\", \"bed\", \"dining\", \"hall\", \"kitchen\", \"living\"]#, \"others\"]\n",
    "    filter = np.any(X_test.isna(), axis = 1).values\n",
    "    X_test = X_test[~filter]\n",
    "    y_test = y_test[~filter]\n",
    "\n",
    "    X_val = pd.merge(val_ids, final_df, on = \"id\", how = \"left\")\n",
    "    X_val = X_val.drop([\"id\",\"log_price\"], axis = 1)\n",
    "    X_val.columns = [\"bath\", \"bed\", \"dining\", \"hall\", \"kitchen\", \"living\"]#, \"others\"]\n",
    "    filter = np.any(X_val.isna(), axis = 1).values\n",
    "    X_val = X_val[~filter]\n",
    "    y_val = y_val[~filter]\n",
    "\n",
    "    batch_size = 128\n",
    "\n",
    "    data_train = tf.data.Dataset.from_tensor_slices({\"bath\": np.squeeze(np.stack(X_train[\"bath\"])),\n",
    "                                                    \"bed\": np.squeeze(np.stack(X_train[\"bed\"])),\n",
    "                                                    \"dining\": np.squeeze(np.stack(X_train[\"dining\"])),\n",
    "                                                    \"hall\": np.squeeze(np.stack(X_train[\"hall\"])),\n",
    "                                                    \"kitchen\": np.squeeze(np.stack(X_train[\"kitchen\"])),\n",
    "                                                    \"living\": np.squeeze(np.stack(X_train[\"living\"])),\n",
    "                                                    \"price\": y_train})\n",
    "    data_train = data_train.cache()\n",
    "    data_train = data_train.shuffle(6000, seed = 13)\n",
    "    train_dataset = data_train.take(len(y_train))\n",
    "    train_dataset = train_dataset.map(transform)\n",
    "    train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "    data_test = tf.data.Dataset.from_tensor_slices({\"bath\": np.squeeze(np.stack(X_test[\"bath\"])),\n",
    "                                                    \"bed\": np.squeeze(np.stack(X_test[\"bed\"])),\n",
    "                                                    \"dining\": np.squeeze(np.stack(X_test[\"dining\"])),\n",
    "                                                    \"hall\": np.squeeze(np.stack(X_test[\"hall\"])),\n",
    "                                                    \"kitchen\": np.squeeze(np.stack(X_test[\"kitchen\"])),\n",
    "                                                    \"living\": np.squeeze(np.stack(X_test[\"living\"])),\n",
    "                                                    \"price\": y_test})\n",
    "    data_test = data_test.cache()\n",
    "    test_dataset = data_test.take(len(y_test))\n",
    "    test_dataset = test_dataset.map(transform)\n",
    "    test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "    data_val = tf.data.Dataset.from_tensor_slices({\"bath\": np.squeeze(np.stack(X_val[\"bath\"])),\n",
    "                                                    \"bed\": np.squeeze(np.stack(X_val[\"bed\"])),\n",
    "                                                    \"dining\": np.squeeze(np.stack(X_val[\"dining\"])),\n",
    "                                                    \"hall\": np.squeeze(np.stack(X_val[\"hall\"])),\n",
    "                                                    \"kitchen\": np.squeeze(np.stack(X_val[\"kitchen\"])),\n",
    "                                                    \"living\": np.squeeze(np.stack(X_val[\"living\"])),\n",
    "                                                    \"price\": y_val})\n",
    "    data_val = data_val.cache()\n",
    "    val_dataset = data_val.take(len(y_val))\n",
    "    val_dataset = val_dataset.map(transform)\n",
    "    val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "    tf.random.set_seed(2)\n",
    "    model = Img_model(dropout= 0.2, nodes1 = 512, nodes2 = 1, l2 = 0)\n",
    "\n",
    "    lr = tf.keras.optimizers.schedules.ExponentialDecay(0.01, decay_steps=100, decay_rate=0.9, staircase=False)\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr),\n",
    "                    loss= \"mse\", metrics= [\"mae\", R_squared])\n",
    "    history = model.fit(train_dataset, epochs = 300, validation_data = val_dataset, verbose = 1)\n",
    "    print(\"Val loss: \" ,np.nanmin(np.array(history.history[\"val_loss\"])))\n",
    "    print(\"Val MAE: \" ,np.nanmin(np.array(history.history[\"val_mae\"])))\n",
    "    print(\"Val R2: \", np.nanmax(np.array(history.history[\"val_R_squared\"])))\n",
    "    model.evaluate(test_dataset)\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = load_data_fold(fold = 2 ,drop_id=False)\n",
    "\n",
    "data_train = tf.data.Dataset.from_tensor_slices({\"bath\": np.squeeze(np.stack(X_train[\"bath\"])),\n",
    "                                                 \"bed\": np.squeeze(np.stack(X_train[\"bed\"])),\n",
    "                                                 \"dining\": np.squeeze(np.stack(X_train[\"dining\"])),\n",
    "                                                 \"hall\": np.squeeze(np.stack(X_train[\"hall\"])),\n",
    "                                                 \"kitchen\": np.squeeze(np.stack(X_train[\"kitchen\"])),\n",
    "                                                 \"living\": np.squeeze(np.stack(X_train[\"living\"])),\n",
    "                                                 #\"others\": np.squeeze(np.stack(X_train[\"others\"])),\n",
    "                                                 \"price\": y_train})\n",
    "data_train = data_train.cache()\n",
    "data_train = data_train.shuffle(6000, seed = 13)\n",
    "train_dataset = data_train.take(len(y_train))\n",
    "train_dataset = train_dataset.map(transform)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "data_test = tf.data.Dataset.from_tensor_slices({\"bath\": np.squeeze(np.stack(X_test[\"bath\"])),\n",
    "                                                 \"bed\": np.squeeze(np.stack(X_test[\"bed\"])),\n",
    "                                                 \"dining\": np.squeeze(np.stack(X_test[\"dining\"])),\n",
    "                                                 \"hall\": np.squeeze(np.stack(X_test[\"hall\"])),\n",
    "                                                 \"kitchen\": np.squeeze(np.stack(X_test[\"kitchen\"])),\n",
    "                                                 \"living\": np.squeeze(np.stack(X_test[\"living\"])),\n",
    "                                                # \"others\": np.squeeze(np.stack(X_test[\"others\"])),\n",
    "                                                 \"price\": y_test})\n",
    "data_test = data_test.cache()\n",
    "test_dataset = data_test.take(len(y_test))\n",
    "test_dataset = test_dataset.map(transform)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "data_val = tf.data.Dataset.from_tensor_slices({\"bath\": np.squeeze(np.stack(X_val[\"bath\"])),\n",
    "                                                 \"bed\": np.squeeze(np.stack(X_val[\"bed\"])),\n",
    "                                                 \"dining\": np.squeeze(np.stack(X_val[\"dining\"])),\n",
    "                                                 \"hall\": np.squeeze(np.stack(X_val[\"hall\"])),\n",
    "                                                 \"kitchen\": np.squeeze(np.stack(X_val[\"kitchen\"])),\n",
    "                                                 \"living\": np.squeeze(np.stack(X_val[\"living\"])),\n",
    "                                                 #\"others\": np.squeeze(np.stack(X_val[\"others\"])),\n",
    "                                                 \"price\": y_val})\n",
    "data_val = data_val.cache()\n",
    "val_dataset = data_val.take(len(y_val))\n",
    "val_dataset = val_dataset.map(transform)\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-14 15:57:02.231569: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\n",
      "2022-02-14 15:57:02.231777: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\n",
      "2022-02-14 15:57:02.232646: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-14 15:57:04.285919: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/24 [=>............................] - ETA: 5s - loss: 21.8686 - R_squared: -58.4945  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-14 15:57:05.920533: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\n",
      "2022-02-14 15:57:05.920560: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\n",
      "2022-02-14 15:57:06.313944: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2022-02-14 15:57:06.333169: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\n",
      "2022-02-14 15:57:06.356842: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: logs/price/ensemble/final/log_price_atleast4/train/plugins/profile/2022_02_14_15_57_06\n",
      "2022-02-14 15:57:06.365010: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to logs/price/ensemble/final/log_price_atleast4/train/plugins/profile/2022_02_14_15_57_06/Air-von-Dominik.fritz.box.trace.json.gz\n",
      "2022-02-14 15:57:06.392105: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: logs/price/ensemble/final/log_price_atleast4/train/plugins/profile/2022_02_14_15_57_06\n",
      "2022-02-14 15:57:06.392423: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to logs/price/ensemble/final/log_price_atleast4/train/plugins/profile/2022_02_14_15_57_06/Air-von-Dominik.fritz.box.memory_profile.json.gz\n",
      "2022-02-14 15:57:06.393896: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: logs/price/ensemble/final/log_price_atleast4/train/plugins/profile/2022_02_14_15_57_06Dumped tool data for xplane.pb to logs/price/ensemble/final/log_price_atleast4/train/plugins/profile/2022_02_14_15_57_06/Air-von-Dominik.fritz.box.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/price/ensemble/final/log_price_atleast4/train/plugins/profile/2022_02_14_15_57_06/Air-von-Dominik.fritz.box.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/price/ensemble/final/log_price_atleast4/train/plugins/profile/2022_02_14_15_57_06/Air-von-Dominik.fritz.box.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/price/ensemble/final/log_price_atleast4/train/plugins/profile/2022_02_14_15_57_06/Air-von-Dominik.fritz.box.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/price/ensemble/final/log_price_atleast4/train/plugins/profile/2022_02_14_15_57_06/Air-von-Dominik.fritz.box.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - ETA: 0s - loss: 19.5935 - R_squared: -46.9115"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-14 15:57:12.135552: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 10s 302ms/step - loss: 19.5935 - R_squared: -46.9115 - val_loss: 1.2334 - val_R_squared: -1.9088\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e8b0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3ade9e400>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3ade9eaf0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e820>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3b10d91f0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3cb3c09d0>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: logs/price/ensemble/final/log_price_atleast4/checkpoint/assets\n",
      "Epoch 2/300\n",
      "24/24 [==============================] - 7s 301ms/step - loss: 15.7364 - R_squared: -37.6142 - val_loss: 5.4295 - val_R_squared: -11.8965\n",
      "Epoch 3/300\n",
      "24/24 [==============================] - 9s 362ms/step - loss: 12.5804 - R_squared: -29.9265 - val_loss: 6.6254 - val_R_squared: -14.7441\n",
      "Epoch 4/300\n",
      "24/24 [==============================] - 8s 306ms/step - loss: 9.9052 - R_squared: -23.3496 - val_loss: 5.3545 - val_R_squared: -11.7098\n",
      "Epoch 5/300\n",
      "24/24 [==============================] - 7s 291ms/step - loss: 7.8761 - R_squared: -18.4170 - val_loss: 4.3538 - val_R_squared: -9.3241\n",
      "Epoch 6/300\n",
      "24/24 [==============================] - 8s 313ms/step - loss: 6.2623 - R_squared: -14.2284 - val_loss: 3.6231 - val_R_squared: -7.5837\n",
      "Epoch 7/300\n",
      "24/24 [==============================] - 8s 308ms/step - loss: 4.9337 - R_squared: -11.0981 - val_loss: 3.0136 - val_R_squared: -6.1348\n",
      "Epoch 8/300\n",
      "24/24 [==============================] - 7s 288ms/step - loss: 3.9571 - R_squared: -8.6846 - val_loss: 2.5321 - val_R_squared: -4.9896\n",
      "Epoch 9/300\n",
      "24/24 [==============================] - 8s 326ms/step - loss: 3.1631 - R_squared: -6.7704 - val_loss: 2.1913 - val_R_squared: -4.1793\n",
      "Epoch 10/300\n",
      "24/24 [==============================] - 7s 302ms/step - loss: 2.4777 - R_squared: -5.0569 - val_loss: 1.7132 - val_R_squared: -3.0425\n",
      "Epoch 11/300\n",
      "24/24 [==============================] - 7s 290ms/step - loss: 2.0326 - R_squared: -4.0131 - val_loss: 1.4537 - val_R_squared: -2.4245\n",
      "Epoch 12/300\n",
      "24/24 [==============================] - 7s 285ms/step - loss: 1.6523 - R_squared: -3.0616 - val_loss: 1.1531 - val_R_squared: -1.7102\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e8b0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3ade9e400>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3ade9eaf0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e820>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3b10d91f0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3cb3c09d0>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: logs/price/ensemble/final/log_price_atleast4/checkpoint/assets\n",
      "Epoch 13/300\n",
      "24/24 [==============================] - 15s 632ms/step - loss: 1.3737 - R_squared: -2.3816 - val_loss: 0.9080 - val_R_squared: -1.1264\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e8b0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3ade9e400>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3ade9eaf0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e820>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3b10d91f0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3cb3c09d0>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: logs/price/ensemble/final/log_price_atleast4/checkpoint/assets\n",
      "Epoch 14/300\n",
      "24/24 [==============================] - 7s 276ms/step - loss: 1.1526 - R_squared: -1.8024 - val_loss: 0.9005 - val_R_squared: -1.1093\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e8b0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3ade9e400>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3ade9eaf0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e820>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3b10d91f0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3cb3c09d0>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: logs/price/ensemble/final/log_price_atleast4/checkpoint/assets\n",
      "Epoch 15/300\n",
      "24/24 [==============================] - 10s 386ms/step - loss: 1.0060 - R_squared: -1.4660 - val_loss: 0.6361 - val_R_squared: -0.4892\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e8b0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3ade9e400>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3ade9eaf0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e820>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3b10d91f0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3cb3c09d0>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: logs/price/ensemble/final/log_price_atleast4/checkpoint/assets\n",
      "Epoch 16/300\n",
      "24/24 [==============================] - 11s 457ms/step - loss: 0.8587 - R_squared: -1.0937 - val_loss: 0.6046 - val_R_squared: -0.4160\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e8b0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3ade9e400>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3ade9eaf0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e820>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3b10d91f0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3cb3c09d0>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: logs/price/ensemble/final/log_price_atleast4/checkpoint/assets\n",
      "Epoch 17/300\n",
      "24/24 [==============================] - 8s 310ms/step - loss: 0.7709 - R_squared: -0.8845 - val_loss: 0.6111 - val_R_squared: -0.4277\n",
      "Epoch 18/300\n",
      "24/24 [==============================] - 7s 266ms/step - loss: 0.6901 - R_squared: -0.6826 - val_loss: 0.6481 - val_R_squared: -0.5119\n",
      "Epoch 19/300\n",
      "24/24 [==============================] - 7s 298ms/step - loss: 0.6284 - R_squared: -0.5373 - val_loss: 0.5353 - val_R_squared: -0.2513\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e8b0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3ade9e400>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3ade9eaf0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e820>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3b10d91f0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3cb3c09d0>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: logs/price/ensemble/final/log_price_atleast4/checkpoint/assets\n",
      "Epoch 20/300\n",
      "24/24 [==============================] - 10s 346ms/step - loss: 0.5900 - R_squared: -0.4456 - val_loss: 0.5130 - val_R_squared: -0.1979\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e8b0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3ade9e400>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3ade9eaf0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e820>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3b10d91f0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3cb3c09d0>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: logs/price/ensemble/final/log_price_atleast4/checkpoint/assets\n",
      "Epoch 21/300\n",
      "24/24 [==============================] - 7s 277ms/step - loss: 0.5797 - R_squared: -0.4150 - val_loss: 0.4086 - val_R_squared: 0.0435\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e8b0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3ade9e400>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3ade9eaf0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e820>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3b10d91f0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3cb3c09d0>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: logs/price/ensemble/final/log_price_atleast4/checkpoint/assets\n",
      "Epoch 22/300\n",
      "24/24 [==============================] - 7s 281ms/step - loss: 0.5318 - R_squared: -0.2968 - val_loss: 0.3932 - val_R_squared: 0.0768\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e8b0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3ade9e400>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3ade9eaf0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e820>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3b10d91f0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3cb3c09d0>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: logs/price/ensemble/final/log_price_atleast4/checkpoint/assets\n",
      "Epoch 23/300\n",
      "24/24 [==============================] - 7s 277ms/step - loss: 0.5093 - R_squared: -0.2572 - val_loss: 0.4408 - val_R_squared: -0.0270\n",
      "Epoch 24/300\n",
      "24/24 [==============================] - 7s 292ms/step - loss: 0.4824 - R_squared: -0.1746 - val_loss: 0.4222 - val_R_squared: 0.0148\n",
      "Epoch 25/300\n",
      "24/24 [==============================] - 7s 289ms/step - loss: 0.4775 - R_squared: -0.1675 - val_loss: 0.5076 - val_R_squared: -0.1775\n",
      "Epoch 26/300\n",
      "24/24 [==============================] - 7s 285ms/step - loss: 0.4837 - R_squared: -0.1826 - val_loss: 0.4020 - val_R_squared: 0.0604\n",
      "Epoch 27/300\n",
      "24/24 [==============================] - 7s 279ms/step - loss: 0.4650 - R_squared: -0.1316 - val_loss: 0.3708 - val_R_squared: 0.1323\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e8b0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3ade9e400>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3ade9eaf0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e820>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3b10d91f0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3cb3c09d0>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: logs/price/ensemble/final/log_price_atleast4/checkpoint/assets\n",
      "Epoch 28/300\n",
      "24/24 [==============================] - 7s 276ms/step - loss: 0.4501 - R_squared: -0.1052 - val_loss: 0.3730 - val_R_squared: 0.1232\n",
      "Epoch 29/300\n",
      "24/24 [==============================] - 7s 304ms/step - loss: 0.4411 - R_squared: -0.0801 - val_loss: 0.3751 - val_R_squared: 0.1157\n",
      "Epoch 30/300\n",
      "24/24 [==============================] - 8s 341ms/step - loss: 0.4438 - R_squared: -0.0876 - val_loss: 0.4058 - val_R_squared: 0.0566\n",
      "Epoch 31/300\n",
      "24/24 [==============================] - 8s 313ms/step - loss: 0.4082 - R_squared: -1.5207e-04 - val_loss: 0.3654 - val_R_squared: 0.1440\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e8b0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3ade9e400>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3ade9eaf0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e820>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3b10d91f0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3cb3c09d0>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: logs/price/ensemble/final/log_price_atleast4/checkpoint/assets\n",
      "Epoch 32/300\n",
      "24/24 [==============================] - 7s 292ms/step - loss: 0.4177 - R_squared: -0.0241 - val_loss: 0.3754 - val_R_squared: 0.1230\n",
      "Epoch 33/300\n",
      "24/24 [==============================] - 8s 315ms/step - loss: 0.4148 - R_squared: -0.0058 - val_loss: 0.3595 - val_R_squared: 0.1597\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e8b0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3ade9e400>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3ade9eaf0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e820>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3b10d91f0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3cb3c09d0>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: logs/price/ensemble/final/log_price_atleast4/checkpoint/assets\n",
      "Epoch 34/300\n",
      "24/24 [==============================] - 7s 298ms/step - loss: 0.4092 - R_squared: 0.0036 - val_loss: 0.3734 - val_R_squared: 0.1252\n",
      "Epoch 35/300\n",
      "24/24 [==============================] - 8s 309ms/step - loss: 0.4019 - R_squared: 0.0145 - val_loss: 0.3646 - val_R_squared: 0.1449\n",
      "Epoch 36/300\n",
      "24/24 [==============================] - 8s 313ms/step - loss: 0.3956 - R_squared: 0.0298 - val_loss: 0.3663 - val_R_squared: 0.1412\n",
      "Epoch 37/300\n",
      "24/24 [==============================] - 8s 315ms/step - loss: 0.3880 - R_squared: 0.0552 - val_loss: 0.3701 - val_R_squared: 0.1346\n",
      "Epoch 38/300\n",
      "24/24 [==============================] - 7s 289ms/step - loss: 0.4138 - R_squared: -0.0063 - val_loss: 0.3856 - val_R_squared: 0.0929\n",
      "Epoch 39/300\n",
      "24/24 [==============================] - 8s 333ms/step - loss: 0.3881 - R_squared: 0.0501 - val_loss: 0.3812 - val_R_squared: 0.1098\n",
      "Epoch 40/300\n",
      "24/24 [==============================] - 7s 293ms/step - loss: 0.3902 - R_squared: 0.0460 - val_loss: 0.3876 - val_R_squared: 0.0994\n",
      "Epoch 41/300\n",
      "24/24 [==============================] - 7s 267ms/step - loss: 0.3803 - R_squared: 0.0724 - val_loss: 0.4121 - val_R_squared: 0.0416\n",
      "Epoch 42/300\n",
      "24/24 [==============================] - 7s 304ms/step - loss: 0.4041 - R_squared: 0.0112 - val_loss: 0.3848 - val_R_squared: 0.1073\n",
      "Epoch 43/300\n",
      "24/24 [==============================] - 8s 316ms/step - loss: 0.3718 - R_squared: 0.0830 - val_loss: 0.4166 - val_R_squared: 0.0297\n",
      "Epoch 44/300\n",
      "24/24 [==============================] - 9s 348ms/step - loss: 0.3765 - R_squared: 0.0776 - val_loss: 0.3608 - val_R_squared: 0.1618\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e8b0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3ade9e400>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3ade9eaf0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e820>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3b10d91f0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3cb3c09d0>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: logs/price/ensemble/final/log_price_atleast4/checkpoint/assets\n",
      "Epoch 45/300\n",
      "24/24 [==============================] - 7s 268ms/step - loss: 0.3784 - R_squared: 0.0807 - val_loss: 0.3625 - val_R_squared: 0.1536\n",
      "Epoch 46/300\n",
      "24/24 [==============================] - 7s 279ms/step - loss: 0.3652 - R_squared: 0.1124 - val_loss: 0.3718 - val_R_squared: 0.1290\n",
      "Epoch 47/300\n",
      "24/24 [==============================] - 7s 286ms/step - loss: 0.3800 - R_squared: 0.0695 - val_loss: 0.3809 - val_R_squared: 0.1180\n",
      "Epoch 48/300\n",
      "24/24 [==============================] - 7s 274ms/step - loss: 0.3712 - R_squared: 0.0951 - val_loss: 0.3690 - val_R_squared: 0.1430\n",
      "Epoch 49/300\n",
      "24/24 [==============================] - 7s 278ms/step - loss: 0.3515 - R_squared: 0.1455 - val_loss: 0.3625 - val_R_squared: 0.1544\n",
      "Epoch 50/300\n",
      "24/24 [==============================] - 7s 288ms/step - loss: 0.3535 - R_squared: 0.1362 - val_loss: 0.3539 - val_R_squared: 0.1759\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e8b0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3ade9e400>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3ade9eaf0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x3ade9e820>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x3b10d91f0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x3cb3c09d0>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: logs/price/ensemble/final/log_price_atleast4/checkpoint/assets\n",
      "Epoch 51/300\n",
      "24/24 [==============================] - 7s 301ms/step - loss: 0.3561 - R_squared: 0.1235 - val_loss: 0.3587 - val_R_squared: 0.1622\n",
      "Epoch 52/300\n",
      "24/24 [==============================] - 9s 371ms/step - loss: 0.3510 - R_squared: 0.1445 - val_loss: 0.3666 - val_R_squared: 0.1461\n",
      "Epoch 53/300\n",
      "24/24 [==============================] - 8s 314ms/step - loss: 0.3571 - R_squared: 0.1310 - val_loss: 0.3698 - val_R_squared: 0.1422\n",
      "Epoch 54/300\n",
      "24/24 [==============================] - 8s 316ms/step - loss: 0.3499 - R_squared: 0.1491 - val_loss: 0.3637 - val_R_squared: 0.1560\n",
      "Epoch 55/300\n",
      "24/24 [==============================] - 7s 295ms/step - loss: 0.3464 - R_squared: 0.1510 - val_loss: 0.3916 - val_R_squared: 0.0860\n",
      "Epoch 56/300\n",
      "24/24 [==============================] - 8s 334ms/step - loss: 0.3548 - R_squared: 0.1338 - val_loss: 0.3796 - val_R_squared: 0.1216\n",
      "Epoch 57/300\n",
      "24/24 [==============================] - 7s 292ms/step - loss: 0.3430 - R_squared: 0.1637 - val_loss: 0.3697 - val_R_squared: 0.1431\n",
      "Epoch 58/300\n",
      "24/24 [==============================] - 8s 312ms/step - loss: 0.3525 - R_squared: 0.1357 - val_loss: 0.3871 - val_R_squared: 0.0998\n",
      "Epoch 59/300\n",
      "24/24 [==============================] - 7s 299ms/step - loss: 0.3460 - R_squared: 0.1635 - val_loss: 0.3805 - val_R_squared: 0.1156\n",
      "Epoch 60/300\n",
      "24/24 [==============================] - 7s 291ms/step - loss: 0.3265 - R_squared: 0.2015 - val_loss: 0.3870 - val_R_squared: 0.1027\n",
      "Epoch 61/300\n",
      "24/24 [==============================] - 7s 283ms/step - loss: 0.3270 - R_squared: 0.2012 - val_loss: 0.4115 - val_R_squared: 0.0485\n",
      "Epoch 62/300\n",
      "24/24 [==============================] - 8s 308ms/step - loss: 0.3246 - R_squared: 0.2110 - val_loss: 0.3889 - val_R_squared: 0.1004\n",
      "Epoch 63/300\n",
      "24/24 [==============================] - 7s 283ms/step - loss: 0.3258 - R_squared: 0.2018 - val_loss: 0.3944 - val_R_squared: 0.0860\n",
      "Epoch 64/300\n",
      "24/24 [==============================] - 8s 333ms/step - loss: 0.3381 - R_squared: 0.1722 - val_loss: 0.3697 - val_R_squared: 0.1425\n",
      "Epoch 65/300\n",
      "24/24 [==============================] - 7s 286ms/step - loss: 0.3305 - R_squared: 0.1890 - val_loss: 0.3762 - val_R_squared: 0.1280\n",
      "Epoch 66/300\n",
      "24/24 [==============================] - 7s 276ms/step - loss: 0.3097 - R_squared: 0.2418 - val_loss: 0.3795 - val_R_squared: 0.1171\n",
      "Epoch 67/300\n",
      "24/24 [==============================] - 7s 296ms/step - loss: 0.3248 - R_squared: 0.2043 - val_loss: 0.3817 - val_R_squared: 0.1137\n",
      "Epoch 68/300\n",
      "24/24 [==============================] - 7s 298ms/step - loss: 0.3019 - R_squared: 0.2662 - val_loss: 0.3856 - val_R_squared: 0.1043\n",
      "Epoch 69/300\n",
      "24/24 [==============================] - 7s 289ms/step - loss: 0.3099 - R_squared: 0.2381 - val_loss: 0.3855 - val_R_squared: 0.1077\n",
      "Epoch 70/300\n",
      "24/24 [==============================] - 7s 291ms/step - loss: 0.3021 - R_squared: 0.2618 - val_loss: 0.3798 - val_R_squared: 0.1209\n",
      "Epoch 71/300\n",
      "24/24 [==============================] - 7s 289ms/step - loss: 0.3041 - R_squared: 0.2612 - val_loss: 0.3649 - val_R_squared: 0.1588\n",
      "Epoch 72/300\n",
      "24/24 [==============================] - 7s 276ms/step - loss: 0.3068 - R_squared: 0.2472 - val_loss: 0.3610 - val_R_squared: 0.1674\n",
      "Epoch 73/300\n",
      "24/24 [==============================] - 7s 272ms/step - loss: 0.2965 - R_squared: 0.2770 - val_loss: 0.3656 - val_R_squared: 0.1569\n",
      "Epoch 74/300\n",
      "24/24 [==============================] - 7s 277ms/step - loss: 0.3030 - R_squared: 0.2583 - val_loss: 0.3617 - val_R_squared: 0.1656\n",
      "Epoch 75/300\n",
      "24/24 [==============================] - 7s 280ms/step - loss: 0.2936 - R_squared: 0.2735 - val_loss: 0.3661 - val_R_squared: 0.1510\n",
      "Epoch 76/300\n",
      "24/24 [==============================] - 7s 302ms/step - loss: 0.3014 - R_squared: 0.2654 - val_loss: 0.3632 - val_R_squared: 0.1584\n",
      "Epoch 77/300\n",
      "24/24 [==============================] - 7s 281ms/step - loss: 0.3015 - R_squared: 0.2566 - val_loss: 0.3647 - val_R_squared: 0.1545\n",
      "Epoch 78/300\n",
      "24/24 [==============================] - 7s 276ms/step - loss: 0.2878 - R_squared: 0.2956 - val_loss: 0.3716 - val_R_squared: 0.1394\n",
      "Epoch 79/300\n",
      "24/24 [==============================] - 7s 274ms/step - loss: 0.2859 - R_squared: 0.3005 - val_loss: 0.3687 - val_R_squared: 0.1474\n",
      "Epoch 80/300\n",
      "24/24 [==============================] - 7s 271ms/step - loss: 0.2937 - R_squared: 0.2734 - val_loss: 0.3625 - val_R_squared: 0.1652\n",
      "Epoch 81/300\n",
      "24/24 [==============================] - 7s 276ms/step - loss: 0.2894 - R_squared: 0.2936 - val_loss: 0.3598 - val_R_squared: 0.1689\n",
      "Epoch 82/300\n",
      "24/24 [==============================] - 7s 289ms/step - loss: 0.2796 - R_squared: 0.3178 - val_loss: 0.3657 - val_R_squared: 0.1550\n",
      "Epoch 83/300\n",
      "24/24 [==============================] - 7s 285ms/step - loss: 0.2993 - R_squared: 0.2669 - val_loss: 0.3712 - val_R_squared: 0.1436\n",
      "Epoch 84/300\n",
      "24/24 [==============================] - 7s 272ms/step - loss: 0.2877 - R_squared: 0.2978 - val_loss: 0.3638 - val_R_squared: 0.1620\n",
      "Epoch 85/300\n",
      "24/24 [==============================] - 7s 270ms/step - loss: 0.2889 - R_squared: 0.2964 - val_loss: 0.3720 - val_R_squared: 0.1427\n",
      "Epoch 86/300\n",
      "24/24 [==============================] - 7s 292ms/step - loss: 0.2835 - R_squared: 0.3057 - val_loss: 0.3624 - val_R_squared: 0.1627\n",
      "Epoch 87/300\n",
      "24/24 [==============================] - 8s 323ms/step - loss: 0.2834 - R_squared: 0.3091 - val_loss: 0.3565 - val_R_squared: 0.1708\n",
      "Epoch 88/300\n",
      "24/24 [==============================] - 7s 302ms/step - loss: 0.2794 - R_squared: 0.3155 - val_loss: 0.3603 - val_R_squared: 0.1649\n",
      "Epoch 89/300\n",
      "24/24 [==============================] - 8s 324ms/step - loss: 0.2805 - R_squared: 0.3154 - val_loss: 0.3653 - val_R_squared: 0.1544\n",
      "Epoch 90/300\n",
      "24/24 [==============================] - 8s 325ms/step - loss: 0.2871 - R_squared: 0.2985 - val_loss: 0.3653 - val_R_squared: 0.1539\n",
      "Epoch 91/300\n",
      "24/24 [==============================] - 7s 298ms/step - loss: 0.2785 - R_squared: 0.3187 - val_loss: 0.3704 - val_R_squared: 0.1439\n",
      "Epoch 92/300\n",
      "24/24 [==============================] - 8s 316ms/step - loss: 0.2770 - R_squared: 0.3259 - val_loss: 0.3671 - val_R_squared: 0.1498\n",
      "Epoch 93/300\n",
      "24/24 [==============================] - 8s 315ms/step - loss: 0.2746 - R_squared: 0.3229 - val_loss: 0.3712 - val_R_squared: 0.1403\n",
      "Epoch 94/300\n",
      "24/24 [==============================] - 8s 315ms/step - loss: 0.2854 - R_squared: 0.3026 - val_loss: 0.3664 - val_R_squared: 0.1495\n",
      "Epoch 95/300\n",
      "24/24 [==============================] - 7s 281ms/step - loss: 0.2825 - R_squared: 0.3107 - val_loss: 0.3649 - val_R_squared: 0.1577\n",
      "Epoch 96/300\n",
      "24/24 [==============================] - 9s 344ms/step - loss: 0.2873 - R_squared: 0.2937 - val_loss: 0.3673 - val_R_squared: 0.1538\n",
      "Epoch 97/300\n",
      "24/24 [==============================] - 8s 325ms/step - loss: 0.2667 - R_squared: 0.3400 - val_loss: 0.3694 - val_R_squared: 0.1485\n",
      "Epoch 98/300\n",
      "24/24 [==============================] - 7s 282ms/step - loss: 0.2708 - R_squared: 0.3422 - val_loss: 0.3696 - val_R_squared: 0.1483\n",
      "Epoch 99/300\n",
      "24/24 [==============================] - 7s 291ms/step - loss: 0.2634 - R_squared: 0.3544 - val_loss: 0.3705 - val_R_squared: 0.1453\n",
      "Epoch 100/300\n",
      "24/24 [==============================] - 8s 311ms/step - loss: 0.2568 - R_squared: 0.3730 - val_loss: 0.3702 - val_R_squared: 0.1450\n",
      "Epoch 101/300\n",
      "24/24 [==============================] - 8s 315ms/step - loss: 0.2588 - R_squared: 0.3682 - val_loss: 0.3746 - val_R_squared: 0.1354\n",
      "Epoch 102/300\n",
      "24/24 [==============================] - 10s 387ms/step - loss: 0.2584 - R_squared: 0.3690 - val_loss: 0.3640 - val_R_squared: 0.1599\n",
      "Epoch 103/300\n",
      "24/24 [==============================] - 8s 311ms/step - loss: 0.2626 - R_squared: 0.3589 - val_loss: 0.3617 - val_R_squared: 0.1654\n",
      "Epoch 104/300\n",
      "24/24 [==============================] - 9s 364ms/step - loss: 0.2583 - R_squared: 0.3689 - val_loss: 0.3713 - val_R_squared: 0.1403\n",
      "Epoch 105/300\n",
      "24/24 [==============================] - 7s 283ms/step - loss: 0.2595 - R_squared: 0.3663 - val_loss: 0.3709 - val_R_squared: 0.1422\n",
      "Epoch 106/300\n",
      "24/24 [==============================] - 7s 274ms/step - loss: 0.2487 - R_squared: 0.3912 - val_loss: 0.3726 - val_R_squared: 0.1390\n",
      "Epoch 107/300\n",
      "24/24 [==============================] - 7s 279ms/step - loss: 0.2732 - R_squared: 0.3307 - val_loss: 0.3660 - val_R_squared: 0.1539\n",
      "Epoch 108/300\n",
      "24/24 [==============================] - 7s 286ms/step - loss: 0.2689 - R_squared: 0.3377 - val_loss: 0.3635 - val_R_squared: 0.1606\n",
      "Epoch 109/300\n",
      "24/24 [==============================] - 7s 277ms/step - loss: 0.2630 - R_squared: 0.3548 - val_loss: 0.3589 - val_R_squared: 0.1703\n",
      "Epoch 110/300\n",
      "24/24 [==============================] - 7s 272ms/step - loss: 0.2595 - R_squared: 0.3618 - val_loss: 0.3638 - val_R_squared: 0.1594\n",
      "Epoch 111/300\n",
      "24/24 [==============================] - 7s 282ms/step - loss: 0.2720 - R_squared: 0.3340 - val_loss: 0.3588 - val_R_squared: 0.1695\n",
      "Epoch 112/300\n",
      "24/24 [==============================] - 7s 279ms/step - loss: 0.2589 - R_squared: 0.3632 - val_loss: 0.3607 - val_R_squared: 0.1657\n",
      "Epoch 113/300\n",
      "24/24 [==============================] - 7s 279ms/step - loss: 0.2489 - R_squared: 0.3884 - val_loss: 0.3642 - val_R_squared: 0.1561\n",
      "Epoch 114/300\n",
      "24/24 [==============================] - 9s 353ms/step - loss: 0.2549 - R_squared: 0.3750 - val_loss: 0.3661 - val_R_squared: 0.1541\n",
      "Epoch 115/300\n",
      "24/24 [==============================] - 7s 296ms/step - loss: 0.2558 - R_squared: 0.3789 - val_loss: 0.3624 - val_R_squared: 0.1628\n",
      "Epoch 116/300\n",
      "24/24 [==============================] - 8s 305ms/step - loss: 0.2344 - R_squared: 0.4243 - val_loss: 0.3588 - val_R_squared: 0.1713\n",
      "Epoch 117/300\n",
      "24/24 [==============================] - 7s 283ms/step - loss: 0.2550 - R_squared: 0.3676 - val_loss: 0.3622 - val_R_squared: 0.1626\n",
      "Epoch 118/300\n",
      "24/24 [==============================] - 7s 276ms/step - loss: 0.2475 - R_squared: 0.3976 - val_loss: 0.3580 - val_R_squared: 0.1712\n",
      "Epoch 119/300\n",
      "24/24 [==============================] - 7s 285ms/step - loss: 0.2504 - R_squared: 0.3926 - val_loss: 0.3564 - val_R_squared: 0.1749\n",
      "Epoch 120/300\n",
      "24/24 [==============================] - 7s 300ms/step - loss: 0.2433 - R_squared: 0.4124 - val_loss: 0.3572 - val_R_squared: 0.1735\n",
      "Epoch 121/300\n",
      "24/24 [==============================] - 7s 290ms/step - loss: 0.2429 - R_squared: 0.4024 - val_loss: 0.3568 - val_R_squared: 0.1740\n",
      "Epoch 122/300\n",
      "24/24 [==============================] - 7s 287ms/step - loss: 0.2499 - R_squared: 0.3807 - val_loss: 0.3562 - val_R_squared: 0.1758\n",
      "Epoch 123/300\n",
      "24/24 [==============================] - 7s 294ms/step - loss: 0.2565 - R_squared: 0.3716 - val_loss: 0.3591 - val_R_squared: 0.1707\n",
      "Epoch 124/300\n",
      "24/24 [==============================] - 7s 285ms/step - loss: 0.2528 - R_squared: 0.3837 - val_loss: 0.3594 - val_R_squared: 0.1691\n",
      "Epoch 125/300\n",
      "24/24 [==============================] - 8s 308ms/step - loss: 0.2472 - R_squared: 0.3949 - val_loss: 0.3611 - val_R_squared: 0.1647\n",
      "Epoch 126/300\n",
      "24/24 [==============================] - 7s 290ms/step - loss: 0.2454 - R_squared: 0.3910 - val_loss: 0.3599 - val_R_squared: 0.1677\n",
      "Epoch 127/300\n",
      "24/24 [==============================] - 7s 294ms/step - loss: 0.2352 - R_squared: 0.4226 - val_loss: 0.3604 - val_R_squared: 0.1675\n",
      "Epoch 128/300\n",
      "24/24 [==============================] - 7s 301ms/step - loss: 0.2460 - R_squared: 0.3986 - val_loss: 0.3611 - val_R_squared: 0.1665\n",
      "Epoch 129/300\n",
      "24/24 [==============================] - 7s 289ms/step - loss: 0.2505 - R_squared: 0.3865 - val_loss: 0.3650 - val_R_squared: 0.1556\n",
      "Epoch 130/300\n",
      "24/24 [==============================] - 7s 293ms/step - loss: 0.2313 - R_squared: 0.4350 - val_loss: 0.3619 - val_R_squared: 0.1627\n",
      "Epoch 131/300\n",
      "24/24 [==============================] - 7s 301ms/step - loss: 0.2451 - R_squared: 0.3980 - val_loss: 0.3607 - val_R_squared: 0.1639\n",
      "Epoch 132/300\n",
      "24/24 [==============================] - 7s 295ms/step - loss: 0.2359 - R_squared: 0.4201 - val_loss: 0.3647 - val_R_squared: 0.1555\n",
      "Epoch 133/300\n",
      "24/24 [==============================] - 7s 293ms/step - loss: 0.2388 - R_squared: 0.4153 - val_loss: 0.3685 - val_R_squared: 0.1470\n",
      "Epoch 134/300\n",
      "24/24 [==============================] - 7s 293ms/step - loss: 0.2440 - R_squared: 0.4001 - val_loss: 0.3667 - val_R_squared: 0.1508\n",
      "Epoch 135/300\n",
      "24/24 [==============================] - 7s 288ms/step - loss: 0.2331 - R_squared: 0.4315 - val_loss: 0.3637 - val_R_squared: 0.1569\n",
      "Epoch 136/300\n",
      "24/24 [==============================] - 7s 292ms/step - loss: 0.2300 - R_squared: 0.4370 - val_loss: 0.3619 - val_R_squared: 0.1617\n",
      "Epoch 137/300\n",
      "24/24 [==============================] - 8s 303ms/step - loss: 0.2401 - R_squared: 0.4147 - val_loss: 0.3624 - val_R_squared: 0.1604\n",
      "Epoch 138/300\n",
      "24/24 [==============================] - 7s 290ms/step - loss: 0.2397 - R_squared: 0.4158 - val_loss: 0.3632 - val_R_squared: 0.1587\n",
      "Epoch 139/300\n",
      "24/24 [==============================] - 7s 294ms/step - loss: 0.2347 - R_squared: 0.4249 - val_loss: 0.3641 - val_R_squared: 0.1576\n",
      "Epoch 140/300\n",
      "24/24 [==============================] - 7s 293ms/step - loss: 0.2412 - R_squared: 0.4062 - val_loss: 0.3630 - val_R_squared: 0.1591\n",
      "Epoch 141/300\n",
      "24/24 [==============================] - 7s 297ms/step - loss: 0.2520 - R_squared: 0.3871 - val_loss: 0.3649 - val_R_squared: 0.1550\n",
      "Epoch 142/300\n",
      "24/24 [==============================] - 7s 297ms/step - loss: 0.2352 - R_squared: 0.4252 - val_loss: 0.3625 - val_R_squared: 0.1602\n",
      "Epoch 143/300\n",
      "24/24 [==============================] - 7s 296ms/step - loss: 0.2345 - R_squared: 0.4235 - val_loss: 0.3638 - val_R_squared: 0.1570\n",
      "Epoch 144/300\n",
      "24/24 [==============================] - 7s 297ms/step - loss: 0.2393 - R_squared: 0.4148 - val_loss: 0.3623 - val_R_squared: 0.1601\n",
      "Epoch 145/300\n",
      "24/24 [==============================] - 8s 304ms/step - loss: 0.2321 - R_squared: 0.4329 - val_loss: 0.3623 - val_R_squared: 0.1602\n",
      "Epoch 146/300\n",
      "24/24 [==============================] - 7s 303ms/step - loss: 0.2333 - R_squared: 0.4333 - val_loss: 0.3620 - val_R_squared: 0.1617\n",
      "Epoch 147/300\n",
      "24/24 [==============================] - 7s 298ms/step - loss: 0.2402 - R_squared: 0.4134 - val_loss: 0.3612 - val_R_squared: 0.1641\n",
      "Epoch 148/300\n",
      "24/24 [==============================] - 8s 305ms/step - loss: 0.2343 - R_squared: 0.4280 - val_loss: 0.3610 - val_R_squared: 0.1642\n",
      "Epoch 149/300\n",
      "24/24 [==============================] - 8s 307ms/step - loss: 0.2355 - R_squared: 0.4236 - val_loss: 0.3617 - val_R_squared: 0.1621\n",
      "Epoch 150/300\n",
      "24/24 [==============================] - 8s 326ms/step - loss: 0.2258 - R_squared: 0.4489 - val_loss: 0.3646 - val_R_squared: 0.1557\n",
      "Epoch 151/300\n",
      "24/24 [==============================] - 7s 302ms/step - loss: 0.2295 - R_squared: 0.4362 - val_loss: 0.3638 - val_R_squared: 0.1575\n",
      "Epoch 152/300\n",
      "24/24 [==============================] - 8s 311ms/step - loss: 0.2311 - R_squared: 0.4316 - val_loss: 0.3654 - val_R_squared: 0.1547\n",
      "Epoch 153/300\n",
      "24/24 [==============================] - 7s 304ms/step - loss: 0.2347 - R_squared: 0.4256 - val_loss: 0.3633 - val_R_squared: 0.1592\n",
      "Epoch 154/300\n",
      "24/24 [==============================] - 7s 302ms/step - loss: 0.2356 - R_squared: 0.4159 - val_loss: 0.3620 - val_R_squared: 0.1622\n",
      "Epoch 155/300\n",
      "24/24 [==============================] - 8s 309ms/step - loss: 0.2273 - R_squared: 0.4454 - val_loss: 0.3632 - val_R_squared: 0.1599\n",
      "Epoch 156/300\n",
      "24/24 [==============================] - 8s 315ms/step - loss: 0.2355 - R_squared: 0.4217 - val_loss: 0.3638 - val_R_squared: 0.1581\n",
      "Epoch 157/300\n",
      "24/24 [==============================] - 8s 306ms/step - loss: 0.2334 - R_squared: 0.4314 - val_loss: 0.3647 - val_R_squared: 0.1556\n",
      "Epoch 158/300\n",
      "24/24 [==============================] - 7s 306ms/step - loss: 0.2357 - R_squared: 0.4243 - val_loss: 0.3628 - val_R_squared: 0.1596\n",
      "Epoch 159/300\n",
      "24/24 [==============================] - 9s 362ms/step - loss: 0.2385 - R_squared: 0.4162 - val_loss: 0.3611 - val_R_squared: 0.1631\n",
      "Epoch 160/300\n",
      "24/24 [==============================] - 8s 321ms/step - loss: 0.2199 - R_squared: 0.4661 - val_loss: 0.3601 - val_R_squared: 0.1647\n",
      "Epoch 161/300\n",
      "24/24 [==============================] - 8s 310ms/step - loss: 0.2281 - R_squared: 0.4414 - val_loss: 0.3590 - val_R_squared: 0.1670\n",
      "Epoch 162/300\n",
      "24/24 [==============================] - 7s 299ms/step - loss: 0.2259 - R_squared: 0.4413 - val_loss: 0.3587 - val_R_squared: 0.1680\n",
      "Epoch 163/300\n",
      "24/24 [==============================] - 8s 314ms/step - loss: 0.2243 - R_squared: 0.4516 - val_loss: 0.3592 - val_R_squared: 0.1674\n",
      "Epoch 164/300\n",
      "24/24 [==============================] - 7s 301ms/step - loss: 0.2220 - R_squared: 0.4606 - val_loss: 0.3608 - val_R_squared: 0.1641\n",
      "Epoch 165/300\n",
      "24/24 [==============================] - 7s 304ms/step - loss: 0.2221 - R_squared: 0.4576 - val_loss: 0.3601 - val_R_squared: 0.1657\n",
      "Epoch 166/300\n",
      "24/24 [==============================] - 7s 300ms/step - loss: 0.2294 - R_squared: 0.4413 - val_loss: 0.3602 - val_R_squared: 0.1653\n",
      "Epoch 167/300\n",
      "24/24 [==============================] - 8s 308ms/step - loss: 0.2266 - R_squared: 0.4435 - val_loss: 0.3621 - val_R_squared: 0.1606\n",
      "Epoch 168/300\n",
      "24/24 [==============================] - 7s 298ms/step - loss: 0.2209 - R_squared: 0.4612 - val_loss: 0.3620 - val_R_squared: 0.1610\n",
      "Epoch 169/300\n",
      "24/24 [==============================] - 8s 338ms/step - loss: 0.2285 - R_squared: 0.4439 - val_loss: 0.3609 - val_R_squared: 0.1633\n",
      "Epoch 170/300\n",
      "24/24 [==============================] - 8s 309ms/step - loss: 0.2322 - R_squared: 0.4324 - val_loss: 0.3603 - val_R_squared: 0.1646\n",
      "Epoch 171/300\n",
      "24/24 [==============================] - 7s 298ms/step - loss: 0.2196 - R_squared: 0.4586 - val_loss: 0.3595 - val_R_squared: 0.1668\n",
      "Epoch 172/300\n",
      "24/24 [==============================] - 7s 295ms/step - loss: 0.2348 - R_squared: 0.4245 - val_loss: 0.3600 - val_R_squared: 0.1658\n",
      "Epoch 173/300\n",
      "24/24 [==============================] - 8s 313ms/step - loss: 0.2192 - R_squared: 0.4675 - val_loss: 0.3608 - val_R_squared: 0.1641\n",
      "Epoch 174/300\n",
      "24/24 [==============================] - 8s 314ms/step - loss: 0.2245 - R_squared: 0.4494 - val_loss: 0.3600 - val_R_squared: 0.1662\n",
      "Epoch 175/300\n",
      "24/24 [==============================] - 7s 295ms/step - loss: 0.2302 - R_squared: 0.4356 - val_loss: 0.3597 - val_R_squared: 0.1668\n",
      "Epoch 176/300\n",
      "24/24 [==============================] - 7s 304ms/step - loss: 0.2265 - R_squared: 0.4507 - val_loss: 0.3606 - val_R_squared: 0.1645\n",
      "Epoch 177/300\n",
      "24/24 [==============================] - 9s 353ms/step - loss: 0.2234 - R_squared: 0.4532 - val_loss: 0.3607 - val_R_squared: 0.1646\n",
      "Epoch 178/300\n",
      "24/24 [==============================] - 8s 305ms/step - loss: 0.2231 - R_squared: 0.4520 - val_loss: 0.3600 - val_R_squared: 0.1657\n",
      "Epoch 179/300\n",
      "24/24 [==============================] - 7s 298ms/step - loss: 0.2198 - R_squared: 0.4611 - val_loss: 0.3598 - val_R_squared: 0.1662\n",
      "Epoch 180/300\n",
      "24/24 [==============================] - 7s 304ms/step - loss: 0.2272 - R_squared: 0.4403 - val_loss: 0.3584 - val_R_squared: 0.1688\n",
      "Epoch 181/300\n",
      "24/24 [==============================] - 8s 317ms/step - loss: 0.2271 - R_squared: 0.4471 - val_loss: 0.3583 - val_R_squared: 0.1696\n",
      "Epoch 182/300\n",
      "24/24 [==============================] - 8s 318ms/step - loss: 0.2217 - R_squared: 0.4639 - val_loss: 0.3586 - val_R_squared: 0.1691\n",
      "Epoch 183/300\n",
      "24/24 [==============================] - 7s 297ms/step - loss: 0.2213 - R_squared: 0.4583 - val_loss: 0.3595 - val_R_squared: 0.1671\n",
      "Epoch 184/300\n",
      "24/24 [==============================] - 7s 300ms/step - loss: 0.2125 - R_squared: 0.4762 - val_loss: 0.3600 - val_R_squared: 0.1660\n",
      "Epoch 185/300\n",
      "24/24 [==============================] - 7s 301ms/step - loss: 0.2144 - R_squared: 0.4751 - val_loss: 0.3607 - val_R_squared: 0.1645\n",
      "Epoch 186/300\n",
      "24/24 [==============================] - 8s 311ms/step - loss: 0.2276 - R_squared: 0.4413 - val_loss: 0.3608 - val_R_squared: 0.1641\n",
      "Epoch 187/300\n",
      "24/24 [==============================] - 7s 301ms/step - loss: 0.2201 - R_squared: 0.4596 - val_loss: 0.3603 - val_R_squared: 0.1654\n",
      "Epoch 188/300\n",
      "24/24 [==============================] - 7s 302ms/step - loss: 0.2227 - R_squared: 0.4576 - val_loss: 0.3586 - val_R_squared: 0.1689\n",
      "Epoch 189/300\n",
      "24/24 [==============================] - 7s 301ms/step - loss: 0.2236 - R_squared: 0.4525 - val_loss: 0.3582 - val_R_squared: 0.1701\n",
      "Epoch 190/300\n",
      "24/24 [==============================] - 7s 301ms/step - loss: 0.2161 - R_squared: 0.4709 - val_loss: 0.3591 - val_R_squared: 0.1683\n",
      "Epoch 191/300\n",
      "24/24 [==============================] - 7s 299ms/step - loss: 0.2321 - R_squared: 0.4319 - val_loss: 0.3596 - val_R_squared: 0.1667\n",
      "Epoch 192/300\n",
      "24/24 [==============================] - 8s 306ms/step - loss: 0.2243 - R_squared: 0.4499 - val_loss: 0.3591 - val_R_squared: 0.1674\n",
      "Epoch 193/300\n",
      "24/24 [==============================] - 7s 301ms/step - loss: 0.2116 - R_squared: 0.4853 - val_loss: 0.3593 - val_R_squared: 0.1670\n",
      "Epoch 194/300\n",
      "24/24 [==============================] - 7s 294ms/step - loss: 0.2253 - R_squared: 0.4465 - val_loss: 0.3591 - val_R_squared: 0.1675\n",
      "Epoch 195/300\n",
      "24/24 [==============================] - 7s 297ms/step - loss: 0.2261 - R_squared: 0.4465 - val_loss: 0.3585 - val_R_squared: 0.1688\n",
      "Epoch 196/300\n",
      "24/24 [==============================] - 7s 304ms/step - loss: 0.2180 - R_squared: 0.4686 - val_loss: 0.3587 - val_R_squared: 0.1682\n",
      "Epoch 197/300\n",
      "24/24 [==============================] - 7s 301ms/step - loss: 0.2173 - R_squared: 0.4691 - val_loss: 0.3584 - val_R_squared: 0.1687\n",
      "Epoch 198/300\n",
      "24/24 [==============================] - 8s 305ms/step - loss: 0.2176 - R_squared: 0.4673 - val_loss: 0.3582 - val_R_squared: 0.1691\n",
      "Epoch 199/300\n",
      "24/24 [==============================] - 7s 299ms/step - loss: 0.2230 - R_squared: 0.4558 - val_loss: 0.3585 - val_R_squared: 0.1685\n",
      "Epoch 200/300\n",
      "24/24 [==============================] - 7s 298ms/step - loss: 0.2199 - R_squared: 0.4637 - val_loss: 0.3592 - val_R_squared: 0.1673\n",
      "Epoch 201/300\n",
      "24/24 [==============================] - 8s 308ms/step - loss: 0.2165 - R_squared: 0.4747 - val_loss: 0.3591 - val_R_squared: 0.1674\n",
      "Epoch 202/300\n",
      "24/24 [==============================] - 7s 301ms/step - loss: 0.2301 - R_squared: 0.4338 - val_loss: 0.3590 - val_R_squared: 0.1675\n",
      "Epoch 203/300\n",
      "24/24 [==============================] - 7s 304ms/step - loss: 0.2165 - R_squared: 0.4741 - val_loss: 0.3594 - val_R_squared: 0.1663\n",
      "Epoch 204/300\n",
      "24/24 [==============================] - 7s 302ms/step - loss: 0.2294 - R_squared: 0.4394 - val_loss: 0.3595 - val_R_squared: 0.1662\n",
      "Epoch 205/300\n",
      "24/24 [==============================] - 8s 306ms/step - loss: 0.2190 - R_squared: 0.4646 - val_loss: 0.3592 - val_R_squared: 0.1670\n",
      "Epoch 206/300\n",
      "24/24 [==============================] - 7s 296ms/step - loss: 0.2055 - R_squared: 0.4978 - val_loss: 0.3593 - val_R_squared: 0.1670\n",
      "Epoch 207/300\n",
      "24/24 [==============================] - 7s 304ms/step - loss: 0.2076 - R_squared: 0.4935 - val_loss: 0.3590 - val_R_squared: 0.1678\n",
      "Epoch 208/300\n",
      "24/24 [==============================] - 8s 307ms/step - loss: 0.2001 - R_squared: 0.5146 - val_loss: 0.3591 - val_R_squared: 0.1678\n",
      "Epoch 209/300\n",
      "24/24 [==============================] - 8s 320ms/step - loss: 0.2225 - R_squared: 0.4569 - val_loss: 0.3594 - val_R_squared: 0.1670\n",
      "Epoch 210/300\n",
      "24/24 [==============================] - 7s 306ms/step - loss: 0.2278 - R_squared: 0.4415 - val_loss: 0.3590 - val_R_squared: 0.1678\n",
      "Epoch 211/300\n",
      "24/24 [==============================] - 8s 318ms/step - loss: 0.2126 - R_squared: 0.4822 - val_loss: 0.3588 - val_R_squared: 0.1684\n",
      "Epoch 212/300\n",
      "24/24 [==============================] - 7s 298ms/step - loss: 0.2203 - R_squared: 0.4632 - val_loss: 0.3587 - val_R_squared: 0.1687\n",
      "Epoch 213/300\n",
      "24/24 [==============================] - 8s 307ms/step - loss: 0.2143 - R_squared: 0.4761 - val_loss: 0.3587 - val_R_squared: 0.1685\n",
      "Epoch 214/300\n",
      "24/24 [==============================] - 7s 300ms/step - loss: 0.2175 - R_squared: 0.4702 - val_loss: 0.3592 - val_R_squared: 0.1673\n",
      "Epoch 215/300\n",
      "24/24 [==============================] - 8s 316ms/step - loss: 0.2172 - R_squared: 0.4632 - val_loss: 0.3594 - val_R_squared: 0.1670\n",
      "Epoch 216/300\n",
      "24/24 [==============================] - 8s 317ms/step - loss: 0.2164 - R_squared: 0.4710 - val_loss: 0.3593 - val_R_squared: 0.1672\n",
      "Epoch 217/300\n",
      "24/24 [==============================] - 8s 310ms/step - loss: 0.2276 - R_squared: 0.4470 - val_loss: 0.3589 - val_R_squared: 0.1680\n",
      "Epoch 218/300\n",
      "24/24 [==============================] - 8s 320ms/step - loss: 0.2145 - R_squared: 0.4778 - val_loss: 0.3588 - val_R_squared: 0.1681\n",
      "Epoch 219/300\n",
      "24/24 [==============================] - 7s 303ms/step - loss: 0.2149 - R_squared: 0.4714 - val_loss: 0.3591 - val_R_squared: 0.1675\n",
      "Epoch 220/300\n",
      "24/24 [==============================] - 8s 322ms/step - loss: 0.2198 - R_squared: 0.4648 - val_loss: 0.3594 - val_R_squared: 0.1668\n",
      "Epoch 221/300\n",
      "24/24 [==============================] - 8s 313ms/step - loss: 0.2267 - R_squared: 0.4437 - val_loss: 0.3601 - val_R_squared: 0.1649\n",
      "Epoch 222/300\n",
      "24/24 [==============================] - 8s 308ms/step - loss: 0.2164 - R_squared: 0.4727 - val_loss: 0.3606 - val_R_squared: 0.1639\n",
      "Epoch 223/300\n",
      "24/24 [==============================] - 8s 312ms/step - loss: 0.2131 - R_squared: 0.4765 - val_loss: 0.3612 - val_R_squared: 0.1624\n",
      "Epoch 224/300\n",
      "24/24 [==============================] - 8s 307ms/step - loss: 0.2213 - R_squared: 0.4583 - val_loss: 0.3609 - val_R_squared: 0.1630\n",
      "Epoch 225/300\n",
      "24/24 [==============================] - 8s 312ms/step - loss: 0.2103 - R_squared: 0.4875 - val_loss: 0.3608 - val_R_squared: 0.1633\n",
      "Epoch 226/300\n",
      "24/24 [==============================] - 8s 308ms/step - loss: 0.2148 - R_squared: 0.4748 - val_loss: 0.3607 - val_R_squared: 0.1638\n",
      "Epoch 227/300\n",
      "24/24 [==============================] - 7s 304ms/step - loss: 0.2104 - R_squared: 0.4878 - val_loss: 0.3605 - val_R_squared: 0.1640\n",
      "Epoch 228/300\n",
      "24/24 [==============================] - 7s 301ms/step - loss: 0.2179 - R_squared: 0.4656 - val_loss: 0.3603 - val_R_squared: 0.1649\n",
      "Epoch 229/300\n",
      "24/24 [==============================] - 7s 295ms/step - loss: 0.2221 - R_squared: 0.4522 - val_loss: 0.3601 - val_R_squared: 0.1654\n",
      "Epoch 230/300\n",
      "24/24 [==============================] - 8s 317ms/step - loss: 0.2208 - R_squared: 0.4634 - val_loss: 0.3603 - val_R_squared: 0.1650\n",
      "Epoch 231/300\n",
      "24/24 [==============================] - 7s 304ms/step - loss: 0.2177 - R_squared: 0.4673 - val_loss: 0.3605 - val_R_squared: 0.1647\n",
      "Epoch 232/300\n",
      "24/24 [==============================] - 8s 313ms/step - loss: 0.2267 - R_squared: 0.4437 - val_loss: 0.3603 - val_R_squared: 0.1650\n",
      "Epoch 233/300\n",
      "24/24 [==============================] - 8s 311ms/step - loss: 0.2131 - R_squared: 0.4803 - val_loss: 0.3602 - val_R_squared: 0.1652\n",
      "Epoch 234/300\n",
      "24/24 [==============================] - 8s 322ms/step - loss: 0.2211 - R_squared: 0.4581 - val_loss: 0.3601 - val_R_squared: 0.1653\n",
      "Epoch 235/300\n",
      "24/24 [==============================] - 8s 308ms/step - loss: 0.2099 - R_squared: 0.4858 - val_loss: 0.3600 - val_R_squared: 0.1657\n",
      "Epoch 236/300\n",
      "24/24 [==============================] - 8s 302ms/step - loss: 0.2266 - R_squared: 0.4409 - val_loss: 0.3596 - val_R_squared: 0.1665\n",
      "Epoch 237/300\n",
      "24/24 [==============================] - 7s 301ms/step - loss: 0.2189 - R_squared: 0.4648 - val_loss: 0.3603 - val_R_squared: 0.1651\n",
      "Epoch 238/300\n",
      "24/24 [==============================] - 8s 316ms/step - loss: 0.2148 - R_squared: 0.4712 - val_loss: 0.3600 - val_R_squared: 0.1659\n",
      "Epoch 239/300\n",
      "24/24 [==============================] - 8s 341ms/step - loss: 0.2211 - R_squared: 0.4585 - val_loss: 0.3598 - val_R_squared: 0.1664\n",
      "Epoch 240/300\n",
      "24/24 [==============================] - 8s 307ms/step - loss: 0.2208 - R_squared: 0.4580 - val_loss: 0.3599 - val_R_squared: 0.1661\n",
      "Epoch 241/300\n",
      "24/24 [==============================] - 7s 307ms/step - loss: 0.2229 - R_squared: 0.4608 - val_loss: 0.3600 - val_R_squared: 0.1658\n",
      "Epoch 242/300\n",
      "24/24 [==============================] - 7s 301ms/step - loss: 0.2074 - R_squared: 0.4910 - val_loss: 0.3600 - val_R_squared: 0.1659\n",
      "Epoch 243/300\n",
      "24/24 [==============================] - 8s 312ms/step - loss: 0.2305 - R_squared: 0.4375 - val_loss: 0.3601 - val_R_squared: 0.1658\n",
      "Epoch 244/300\n",
      "24/24 [==============================] - 8s 316ms/step - loss: 0.2103 - R_squared: 0.4819 - val_loss: 0.3597 - val_R_squared: 0.1664\n",
      "Epoch 245/300\n",
      "24/24 [==============================] - 7s 300ms/step - loss: 0.2088 - R_squared: 0.4900 - val_loss: 0.3599 - val_R_squared: 0.1661\n",
      "Epoch 246/300\n",
      "24/24 [==============================] - 8s 319ms/step - loss: 0.2250 - R_squared: 0.4524 - val_loss: 0.3600 - val_R_squared: 0.1658\n",
      "Epoch 247/300\n",
      "24/24 [==============================] - 8s 303ms/step - loss: 0.2109 - R_squared: 0.4870 - val_loss: 0.3601 - val_R_squared: 0.1656\n",
      "Epoch 248/300\n",
      "24/24 [==============================] - 8s 311ms/step - loss: 0.2209 - R_squared: 0.4544 - val_loss: 0.3596 - val_R_squared: 0.1664\n",
      "Epoch 249/300\n",
      "24/24 [==============================] - 8s 306ms/step - loss: 0.2122 - R_squared: 0.4770 - val_loss: 0.3597 - val_R_squared: 0.1663\n",
      "Epoch 250/300\n",
      "24/24 [==============================] - 8s 303ms/step - loss: 0.2046 - R_squared: 0.5048 - val_loss: 0.3600 - val_R_squared: 0.1655\n",
      "Epoch 251/300\n",
      "24/24 [==============================] - 8s 310ms/step - loss: 0.2271 - R_squared: 0.4419 - val_loss: 0.3599 - val_R_squared: 0.1658\n",
      "Epoch 252/300\n",
      "24/24 [==============================] - 7s 303ms/step - loss: 0.2224 - R_squared: 0.4528 - val_loss: 0.3598 - val_R_squared: 0.1661\n",
      "Epoch 253/300\n",
      "24/24 [==============================] - 7s 302ms/step - loss: 0.2176 - R_squared: 0.4651 - val_loss: 0.3597 - val_R_squared: 0.1663\n",
      "Epoch 254/300\n",
      "24/24 [==============================] - 7s 305ms/step - loss: 0.2153 - R_squared: 0.4769 - val_loss: 0.3595 - val_R_squared: 0.1668\n",
      "Epoch 255/300\n",
      "24/24 [==============================] - 11s 470ms/step - loss: 0.2190 - R_squared: 0.4603 - val_loss: 0.3592 - val_R_squared: 0.1676\n",
      "Epoch 256/300\n",
      "24/24 [==============================] - 8s 330ms/step - loss: 0.2145 - R_squared: 0.4748 - val_loss: 0.3588 - val_R_squared: 0.1683\n",
      "Epoch 257/300\n",
      "24/24 [==============================] - 8s 330ms/step - loss: 0.2096 - R_squared: 0.4829 - val_loss: 0.3591 - val_R_squared: 0.1676\n",
      "Epoch 258/300\n",
      "24/24 [==============================] - 9s 387ms/step - loss: 0.2183 - R_squared: 0.4649 - val_loss: 0.3591 - val_R_squared: 0.1677\n",
      "Epoch 259/300\n",
      "24/24 [==============================] - 9s 354ms/step - loss: 0.2226 - R_squared: 0.4545 - val_loss: 0.3592 - val_R_squared: 0.1676\n",
      "Epoch 260/300\n",
      "24/24 [==============================] - 9s 361ms/step - loss: 0.2126 - R_squared: 0.4852 - val_loss: 0.3588 - val_R_squared: 0.1686\n",
      "Epoch 261/300\n",
      "24/24 [==============================] - 8s 340ms/step - loss: 0.2234 - R_squared: 0.4508 - val_loss: 0.3589 - val_R_squared: 0.1683\n",
      "Epoch 262/300\n",
      "24/24 [==============================] - 8s 342ms/step - loss: 0.2125 - R_squared: 0.4776 - val_loss: 0.3591 - val_R_squared: 0.1678\n",
      "Epoch 263/300\n",
      "24/24 [==============================] - 9s 357ms/step - loss: 0.2148 - R_squared: 0.4790 - val_loss: 0.3589 - val_R_squared: 0.1681\n",
      "Epoch 264/300\n",
      "24/24 [==============================] - 9s 348ms/step - loss: 0.2080 - R_squared: 0.4904 - val_loss: 0.3590 - val_R_squared: 0.1679\n",
      "Epoch 265/300\n",
      "24/24 [==============================] - 8s 332ms/step - loss: 0.2302 - R_squared: 0.4402 - val_loss: 0.3590 - val_R_squared: 0.1676\n",
      "Epoch 266/300\n",
      "24/24 [==============================] - 8s 340ms/step - loss: 0.2094 - R_squared: 0.4883 - val_loss: 0.3591 - val_R_squared: 0.1675\n",
      "Epoch 267/300\n",
      "24/24 [==============================] - 9s 361ms/step - loss: 0.2191 - R_squared: 0.4637 - val_loss: 0.3592 - val_R_squared: 0.1673\n",
      "Epoch 268/300\n",
      "24/24 [==============================] - 9s 353ms/step - loss: 0.2171 - R_squared: 0.4662 - val_loss: 0.3591 - val_R_squared: 0.1674\n",
      "Epoch 269/300\n",
      "24/24 [==============================] - 9s 366ms/step - loss: 0.2125 - R_squared: 0.4811 - val_loss: 0.3591 - val_R_squared: 0.1675\n",
      "Epoch 270/300\n",
      "24/24 [==============================] - 9s 387ms/step - loss: 0.2259 - R_squared: 0.4511 - val_loss: 0.3595 - val_R_squared: 0.1665\n",
      "Epoch 271/300\n",
      "24/24 [==============================] - 8s 337ms/step - loss: 0.2119 - R_squared: 0.4812 - val_loss: 0.3598 - val_R_squared: 0.1660\n",
      "Epoch 272/300\n",
      "24/24 [==============================] - 8s 338ms/step - loss: 0.2116 - R_squared: 0.4835 - val_loss: 0.3596 - val_R_squared: 0.1663\n",
      "Epoch 273/300\n",
      "24/24 [==============================] - 9s 355ms/step - loss: 0.2205 - R_squared: 0.4640 - val_loss: 0.3593 - val_R_squared: 0.1669\n",
      "Epoch 274/300\n",
      "24/24 [==============================] - 8s 348ms/step - loss: 0.2217 - R_squared: 0.4594 - val_loss: 0.3595 - val_R_squared: 0.1666\n",
      "Epoch 275/300\n",
      "24/24 [==============================] - 8s 335ms/step - loss: 0.2148 - R_squared: 0.4749 - val_loss: 0.3594 - val_R_squared: 0.1668\n",
      "Epoch 276/300\n",
      "24/24 [==============================] - 8s 340ms/step - loss: 0.2060 - R_squared: 0.4949 - val_loss: 0.3595 - val_R_squared: 0.1665\n",
      "Epoch 277/300\n",
      "24/24 [==============================] - 8s 343ms/step - loss: 0.2136 - R_squared: 0.4737 - val_loss: 0.3594 - val_R_squared: 0.1668\n",
      "Epoch 278/300\n",
      "24/24 [==============================] - 9s 354ms/step - loss: 0.2312 - R_squared: 0.4296 - val_loss: 0.3594 - val_R_squared: 0.1668\n",
      "Epoch 279/300\n",
      "24/24 [==============================] - 9s 353ms/step - loss: 0.2185 - R_squared: 0.4649 - val_loss: 0.3593 - val_R_squared: 0.1670\n",
      "Epoch 280/300\n",
      "24/24 [==============================] - 9s 353ms/step - loss: 0.2131 - R_squared: 0.4808 - val_loss: 0.3591 - val_R_squared: 0.1674\n",
      "Epoch 281/300\n",
      "24/24 [==============================] - 8s 343ms/step - loss: 0.2284 - R_squared: 0.4447 - val_loss: 0.3594 - val_R_squared: 0.1667\n",
      "Epoch 282/300\n",
      "24/24 [==============================] - 8s 340ms/step - loss: 0.2295 - R_squared: 0.4382 - val_loss: 0.3594 - val_R_squared: 0.1669\n",
      "Epoch 283/300\n",
      "24/24 [==============================] - 8s 345ms/step - loss: 0.2170 - R_squared: 0.4675 - val_loss: 0.3593 - val_R_squared: 0.1668\n",
      "Epoch 284/300\n",
      "24/24 [==============================] - 8s 338ms/step - loss: 0.2229 - R_squared: 0.4555 - val_loss: 0.3593 - val_R_squared: 0.1671\n",
      "Epoch 285/300\n",
      "24/24 [==============================] - 10s 392ms/step - loss: 0.2008 - R_squared: 0.5065 - val_loss: 0.3593 - val_R_squared: 0.1670\n",
      "Epoch 286/300\n",
      "24/24 [==============================] - 9s 354ms/step - loss: 0.2167 - R_squared: 0.4681 - val_loss: 0.3593 - val_R_squared: 0.1670\n",
      "Epoch 287/300\n",
      "24/24 [==============================] - 8s 343ms/step - loss: 0.2119 - R_squared: 0.4819 - val_loss: 0.3593 - val_R_squared: 0.1671\n",
      "Epoch 288/300\n",
      "24/24 [==============================] - 9s 368ms/step - loss: 0.2194 - R_squared: 0.4654 - val_loss: 0.3593 - val_R_squared: 0.1669\n",
      "Epoch 289/300\n",
      "24/24 [==============================] - 9s 352ms/step - loss: 0.2096 - R_squared: 0.4879 - val_loss: 0.3594 - val_R_squared: 0.1667\n",
      "Epoch 290/300\n",
      "24/24 [==============================] - 9s 365ms/step - loss: 0.2052 - R_squared: 0.4993 - val_loss: 0.3592 - val_R_squared: 0.1673\n",
      "Epoch 291/300\n",
      "24/24 [==============================] - 8s 346ms/step - loss: 0.2177 - R_squared: 0.4644 - val_loss: 0.3594 - val_R_squared: 0.1668\n",
      "Epoch 292/300\n",
      "24/24 [==============================] - 9s 349ms/step - loss: 0.2208 - R_squared: 0.4603 - val_loss: 0.3596 - val_R_squared: 0.1664\n",
      "Epoch 293/300\n",
      "24/24 [==============================] - 9s 363ms/step - loss: 0.2061 - R_squared: 0.4995 - val_loss: 0.3593 - val_R_squared: 0.1671\n",
      "Epoch 294/300\n",
      "24/24 [==============================] - 9s 363ms/step - loss: 0.2138 - R_squared: 0.4696 - val_loss: 0.3592 - val_R_squared: 0.1673\n",
      "Epoch 295/300\n",
      "24/24 [==============================] - 9s 365ms/step - loss: 0.2142 - R_squared: 0.4791 - val_loss: 0.3591 - val_R_squared: 0.1675\n",
      "Epoch 296/300\n",
      "24/24 [==============================] - 9s 372ms/step - loss: 0.2183 - R_squared: 0.4644 - val_loss: 0.3590 - val_R_squared: 0.1679\n",
      "Epoch 297/300\n",
      "24/24 [==============================] - 9s 347ms/step - loss: 0.2267 - R_squared: 0.4458 - val_loss: 0.3589 - val_R_squared: 0.1680\n",
      "Epoch 298/300\n",
      "24/24 [==============================] - 9s 369ms/step - loss: 0.2047 - R_squared: 0.4905 - val_loss: 0.3589 - val_R_squared: 0.1681\n",
      "Epoch 299/300\n",
      "24/24 [==============================] - 9s 364ms/step - loss: 0.2170 - R_squared: 0.4683 - val_loss: 0.3590 - val_R_squared: 0.1678\n",
      "Epoch 300/300\n",
      "24/24 [==============================] - 9s 369ms/step - loss: 0.2065 - R_squared: 0.4974 - val_loss: 0.3590 - val_R_squared: 0.1679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x3ae14dcd0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# track and save training\n",
    "logdir = \"logs/price/ensemble/final/log_price_atleast4\"\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = logdir+\"/checkpoint\", monitor='val_R_squared', save_best_only=True, mode='max')\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model = Img_model(dropout= 0.2, nodes1 = 512, nodes2 = 1, l2 = 0)\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(0.01, decay_steps=100, decay_rate=0.9, staircase=False)\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr),\n",
    "                loss= \"mse\", metrics= R_squared)\n",
    "model.fit(train_dataset, epochs = 300, validation_data = val_dataset, callbacks = [checkpoint_callback, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18464196],\n",
       "       [0.18135749],\n",
       "       [0.12258715],\n",
       "       [0.13475364],\n",
       "       [0.21524999],\n",
       "       [0.16140978]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final weights to see the contribution - for last epoch\n",
    "model.layers[-1].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-14 16:46:32.039347: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open logs/price/ensemble/final/log_price_atleast4/checkpoint: Failed precondition: logs/price/ensemble/final/log_price_atleast4/checkpoint; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x3b3735580>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload best epoch\n",
    "ckp = \"logs/price/ensemble/final/log_price_atleast4/checkpoint\"\n",
    "model.load_weights(ckp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21551664],\n",
       "       [0.21356234],\n",
       "       [0.1539262 ],\n",
       "       [0.17068407],\n",
       "       [0.24233142],\n",
       "       [0.00397941]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final weights to see the contribution - for best epoch\n",
    "model.layers[-1].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lr}\n",
      "\\toprule\n",
      "{} &    weight \\\\\n",
      "\\midrule\n",
      "bath    &  0.215517 \\\\\n",
      "bed     &  0.213562 \\\\\n",
      "dining  &  0.153926 \\\\\n",
      "hall    &  0.170684 \\\\\n",
      "kitchen &  0.242331 \\\\\n",
      "living  &  0.003979 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols= ['weight']\n",
    "index = [\"bath\", \"bed\", \"dining\", \"hall\", \"kitchen\", \"living\"]\n",
    "\n",
    "weights_rooms = pd.DataFrame(model.layers[-1].get_weights()[0], index=index, columns=cols)\n",
    "\n",
    "print(weights_rooms.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_6b39d_row0_col0 {\n",
       "  background-color: #00682a;\n",
       "  color: #f1f1f1;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_6b39d_row1_col0 {\n",
       "  background-color: #006b2b;\n",
       "  color: #f1f1f1;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_6b39d_row2_col0 {\n",
       "  background-color: #3fa95c;\n",
       "  color: #f1f1f1;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_6b39d_row3_col0 {\n",
       "  background-color: #2f974e;\n",
       "  color: #f1f1f1;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_6b39d_row4_col0 {\n",
       "  background-color: #00441b;\n",
       "  color: #f1f1f1;\n",
       "  font-size: 10px;\n",
       "}\n",
       "#T_6b39d_row5_col0 {\n",
       "  background-color: #f7fcf5;\n",
       "  color: #000000;\n",
       "  font-size: 10px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_6b39d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_6b39d_level0_col0\" class=\"col_heading level0 col0\" >weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6b39d_level0_row0\" class=\"row_heading level0 row0\" >bath</th>\n",
       "      <td id=\"T_6b39d_row0_col0\" class=\"data row0 col0\" >0.215517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6b39d_level0_row1\" class=\"row_heading level0 row1\" >bed</th>\n",
       "      <td id=\"T_6b39d_row1_col0\" class=\"data row1 col0\" >0.213562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6b39d_level0_row2\" class=\"row_heading level0 row2\" >dining</th>\n",
       "      <td id=\"T_6b39d_row2_col0\" class=\"data row2 col0\" >0.153926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6b39d_level0_row3\" class=\"row_heading level0 row3\" >hall</th>\n",
       "      <td id=\"T_6b39d_row3_col0\" class=\"data row3 col0\" >0.170684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6b39d_level0_row4\" class=\"row_heading level0 row4\" >kitchen</th>\n",
       "      <td id=\"T_6b39d_row4_col0\" class=\"data row4 col0\" >0.242331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6b39d_level0_row5\" class=\"row_heading level0 row5\" >living</th>\n",
       "      <td id=\"T_6b39d_row5_col0\" class=\"data row5 col0\" >0.003979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x3ade9ea90>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(model.layers[-1].get_weights()[0], index=Cols, columns=Index).style.background_gradient(cmap ='Greens')\\\n",
    "        .set_properties(**{'font-size': '10px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 3s 366ms/step - loss: 0.3433 - R_squared: 0.1206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.34333011507987976, 0.12056545168161392]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate test data\n",
    "model.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f0e252e38867c5c5dbaa4d840f6f52d4785511881d0242d037299812a0aec8ad"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('tensorflow_m1': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
