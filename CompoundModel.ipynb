{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabnet Image Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import tabnet\n",
    "from load_data import *\n",
    "import os\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "\n",
    "# Display\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only consider a model with the best hyperparameters identified for the individual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for best fold of Tabnet\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = load_data_fold(fold=4, drop_id = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input process for Image Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load paths and labels\n",
    "img_folder = \"data/images_resized\"\n",
    "img_df = pd.read_csv(\"data/img_paths.csv\")\n",
    "img_df.reset_index(drop=True, inplace = True)\n",
    "\n",
    "# load label book\n",
    "label_cat = [\"bathroom\", \"bedroom\", \"dining\", \"hallway\", \"kitchen\", \"living\"]\n",
    "label = np.arange(6)\n",
    "label_book = pd.DataFrame({\"label\": label_cat, \"categorical_label\": label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prices\n",
    "url_listing = \"http://data.insideairbnb.com/ireland/leinster/dublin/2021-11-07/data/listings.csv.gz\"\n",
    "listings = pd.read_csv(url_listing)\n",
    "urls = listings[\"listing_url\"]\n",
    "ids = listings[\"id\"]\n",
    "price = listings[\"price\"]\n",
    "price = price.str.replace(\"$\",\"\")\n",
    "price = price.str.replace(\",\",\"\")\n",
    "price = price.astype(float)\n",
    "listings[\"price\"] = price\n",
    "listings[\"log_price\"] = np.log(price)\n",
    "listings = listings[listings[\"price\"]<500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>id</th>\n",
       "      <th>img_no</th>\n",
       "      <th>label</th>\n",
       "      <th>log_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44077_0.png</td>\n",
       "      <td>44077</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.174387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44077_1.png</td>\n",
       "      <td>44077</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.174387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44077_2.png</td>\n",
       "      <td>44077</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.174387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44077_3.png</td>\n",
       "      <td>44077</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.174387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44077_4.png</td>\n",
       "      <td>44077</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.174387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      img_path     id  img_no  label  log_price\n",
       "0  44077_0.png  44077       0    4.0   4.174387\n",
       "1  44077_1.png  44077       1    5.0   4.174387\n",
       "2  44077_2.png  44077       2    1.0   4.174387\n",
       "3  44077_3.png  44077       3    1.0   4.174387\n",
       "4  44077_4.png  44077       4    1.0   4.174387"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_df = listings[[\"log_price\", \"id\"]]\n",
    "df = pd.merge(img_df, price_df, on = \"id\", how = \"left\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = np.any(df.isna(), axis = 1)\n",
    "df = df[~filter]\n",
    "\n",
    "# drop \"others\"\n",
    "filter = df[\"label\"] == 6.0\n",
    "df = df[~filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool = []\n",
    "for id in df[\"id\"]:\n",
    "    tmp = df[df[\"id\"] == id]\n",
    "    if len(np.unique(tmp[\"label\"])) >= 4:\n",
    "        bool.append(True)\n",
    "    else:\n",
    "        bool.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df[bool]\n",
    "np.unique(df_new[\"id\"].values).shape\n",
    "df = df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56704, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_pipeline(room = 0, df = df):\n",
    "    ids = []\n",
    "    features = []\n",
    "    \n",
    "    # FILTER DF\n",
    "    df_room = df[df[\"label\"] == room]\n",
    "    \n",
    "    # RESNET\n",
    "    resnet = tf.keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', pooling=\"avg\", input_shape = (None,None,3))\n",
    "    resnet_pre = keras.applications.resnet50.preprocess_input\n",
    "    resnet.trainable = False\n",
    "    \n",
    "    for id in tqdm(np.unique(df[\"id\"])):\n",
    "        filter = df_room[\"id\"] == id  \n",
    "\n",
    "        try:\n",
    "            l = []\n",
    "            if filter.sum() == 0:\n",
    "                dummy_image = np.zeros((1,256,256,3))\n",
    "                dummy_image = resnet_pre(dummy_image)\n",
    "                dummy_image = resnet(dummy_image)\n",
    "                l.append(dummy_image)                \n",
    "            else:\n",
    "                path_id = df_room[\"img_path\"][filter]\n",
    "                for p in path_id:\n",
    "                    img_tmp = plt.imread(\"data/images_resized/\"+p)\n",
    "                    img_tmp = np.expand_dims(img_tmp, axis = 0)\n",
    "                    img_tmp = resnet_pre(img_tmp)\n",
    "                    img_tmp = resnet(img_tmp)\n",
    "                    l.append(img_tmp)\n",
    "            l = np.stack(l)\n",
    "            l = np.max(l, axis = 0)\n",
    "            features.append(l)\n",
    "            ids.append(id)\n",
    "        except:\n",
    "            dummy_image = np.zeros((1,256,256,3))\n",
    "            dummy_image = resnet_pre(dummy_image)\n",
    "            dummy_image = resnet(dummy_image)\n",
    "            l.append(dummy_image)\n",
    "            l = np.stack(l)\n",
    "            l = np.max(l, axis = 0)\n",
    "            features.append(l)\n",
    "            ids.append(id)\n",
    "            continue\n",
    "    features = np.squeeze(np.stack(features))\n",
    "    filter = np.nonzero(features.sum(axis = 0))[0]\n",
    "    features = features[:,filter]\n",
    "    print(len(filter), \" features are nonzero.\")\n",
    "    features = features.tolist()\n",
    "     \n",
    "    return features, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(df):\n",
    "    basis_df = df[[\"id\",\"log_price\"]]\n",
    "    basis_df = basis_df.drop_duplicates()\n",
    "    \n",
    "    features = []\n",
    "    ids = []\n",
    "    \n",
    "    for i in tqdm(np.unique(df[\"label\"])):\n",
    "        feat_cat, ids_cat = input_pipeline(i,df)\n",
    "        df_tmp = pd.DataFrame({\"features_\"+str(i): feat_cat, \"id\": ids_cat})\n",
    "        basis_df = pd.merge(basis_df, df_tmp, on = \"id\", how = \"left\")\n",
    "    return basis_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(df):\n",
    "    counter_overall_dummy = 0\n",
    "    counter_overall_img = 0\n",
    "\n",
    "    def input_pipeline(room = 0, df = df):\n",
    "        ids = []\n",
    "        features = []\n",
    "        \n",
    "        # FILTER DF\n",
    "        df_room = df[df[\"label\"] == room]\n",
    "        \n",
    "        # RESNET\n",
    "        resnet = tf.keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', pooling=\"avg\", input_shape = (None,None,3))\n",
    "        resnet_pre = keras.applications.resnet50.preprocess_input\n",
    "        resnet.trainable = False\n",
    "        counter_dummy = 0\n",
    "        counter_img = 0\n",
    "        for id in tqdm_notebook(np.unique(df[\"id\"])):\n",
    "            filter = df_room[\"id\"] == id  \n",
    "\n",
    "            try:\n",
    "                l = []\n",
    "                if filter.sum() == 0:\n",
    "                    dummy_image = np.zeros((1,256,256,3))\n",
    "                    dummy_image = resnet_pre(dummy_image)\n",
    "                    dummy_image = resnet(dummy_image)\n",
    "                    l.append(dummy_image)\n",
    "                    counter_dummy += 1                \n",
    "                else:\n",
    "                    path_id = df_room[\"img_path\"][filter]\n",
    "                    for p in path_id:\n",
    "                        img_tmp = plt.imread(\"data/images_resized/\"+p)\n",
    "                        img_tmp = np.expand_dims(img_tmp, axis = 0)\n",
    "                        img_tmp = resnet_pre(img_tmp)\n",
    "                        img_tmp = resnet(img_tmp)\n",
    "                        l.append(img_tmp)\n",
    "                        counter_img += 1\n",
    "                l = np.stack(l)\n",
    "                l = np.max(l, axis = 0)\n",
    "                features.append(l)\n",
    "                ids.append(id)\n",
    "            except:\n",
    "                dummy_image = np.zeros((1,256,256,3))\n",
    "                dummy_image = resnet_pre(dummy_image)\n",
    "                dummy_image = resnet(dummy_image)\n",
    "                l.append(dummy_image)\n",
    "                l = np.stack(l)\n",
    "                l = np.max(l, axis = 0)\n",
    "                features.append(l)\n",
    "                ids.append(id)\n",
    "                continue\n",
    "        features = np.squeeze(np.stack(features))\n",
    "        filter = np.nonzero(features.sum(axis = 0))[0]\n",
    "        features = features[:,filter]\n",
    "        print(len(filter), \" features are nonzero.\")\n",
    "        features = features.tolist()\n",
    "        print(counter_dummy, \"dummy images were added.\")\n",
    "        return features, ids, counter_dummy, counter_img\n",
    "\n",
    "    basis_df = df[[\"id\",\"log_price\"]]\n",
    "    basis_df = basis_df.drop_duplicates()\n",
    "    \n",
    "    features = []\n",
    "    ids = []\n",
    "    \n",
    "    for i in tqdm_notebook(np.unique(df[\"label\"])):\n",
    "        feat_cat, ids_cat, counter_dummy, counter_img = input_pipeline(i,df)\n",
    "        df_tmp = pd.DataFrame({\"features_\"+str(i): feat_cat, \"id\": ids_cat})\n",
    "        basis_df = pd.merge(basis_df, df_tmp, on = \"id\", how = \"left\")\n",
    "        counter_overall_dummy += counter_dummy\n",
    "        counter_overall_img += counter_img\n",
    "    print(counter_overall_dummy)\n",
    "    print(counter_overall_img)\n",
    "    return basis_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/97/j215pw6x7sq158bvx1ktlhf80000gn/T/ipykernel_3389/1717659073.py:66: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm_notebook(np.unique(df[\"label\"])):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b107699b3f464b9c7b25d99955c016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 15:23:59.798978: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-02-22 15:23:59.799395: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/97/j215pw6x7sq158bvx1ktlhf80000gn/T/ipykernel_3389/1717659073.py:18: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for id in tqdm_notebook(np.unique(df[\"id\"])):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff236ea9c0a456fa2d9ccbb81efed2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1416  features are nonzero.\n",
      "266 dummy images were added.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/97/j215pw6x7sq158bvx1ktlhf80000gn/T/ipykernel_3389/1717659073.py:18: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for id in tqdm_notebook(np.unique(df[\"id\"])):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90e016d3cfe497f85ad4398b0c1c2bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_df = data_generator(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns = [\"id\", \"log_price\", \"bath\", \"bed\", \"dining\", \"hall\", \"kitchen\", \"living\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input process for TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set columns for tabnet\n",
    "bin_col = [col for col in X_train if np.isin(X_train[col].unique(), [0, 1]).all()]\n",
    "num_col = [col for col in X_train if ~np.isin(X_train[col].unique(), [0, 1]).all()]\n",
    "col_names = bin_col + num_col\n",
    "feature_columns = []\n",
    "col_names.remove(\"id\")\n",
    "for col in col_names:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SHAPE:  (2710, 77)\n",
      "TEST SHAPE:  (853, 77)\n",
      "VAL SHAPE:  (706, 77)\n"
     ]
    }
   ],
   "source": [
    "# merge data for tabnet and images to get consistent ids\n",
    "data_df_train = pd.merge(final_df, X_train, on=\"id\", how=\"inner\")\n",
    "data_df_train.dropna(inplace = True)\n",
    "print(\"TRAIN SHAPE: \", data_df_train.shape)\n",
    "\n",
    "data_df_test = pd.merge(final_df, X_test, on=\"id\", how=\"inner\")\n",
    "data_df_test.dropna(inplace = True)\n",
    "print(\"TEST SHAPE: \", data_df_test.shape)\n",
    "\n",
    "data_df_val = pd.merge(final_df, X_val, on=\"id\", how=\"inner\")\n",
    "data_df_val.dropna(inplace = True)\n",
    "print(\"VAL SHAPE: \", data_df_val.shape)\n",
    "\n",
    "# extract consistent columns\n",
    "X_train_tab = data_df_train.filter(col_names)\n",
    "X_test_tab = data_df_test.filter(col_names)\n",
    "X_val_tab = data_df_val.filter(col_names)\n",
    "\n",
    "data_df_train.drop(col_names, axis = 1, inplace = True)\n",
    "data_df_test.drop(col_names, axis = 1, inplace = True)\n",
    "data_df_val.drop(col_names, axis = 1, inplace = True)\n",
    "\n",
    "# extract consistent price\n",
    "y_train = data_df_train.pop(\"log_price\")\n",
    "y_test = data_df_test.pop(\"log_price\")\n",
    "y_val = data_df_val.pop(\"log_price\")\n",
    "\n",
    "# drop id\n",
    "X_train = data_df_train.drop(\"id\", axis = 1)\n",
    "X_test = data_df_test.drop(\"id\", axis = 1)\n",
    "X_val = data_df_val.drop(\"id\", axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(ds):\n",
    "    # tabnet\n",
    "    features = tf.unstack(ds[\"features\"])\n",
    "    features = dict(zip(col_names, features))\n",
    "\n",
    "    # images\n",
    "    bath = tf.unstack(ds[\"bath\"])\n",
    "    bed = tf.unstack(ds[\"bed\"])\n",
    "    dining = tf.unstack(ds[\"dining\"])\n",
    "    hall = tf.unstack(ds[\"hall\"])\n",
    "    kitchen = tf.unstack(ds[\"kitchen\"])\n",
    "    living = tf.unstack(ds[\"living\"])\n",
    "    \n",
    "    prices = ds[\"price\"]\n",
    "    \n",
    "    y = prices\n",
    "    return (bath, bed, dining, hall, kitchen, living, features), y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct tensorflow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(X_train.shape[0] * 0.9)\n",
    "batch_size = int(X_train.shape[0] * 0.1)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(features, prices, random_state = 123, test_size = 0.2)\n",
    "# X_train, X_val, y_train, y_val  = train_test_split(X_train, y_train, test_size=0.2, random_state=1) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "data_train = tf.data.Dataset.from_tensor_slices({\"bath\": np.squeeze(np.stack(X_train[\"bath\"])),\n",
    "                                                 \"bed\": np.squeeze(np.stack(X_train[\"bed\"])),\n",
    "                                                 \"dining\": np.squeeze(np.stack(X_train[\"dining\"])),\n",
    "                                                 \"hall\": np.squeeze(np.stack(X_train[\"hall\"])),\n",
    "                                                 \"kitchen\": np.squeeze(np.stack(X_train[\"kitchen\"])),\n",
    "                                                 \"living\": np.squeeze(np.stack(X_train[\"living\"])),\n",
    "                                                 \"features\": X_train_tab,\n",
    "                                                 \"price\": y_train})\n",
    "data_train = data_train.cache()\n",
    "data_train = data_train.shuffle(6000, seed = 13)\n",
    "train_dataset = data_train.take(len(y_train))\n",
    "train_dataset = train_dataset.map(transform)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "data_test = tf.data.Dataset.from_tensor_slices({\"bath\": np.squeeze(np.stack(X_test[\"bath\"])),\n",
    "                                                 \"bed\": np.squeeze(np.stack(X_test[\"bed\"])),\n",
    "                                                 \"dining\": np.squeeze(np.stack(X_test[\"dining\"])),\n",
    "                                                 \"hall\": np.squeeze(np.stack(X_test[\"hall\"])),\n",
    "                                                 \"kitchen\": np.squeeze(np.stack(X_test[\"kitchen\"])),\n",
    "                                                 \"living\": np.squeeze(np.stack(X_test[\"living\"])),\n",
    "                                                 \"features\": X_test_tab,\n",
    "                                                 \"price\": y_test})\n",
    "data_test = data_test.cache()\n",
    "test_dataset = data_test.take(len(y_test))\n",
    "test_dataset = test_dataset.map(transform)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "data_val = tf.data.Dataset.from_tensor_slices({\"bath\": np.squeeze(np.stack(X_val[\"bath\"])),\n",
    "                                                 \"bed\": np.squeeze(np.stack(X_val[\"bed\"])),\n",
    "                                                 \"dining\": np.squeeze(np.stack(X_val[\"dining\"])),\n",
    "                                                 \"hall\": np.squeeze(np.stack(X_val[\"hall\"])),\n",
    "                                                 \"kitchen\": np.squeeze(np.stack(X_val[\"kitchen\"])),\n",
    "                                                 \"living\": np.squeeze(np.stack(X_val[\"living\"])),\n",
    "                                                 \"features\": X_val_tab,\n",
    "                                                 \"price\": y_val})\n",
    "data_val = data_val.cache()\n",
    "val_dataset = data_val.take(len(y_val))\n",
    "val_dataset = val_dataset.map(transform)\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend\n",
    "class weight_constr(tf.keras.constraints.Constraint):\n",
    "  \"\"\"Constrains weight tensors to be centered around `ref_value`.\"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    self.ref_value = 1\n",
    "\n",
    "  def __call__(self, w):\n",
    "    nonneg = w * tf.cast(tf.greater_equal(w, 0.), backend.floatx())\n",
    "    sum_w = tf.reduce_sum(nonneg)\n",
    "    nonneg_one = nonneg/sum_w\n",
    "    return nonneg_one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup model\n",
    "class Compound_model(tf.keras.Model):\n",
    "\n",
    "  def __init__(self,feature_columns, dropout = 0, l2 = 0, nodes1 = 512, nodes2 = 1,\n",
    "                 num_features=None,\n",
    "                 feature_dim=32,\n",
    "                 output_dim=32,\n",
    "                 num_decision_steps=3,\n",
    "                 relaxation_factor=1.5,\n",
    "                 sparsity_coefficient=1e-5,\n",
    "                 norm_type='group',\n",
    "                 batch_momentum=0.98,\n",
    "                 virtual_batch_size=None,\n",
    "                 num_groups=1,\n",
    "                 epsilon=1e-5):\n",
    "    super().__init__()\n",
    "    # TabNet\n",
    "    self.tabnet = tabnet.TabNet(feature_columns=feature_columns,\n",
    "                          num_features=num_features,\n",
    "                          feature_dim=feature_dim,\n",
    "                          output_dim=output_dim,\n",
    "                          num_decision_steps=num_decision_steps,\n",
    "                          relaxation_factor=relaxation_factor,\n",
    "                          sparsity_coefficient=sparsity_coefficient,\n",
    "                          norm_type=norm_type,\n",
    "                          batch_momentum=batch_momentum,\n",
    "                          virtual_batch_size=virtual_batch_size,\n",
    "                          num_groups=num_groups,\n",
    "                          epsilon=epsilon)\n",
    "   \n",
    "    \n",
    "    # bathroom\n",
    "    self.bn1_bath = tf.keras.layers.BatchNormalization()\n",
    "    self.drop1_bath = tf.keras.layers.Dropout(dropout)\n",
    "    self.dense1_bath = tf.keras.layers.Dense(nodes1, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.bn2_bath = tf.keras.layers.BatchNormalization()\n",
    "    self.drop2_bath = tf.keras.layers.Dropout(dropout)\n",
    "    #self.dense2_bath = tf.keras.layers.Dense(nodes2, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.dense2_bath = tf.keras.layers.Dense(nodes2, kernel_regularizer = keras.regularizers.l2(l2))\n",
    " \n",
    "    # bedroom\n",
    "    self.bn1_bed = tf.keras.layers.BatchNormalization()\n",
    "    self.drop1_bed = tf.keras.layers.Dropout(dropout)\n",
    "    self.dense1_bed = tf.keras.layers.Dense(nodes1, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.bn2_bed = tf.keras.layers.BatchNormalization()\n",
    "    self.drop2_bed = tf.keras.layers.Dropout(dropout)\n",
    "    #self.dense2_bed = tf.keras.layers.Dense(nodes2, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.dense2_bed = tf.keras.layers.Dense(nodes2, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "       \n",
    "      # dining\n",
    "    self.bn1_dining = tf.keras.layers.BatchNormalization()\n",
    "    self.drop1_dining = tf.keras.layers.Dropout(dropout)\n",
    "    self.dense1_dining = tf.keras.layers.Dense(nodes1, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.bn2_dining = tf.keras.layers.BatchNormalization()\n",
    "    self.drop2_dining = tf.keras.layers.Dropout(dropout)\n",
    "    #self.dense2_dining = tf.keras.layers.Dense(nodes2, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.dense2_dining = tf.keras.layers.Dense(nodes2, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "   \n",
    "    # hall\n",
    "    self.bn1_hall = tf.keras.layers.BatchNormalization()\n",
    "    self.drop1_hall = tf.keras.layers.Dropout(dropout)\n",
    "    self.dense1_hall = tf.keras.layers.Dense(nodes1, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.bn2_hall = tf.keras.layers.BatchNormalization()\n",
    "    self.drop2_hall = tf.keras.layers.Dropout(dropout)\n",
    "    #self.dense2_hall = tf.keras.layers.Dense(nodes2, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.dense2_hall = tf.keras.layers.Dense(nodes2, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    \n",
    "    # kitchen\n",
    "    self.bn1_kitchen = tf.keras.layers.BatchNormalization()\n",
    "    self.drop1_kitchen = tf.keras.layers.Dropout(dropout)\n",
    "    self.dense1_kitchen = tf.keras.layers.Dense(nodes1, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.bn2_kitchen = tf.keras.layers.BatchNormalization()\n",
    "    self.drop2_kitchen = tf.keras.layers.Dropout(dropout)\n",
    "    #self.dense2_kitchen = tf.keras.layers.Dense(nodes2, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.dense2_kitchen = tf.keras.layers.Dense(nodes2, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "  \n",
    "    # livingroom\n",
    "    self.bn1_living = tf.keras.layers.BatchNormalization()\n",
    "    self.drop1_living = tf.keras.layers.Dropout(dropout)\n",
    "    self.dense1_living = tf.keras.layers.Dense(nodes1, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.bn2_living = tf.keras.layers.BatchNormalization()\n",
    "    self.drop2_living = tf.keras.layers.Dropout(dropout)\n",
    "    #self.dense2_living = tf.keras.layers.Dense(nodes2, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.dense2_living = tf.keras.layers.Dense(nodes2, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    \n",
    "    # bring all images together\n",
    "    self.bn_img = tf.keras.layers.BatchNormalization()\n",
    "    self.drop_img = tf.keras.layers.Dropout(dropout)\n",
    "    self.dense_img = tf.keras.layers.Dense(1, activation = tf.nn.relu)\n",
    "    \n",
    "    # bring features of TabNet together\n",
    "    self.dense_tabnet = tf.keras.layers.Dense(1, activation = tf.nn.relu)\n",
    "        \n",
    "    # final prediction\n",
    "    self.bn_final = tf.keras.layers.BatchNormalization()\n",
    "    self.drop_final = tf.keras.layers.Dropout(dropout)\n",
    "    self.dense_final = tf.keras.layers.Dense(1, kernel_constraint =weight_constr())\n",
    "    \n",
    "  def call(self, inputs, training = None):\n",
    "    \n",
    "    # bathroom\n",
    "    x = self.bn1_bath(inputs[0])\n",
    "    x = self.drop1_bath(x)\n",
    "    x = self.dense1_bath(x)\n",
    "    x = self.bn2_bath(x)\n",
    "    x = self.drop2_bath(x)\n",
    "    bath_out = self.dense2_bath(x)\n",
    "    \n",
    "    \n",
    "    # bedroom\n",
    "    x = self.bn1_bed(inputs[1])\n",
    "    x = self.drop1_bed(x)\n",
    "    x = self.dense1_bed(x)\n",
    "    x = self.bn2_bed(x)\n",
    "    x = self.drop2_bed(x)\n",
    "    bed_out = self.dense2_bed(x)\n",
    "    \n",
    "    # diningroom\n",
    "    x = self.bn1_dining(inputs[2])\n",
    "    x = self.drop1_dining(x)\n",
    "    x = self.dense1_dining(x)\n",
    "    x = self.bn2_dining(x)\n",
    "    x = self.drop2_dining(x)\n",
    "    dining_out = self.dense2_dining(x)\n",
    "    \n",
    "    # hallroom\n",
    "    x = self.bn1_hall(inputs[3])\n",
    "    x = self.drop1_hall(x)\n",
    "    x = self.dense1_hall(x)\n",
    "    x = self.bn2_hall(x)\n",
    "    x = self.drop2_hall(x)\n",
    "    hall_out = self.dense2_hall(x)\n",
    "    \n",
    "    # kitchen\n",
    "    x = self.bn1_kitchen(inputs[4])\n",
    "    x = self.drop1_kitchen(x)\n",
    "    x = self.dense1_kitchen(x)\n",
    "    x = self.bn2_kitchen(x)\n",
    "    x = self.drop2_kitchen(x)\n",
    "    kitchen_out = self.dense2_kitchen(x)\n",
    "    \n",
    "    # livingroom\n",
    "    x = self.bn1_living(inputs[5])\n",
    "    x = self.drop1_living(x)\n",
    "    x = self.dense1_living(x)\n",
    "    x = self.bn2_living(x)\n",
    "    x = self.drop2_living(x)\n",
    "    living_out = self.dense2_living(x)\n",
    "    \n",
    "    \n",
    "    # tabnet\n",
    "    self.activations = self.tabnet(inputs[6], training=True)\n",
    "    out_tabnet = self.dense_tabnet(self.activations)\n",
    "\n",
    "    # join images\n",
    "    out_img = tf.keras.layers.concatenate([bath_out, bed_out, dining_out, hall_out, kitchen_out, living_out])\n",
    "    out_img = self.bn_img(out_img)\n",
    "    out_img = self.drop_img(out_img)\n",
    "    out_img = self.dense_img(out_img)\n",
    "\n",
    "    # join\n",
    "    out = tf.keras.layers.concatenate([out_img, out_tabnet])\n",
    "    out = self.bn_final(out)\n",
    "    out = self.drop_final(out)\n",
    "    return self.dense_final(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_squared(y, y_pred):\n",
    "  residual = tf.reduce_sum(tf.square(tf.subtract(y, y_pred)))\n",
    "  total = tf.reduce_sum(tf.square(tf.subtract(y, tf.reduce_mean(y))))\n",
    "  r2 = tf.subtract(1.0, tf.math.divide(residual, total))  \n",
    "  return r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Compound from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TabNet]: 5 features will be used for decision steps.\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "in user code:\n\n    /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:855 train_function  *\n        return step_function(self, iterator)\n    /var/folders/97/j215pw6x7sq158bvx1ktlhf80000gn/T/ipykernel_2812/2176399485.py:152 call  *\n        self.activations = self.tabnet(inputs[6], training=True)\n    /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tabnet/tabnet.py:224 call  *\n        output_aggregated = tf.zeros([batch_size, self.output_dim])\n    /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper  **\n        return target(*args, **kwargs)\n    /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:2911 wrapped\n        tensor = fun(*args, **kwargs)\n    /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:2960 zeros\n        output = _constant_if_small(zero, shape, dtype, name)\n    /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:2896 _constant_if_small\n        if np.prod(shape) < 1000:\n    <__array_function__ internals>:5 prod\n        \n    /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3051 prod\n        return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n    /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86 _wrapreduction\n        return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n    /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:867 __array__\n        raise NotImplementedError(\n\n    NotImplementedError: Cannot convert a symbolic Tensor (compound_model_2/tab_net_2/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/dmnk/OneDrive - stud.uni-goettingen.de/Dokumente/3. Semester/SeminarDL/DubAir/CompoundModel.ipynb Cell 29'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dmnk/OneDrive%20-%20stud.uni-goettingen.de/Dokumente/3.%20Semester/SeminarDL/DubAir/CompoundModel.ipynb#ch0000025?line=6'>7</a>\u001b[0m optimizer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(lr)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dmnk/OneDrive%20-%20stud.uni-goettingen.de/Dokumente/3.%20Semester/SeminarDL/DubAir/CompoundModel.ipynb#ch0000025?line=7'>8</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m , metrics\u001b[39m=\u001b[39m[R_squared,\u001b[39m\"\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dmnk/OneDrive%20-%20stud.uni-goettingen.de/Dokumente/3.%20Semester/SeminarDL/DubAir/CompoundModel.ipynb#ch0000025?line=9'>10</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_dataset, validation_data \u001b[39m=\u001b[39;49m test_dataset, epochs\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1183\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1175'>1176</a>\u001b[0m \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1176'>1177</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1177'>1178</a>\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1178'>1179</a>\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1179'>1180</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1180'>1181</a>\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1181'>1182</a>\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1182'>1183</a>\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1183'>1184</a>\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1184'>1185</a>\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:889\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=885'>886</a>\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=887'>888</a>\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=888'>889</a>\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=890'>891</a>\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=891'>892</a>\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:933\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=929'>930</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=930'>931</a>\u001b[0m   \u001b[39m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=931'>932</a>\u001b[0m   initializers \u001b[39m=\u001b[39m []\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=932'>933</a>\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize(args, kwds, add_initializers_to\u001b[39m=\u001b[39;49minitializers)\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=933'>934</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=934'>935</a>\u001b[0m   \u001b[39m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=935'>936</a>\u001b[0m   \u001b[39m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=936'>937</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:763\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=759'>760</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lifted_initializer_graph \u001b[39m=\u001b[39m lifted_initializer_graph\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=760'>761</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph_deleter \u001b[39m=\u001b[39m FunctionDeleter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lifted_initializer_graph)\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=761'>762</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_stateful_fn \u001b[39m=\u001b[39m (\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=762'>763</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateful_fn\u001b[39m.\u001b[39;49m_get_concrete_function_internal_garbage_collected(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=763'>764</a>\u001b[0m         \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds))\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=765'>766</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvalid_creator_scope\u001b[39m(\u001b[39m*\u001b[39munused_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munused_kwds):\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=766'>767</a>\u001b[0m   \u001b[39m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3050\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3047'>3048</a>\u001b[0m   args, kwargs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3048'>3049</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3049'>3050</a>\u001b[0m   graph_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3050'>3051</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3444\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3439'>3440</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_define_function_with_shape_relaxation(\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3440'>3441</a>\u001b[0m       args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3442'>3443</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39mmissed\u001b[39m.\u001b[39madd(call_context_key)\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3443'>3444</a>\u001b[0m graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_graph_function(args, kwargs)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3444'>3445</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39mprimary[cache_key] \u001b[39m=\u001b[39m graph_function\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3446'>3447</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3279\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3273'>3274</a>\u001b[0m missing_arg_names \u001b[39m=\u001b[39m [\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3274'>3275</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg, i) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(missing_arg_names)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3275'>3276</a>\u001b[0m ]\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3276'>3277</a>\u001b[0m arg_names \u001b[39m=\u001b[39m base_arg_names \u001b[39m+\u001b[39m missing_arg_names\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3277'>3278</a>\u001b[0m graph_function \u001b[39m=\u001b[39m ConcreteFunction(\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3278'>3279</a>\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3279'>3280</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3280'>3281</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3281'>3282</a>\u001b[0m         args,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3282'>3283</a>\u001b[0m         kwargs,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3283'>3284</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_signature,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3284'>3285</a>\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3285'>3286</a>\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3286'>3287</a>\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3287'>3288</a>\u001b[0m         override_flat_arg_shapes\u001b[39m=\u001b[39;49moverride_flat_arg_shapes,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3288'>3289</a>\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value),\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3289'>3290</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3290'>3291</a>\u001b[0m     function_spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3291'>3292</a>\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3292'>3293</a>\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3293'>3294</a>\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3294'>3295</a>\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3295'>3296</a>\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=3296'>3297</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:999\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=995'>996</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=996'>997</a>\u001b[0m   _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=998'>999</a>\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1000'>1001</a>\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1001'>1002</a>\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1002'>1003</a>\u001b[0m func_outputs \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mmap_structure(convert, func_outputs,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1003'>1004</a>\u001b[0m                                   expand_composites\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:672\u001b[0m, in \u001b[0;36mFunction._defun_with_scope.<locals>.wrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=667'>668</a>\u001b[0m \u001b[39mwith\u001b[39;00m default_graph\u001b[39m.\u001b[39m_variable_creator_scope(scope, priority\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m):  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=668'>669</a>\u001b[0m   \u001b[39m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=669'>670</a>\u001b[0m   \u001b[39m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=670'>671</a>\u001b[0m   \u001b[39mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=671'>672</a>\u001b[0m     out \u001b[39m=\u001b[39m weak_wrapped_fn()\u001b[39m.\u001b[39;49m__wrapped__(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=672'>673</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:986\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=983'>984</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=984'>985</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=985'>986</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=986'>987</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=987'>988</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: in user code:\n\n    /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:855 train_function  *\n        return step_function(self, iterator)\n    /var/folders/97/j215pw6x7sq158bvx1ktlhf80000gn/T/ipykernel_2812/2176399485.py:152 call  *\n        self.activations = self.tabnet(inputs[6], training=True)\n    /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tabnet/tabnet.py:224 call  *\n        output_aggregated = tf.zeros([batch_size, self.output_dim])\n    /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper  **\n        return target(*args, **kwargs)\n    /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:2911 wrapped\n        tensor = fun(*args, **kwargs)\n    /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:2960 zeros\n        output = _constant_if_small(zero, shape, dtype, name)\n    /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:2896 _constant_if_small\n        if np.prod(shape) < 1000:\n    <__array_function__ internals>:5 prod\n        \n    /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3051 prod\n        return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n    /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86 _wrapreduction\n        return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n    /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:867 __array__\n        raise NotImplementedError(\n\n    NotImplementedError: Cannot convert a symbolic Tensor (compound_model_2/tab_net_2/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(2)\n",
    "model = Compound_model(feature_columns = feature_columns,\n",
    "                                output_dim=120, feature_dim=125, num_groups=1,\n",
    "                                num_decision_steps=2, relaxation_factor=1.5,\n",
    "                                sparsity_coefficient=1e-05, dropout= 0.2, nodes1 = 512, nodes2 = 16, l2 = 0)\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(0.01, decay_steps=100, decay_rate=0.95, staircase=False)\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "model.compile(optimizer, loss='mse' , metrics=[R_squared,\"mae\"])\n",
    "\n",
    "model.fit(train_dataset, validation_data = test_dataset, epochs=300, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 2s 228ms/step - loss: 9.0255 - R_squared: -24.2988 - mae: 2.4546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9.025516510009766, -24.298797607421875, 2.4545772075653076]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate weights of final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.       ],\n",
       "        [ 1.0000001]], dtype=float32),\n",
       " array([0.19783714], dtype=float32)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[-1].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload optimal weights from best TabNet and Image Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Compound_model(feature_columns = feature_columns,\n",
    "                                output_dim=120, feature_dim=125, num_groups=1,\n",
    "                                num_decision_steps=2, relaxation_factor=1.5,\n",
    "                                sparsity_coefficient=1e-05, dropout= 0.2, nodes1 = 512, nodes2 = 1, l2 = 0)\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(0.01, decay_steps=100, decay_rate=0.95, staircase=False)\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "model.compile(optimizer, loss='mse' , metrics=[R_squared,\"mae\"])\n",
    "\n",
    "model.fit(train_dataset, validation_data = test_dataset, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load best TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TabNet]: 5 features will be used for decision steps.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 21:27:00.126076: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 3s 138ms/step - loss: 12.5054 - R_squared: -24.5086 - mse: 12.5054 - mae: 3.1016\n"
     ]
    }
   ],
   "source": [
    "X_train_tn, X_test_tn, X_val_tn, y_train_tn, y_test_tn, y_val_tn = load_data(for_dendro = False)\n",
    "\n",
    "def transform(ds):\n",
    "    features = tf.unstack(ds[\"features\"])\n",
    "    prices = ds[\"price\"]\n",
    "\n",
    "    x = dict(zip(col_names, features))\n",
    "    y = prices\n",
    "    return x, y\n",
    "\n",
    "def R_squared(y, y_pred):\n",
    "  residual = tf.reduce_sum(tf.square(tf.subtract(y, y_pred)))\n",
    "  total = tf.reduce_sum(tf.square(tf.subtract(y, tf.reduce_mean(y))))\n",
    "  r2 = tf.subtract(1.0, tf.math.divide(residual, total))\n",
    "  \n",
    "  return r2\n",
    "\n",
    "bin_col = [col for col in X_train_tn if np.isin(X_train_tn[col].unique(), [0, 1]).all()]\n",
    "num_col = [col for col in X_train_tn if ~np.isin(X_train_tn[col].unique(), [0, 1]).all()]\n",
    "col_names = bin_col + num_col\n",
    "\n",
    "train_size = int(X_train_tn.shape[0] * 0.9)\n",
    "batch_size = int(X_train_tn.shape[0] * 0.1)\n",
    "\n",
    "data_train = tf.data.Dataset.from_tensor_slices({\"features\": X_train_tn, \"price\": y_train_tn})\n",
    "data_train = data_train.shuffle(6000, seed = 13)\n",
    "train_dataset = data_train.take(len(X_train_tn))\n",
    "train_dataset = train_dataset.map(transform)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "data_test = tf.data.Dataset.from_tensor_slices({\"features\": X_val_tn, \"price\": y_val_tn})\n",
    "test_dataset = data_test.take(len(X_val_tn))\n",
    "test_dataset = test_dataset.map(transform)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "\n",
    "feature_columns = []\n",
    "\n",
    "for col in col_names:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(col))\n",
    "od = 120\n",
    "fd =  125\n",
    "nds = 2\n",
    "rf = 1.5\n",
    "sc = 1e-05\n",
    "\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(0.01, decay_steps=100, decay_rate=0.95, staircase=False)\n",
    "#lr = 0.01\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "model_tn = tabnet.TabNetRegression(feature_columns, num_regressors=1,\n",
    "                                output_dim=od, feature_dim=fd, num_groups=1,\n",
    "                                num_decision_steps=nds, relaxation_factor=rf)\n",
    "\n",
    "model_tn.compile(optimizer, loss='mse' , metrics=[R_squared, \"mse\", \"mae\"])\n",
    "model_tn.fit(test_dataset)\n",
    "model_tn.load_weights(\"TabNet_Selected/best_model3.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load best Image Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(ds):\n",
    "    bath = tf.unstack(ds[\"bath\"])\n",
    "    bed = tf.unstack(ds[\"bed\"])\n",
    "    dining = tf.unstack(ds[\"dining\"])\n",
    "    hall = tf.unstack(ds[\"hall\"])\n",
    "    kitchen = tf.unstack(ds[\"kitchen\"])\n",
    "    living = tf.unstack(ds[\"living\"])\n",
    "   # others = tf.unstack(ds[\"others\"])\n",
    "\n",
    "    prices = ds[\"price\"]\n",
    "    \n",
    "    y = prices\n",
    "    return (bath, bed, dining, hall, kitchen, living), y\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# Load data\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = load_data_fold(fold=4, drop_id = False)\n",
    "\n",
    "train_ids = X_train[\"id\"]\n",
    "test_ids = X_test[\"id\"]\n",
    "val_ids = X_val[\"id\"]\n",
    "\n",
    "X_train = pd.merge(train_ids, final_df, on = \"id\", how = \"left\")\n",
    "X_train = X_train.drop([\"id\",\"log_price\"], axis = 1)\n",
    "X_train.columns = [\"bath\", \"bed\", \"dining\", \"hall\", \"kitchen\", \"living\"]#, \"others\"]\n",
    "filter = np.any(X_train.isna(), axis = 1).values\n",
    "X_train = X_train[~filter]\n",
    "y_train = y_train[~filter]\n",
    "\n",
    "X_test = pd.merge(test_ids, final_df, on = \"id\", how = \"left\")\n",
    "X_test = X_test.drop([\"id\",\"log_price\"], axis = 1)\n",
    "X_test.columns = [\"bath\", \"bed\", \"dining\", \"hall\", \"kitchen\", \"living\"]#, \"others\"]\n",
    "filter = np.any(X_test.isna(), axis = 1).values\n",
    "X_test = X_test[~filter]\n",
    "y_test = y_test[~filter]\n",
    "\n",
    "X_val = pd.merge(val_ids, final_df, on = \"id\", how = \"left\")\n",
    "X_val = X_val.drop([\"id\",\"log_price\"], axis = 1)\n",
    "X_val.columns = [\"bath\", \"bed\", \"dining\", \"hall\", \"kitchen\", \"living\"]#, \"others\"]\n",
    "filter = np.any(X_val.isna(), axis = 1).values\n",
    "X_val = X_val[~filter]\n",
    "y_val = y_val[~filter]\n",
    "\n",
    "data_train = tf.data.Dataset.from_tensor_slices({\"bath\": np.squeeze(np.stack(X_train[\"bath\"])),\n",
    "                                                 \"bed\": np.squeeze(np.stack(X_train[\"bed\"])),\n",
    "                                                 \"dining\": np.squeeze(np.stack(X_train[\"dining\"])),\n",
    "                                                 \"hall\": np.squeeze(np.stack(X_train[\"hall\"])),\n",
    "                                                 \"kitchen\": np.squeeze(np.stack(X_train[\"kitchen\"])),\n",
    "                                                 \"living\": np.squeeze(np.stack(X_train[\"living\"])),\n",
    "                                                 #\"others\": np.squeeze(np.stack(X_train[\"others\"])),\n",
    "                                                 \"price\": y_train})\n",
    "data_train = data_train.cache()\n",
    "data_train = data_train.shuffle(6000, seed = 13)\n",
    "train_dataset = data_train.take(len(y_train))\n",
    "train_dataset = train_dataset.map(transform)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "data_test = tf.data.Dataset.from_tensor_slices({\"bath\": np.squeeze(np.stack(X_test[\"bath\"])),\n",
    "                                                 \"bed\": np.squeeze(np.stack(X_test[\"bed\"])),\n",
    "                                                 \"dining\": np.squeeze(np.stack(X_test[\"dining\"])),\n",
    "                                                 \"hall\": np.squeeze(np.stack(X_test[\"hall\"])),\n",
    "                                                 \"kitchen\": np.squeeze(np.stack(X_test[\"kitchen\"])),\n",
    "                                                 \"living\": np.squeeze(np.stack(X_test[\"living\"])),\n",
    "                                                # \"others\": np.squeeze(np.stack(X_test[\"others\"])),\n",
    "                                                 \"price\": y_test})\n",
    "data_test = data_test.cache()\n",
    "test_dataset = data_test.take(len(y_test))\n",
    "test_dataset = test_dataset.map(transform)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "data_val = tf.data.Dataset.from_tensor_slices({\"bath\": np.squeeze(np.stack(X_val[\"bath\"])),\n",
    "                                                 \"bed\": np.squeeze(np.stack(X_val[\"bed\"])),\n",
    "                                                 \"dining\": np.squeeze(np.stack(X_val[\"dining\"])),\n",
    "                                                 \"hall\": np.squeeze(np.stack(X_val[\"hall\"])),\n",
    "                                                 \"kitchen\": np.squeeze(np.stack(X_val[\"kitchen\"])),\n",
    "                                                 \"living\": np.squeeze(np.stack(X_val[\"living\"])),\n",
    "                                                 #\"others\": np.squeeze(np.stack(X_val[\"others\"])),\n",
    "                                                 \"price\": y_val})\n",
    "data_val = data_val.cache()\n",
    "val_dataset = data_val.take(len(y_val))\n",
    "val_dataset = val_dataset.map(transform)\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup model\n",
    "class Img_model(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, dropout = 0, l2 = 0, nodes1 = 512, nodes2 = 1):\n",
    "    super().__init__()\n",
    "    \n",
    "    # bathroom\n",
    "    self.bn1_bath = tf.keras.layers.BatchNormalization()\n",
    "    self.drop1_bath = tf.keras.layers.Dropout(dropout)\n",
    "    self.dense1_bath = tf.keras.layers.Dense(nodes1, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.bn2_bath = tf.keras.layers.BatchNormalization()\n",
    "    self.drop2_bath = tf.keras.layers.Dropout(dropout)\n",
    "    #self.dense2_bath = tf.keras.layers.Dense(nodes2, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.dense2_bath = tf.keras.layers.Dense(nodes2, kernel_regularizer = keras.regularizers.l2(l2))\n",
    " \n",
    "    # bedroom\n",
    "    self.bn1_bed = tf.keras.layers.BatchNormalization()\n",
    "    self.drop1_bed = tf.keras.layers.Dropout(dropout)\n",
    "    self.dense1_bed = tf.keras.layers.Dense(nodes1, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.bn2_bed = tf.keras.layers.BatchNormalization()\n",
    "    self.drop2_bed = tf.keras.layers.Dropout(dropout)\n",
    "    #self.dense2_bed = tf.keras.layers.Dense(nodes2, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.dense2_bed = tf.keras.layers.Dense(nodes2, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "       \n",
    "      # dining\n",
    "    self.bn1_dining = tf.keras.layers.BatchNormalization()\n",
    "    self.drop1_dining = tf.keras.layers.Dropout(dropout)\n",
    "    self.dense1_dining = tf.keras.layers.Dense(nodes1, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.bn2_dining = tf.keras.layers.BatchNormalization()\n",
    "    self.drop2_dining = tf.keras.layers.Dropout(dropout)\n",
    "    #self.dense2_dining = tf.keras.layers.Dense(nodes2, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.dense2_dining = tf.keras.layers.Dense(nodes2, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "   \n",
    "    # hall\n",
    "    self.bn1_hall = tf.keras.layers.BatchNormalization()\n",
    "    self.drop1_hall = tf.keras.layers.Dropout(dropout)\n",
    "    self.dense1_hall = tf.keras.layers.Dense(nodes1, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.bn2_hall = tf.keras.layers.BatchNormalization()\n",
    "    self.drop2_hall = tf.keras.layers.Dropout(dropout)\n",
    "    #self.dense2_hall = tf.keras.layers.Dense(nodes2, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.dense2_hall = tf.keras.layers.Dense(nodes2, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    \n",
    "      # kitchen\n",
    "    self.bn1_kitchen = tf.keras.layers.BatchNormalization()\n",
    "    self.drop1_kitchen = tf.keras.layers.Dropout(dropout)\n",
    "    self.dense1_kitchen = tf.keras.layers.Dense(nodes1, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.bn2_kitchen = tf.keras.layers.BatchNormalization()\n",
    "    self.drop2_kitchen = tf.keras.layers.Dropout(dropout)\n",
    "    #self.dense2_kitchen = tf.keras.layers.Dense(nodes2, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.dense2_kitchen = tf.keras.layers.Dense(nodes2, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "  \n",
    "      # livingroom\n",
    "    self.bn1_living = tf.keras.layers.BatchNormalization()\n",
    "    self.drop1_living = tf.keras.layers.Dropout(dropout)\n",
    "    self.dense1_living = tf.keras.layers.Dense(nodes1, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.bn2_living = tf.keras.layers.BatchNormalization()\n",
    "    self.drop2_living = tf.keras.layers.Dropout(dropout)\n",
    "    #self.dense2_living = tf.keras.layers.Dense(nodes2, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "    self.dense2_living = tf.keras.layers.Dense(nodes2, kernel_regularizer = keras.regularizers.l2(l2))\n",
    "\n",
    "    # final prediction\n",
    "    self.bn_final = tf.keras.layers.BatchNormalization()\n",
    "    self.drop_final = tf.keras.layers.Dropout(dropout)\n",
    "    self.dense_final = tf.keras.layers.Dense(1, kernel_constraint =weight_constr())\n",
    "    \n",
    "  def call(self, inputs, training = None):\n",
    "    \n",
    "    # bathroom\n",
    "    x = self.bn1_bath(inputs[0])\n",
    "    x = self.drop1_bath(x)\n",
    "    x = self.dense1_bath(x)\n",
    "    x = self.bn2_bath(x)\n",
    "    x = self.drop2_bath(x)\n",
    "    bath_out = self.dense2_bath(x)\n",
    "    \n",
    "    \n",
    "    # bedroom\n",
    "    x = self.bn1_bed(inputs[1])\n",
    "    x = self.drop1_bed(x)\n",
    "    x = self.dense1_bed(x)\n",
    "    x = self.bn2_bed(x)\n",
    "    x = self.drop2_bed(x)\n",
    "    bed_out = self.dense2_bed(x)\n",
    "    \n",
    "    # diningroom\n",
    "    x = self.bn1_dining(inputs[2])\n",
    "    x = self.drop1_dining(x)\n",
    "    x = self.dense1_dining(x)\n",
    "    x = self.bn2_dining(x)\n",
    "    x = self.drop2_dining(x)\n",
    "    dining_out = self.dense2_dining(x)\n",
    "    \n",
    "    # hallroom\n",
    "    x = self.bn1_hall(inputs[3])\n",
    "    x = self.drop1_hall(x)\n",
    "    x = self.dense1_hall(x)\n",
    "    x = self.bn2_hall(x)\n",
    "    x = self.drop2_hall(x)\n",
    "    hall_out = self.dense2_hall(x)\n",
    "    \n",
    "    # kitchen\n",
    "    x = self.bn1_kitchen(inputs[4])\n",
    "    x = self.drop1_kitchen(x)\n",
    "    x = self.dense1_kitchen(x)\n",
    "    x = self.bn2_kitchen(x)\n",
    "    x = self.drop2_kitchen(x)\n",
    "    kitchen_out = self.dense2_kitchen(x)\n",
    "    \n",
    "    # livingroom\n",
    "    x = self.bn1_living(inputs[5])\n",
    "    x = self.drop1_living(x)\n",
    "    x = self.dense1_living(x)\n",
    "    x = self.bn2_living(x)\n",
    "    x = self.drop2_living(x)\n",
    "    living_out = self.dense2_living(x)\n",
    "    \n",
    "    # others\n",
    "    # x = self.bn1_others(inputs[6])\n",
    "    # x = self.drop1_others(x)\n",
    "    # x = self.dense1_others(x)\n",
    "    # x = self.bn2_others(x)\n",
    "    # x = self.drop2_others(x)\n",
    "    # others_out = self.dense2_others(x)\n",
    "\n",
    "    # join\n",
    "    out = tf.keras.layers.concatenate([bath_out, bed_out, dining_out, hall_out, kitchen_out, living_out])#, others_out])\n",
    "    out = self.bn_final(out)\n",
    "    out = self.drop_final(out)\n",
    "    return self.dense_final(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['img_model_6/batch_normalization_280/gamma:0', 'img_model_6/batch_normalization_280/beta:0', 'img_model_6/dense_284/bias:0', 'img_model_6/batch_normalization_281/gamma:0', 'img_model_6/batch_normalization_281/beta:0', 'img_model_6/dense_285/bias:0', 'img_model_6/batch_normalization_282/gamma:0', 'img_model_6/batch_normalization_282/beta:0', 'img_model_6/dense_286/bias:0', 'img_model_6/batch_normalization_283/gamma:0', 'img_model_6/batch_normalization_283/beta:0', 'img_model_6/dense_287/bias:0', 'img_model_6/batch_normalization_284/gamma:0', 'img_model_6/batch_normalization_284/beta:0', 'img_model_6/dense_288/bias:0', 'img_model_6/batch_normalization_285/gamma:0', 'img_model_6/batch_normalization_285/beta:0', 'img_model_6/dense_289/bias:0', 'img_model_6/batch_normalization_286/gamma:0', 'img_model_6/batch_normalization_286/beta:0', 'img_model_6/dense_290/bias:0', 'img_model_6/batch_normalization_287/gamma:0', 'img_model_6/batch_normalization_287/beta:0', 'img_model_6/dense_291/bias:0', 'img_model_6/batch_normalization_288/gamma:0', 'img_model_6/batch_normalization_288/beta:0', 'img_model_6/dense_292/bias:0', 'img_model_6/batch_normalization_289/gamma:0', 'img_model_6/batch_normalization_289/beta:0', 'img_model_6/dense_293/bias:0', 'img_model_6/batch_normalization_290/gamma:0', 'img_model_6/batch_normalization_290/beta:0', 'img_model_6/dense_294/bias:0', 'img_model_6/batch_normalization_291/gamma:0', 'img_model_6/batch_normalization_291/beta:0', 'img_model_6/dense_295/bias:0', 'img_model_6/batch_normalization_292/gamma:0', 'img_model_6/batch_normalization_292/beta:0', 'img_model_6/dense_296/kernel:0', 'img_model_6/dense_296/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['img_model_6/batch_normalization_280/gamma:0', 'img_model_6/batch_normalization_280/beta:0', 'img_model_6/dense_284/bias:0', 'img_model_6/batch_normalization_281/gamma:0', 'img_model_6/batch_normalization_281/beta:0', 'img_model_6/dense_285/bias:0', 'img_model_6/batch_normalization_282/gamma:0', 'img_model_6/batch_normalization_282/beta:0', 'img_model_6/dense_286/bias:0', 'img_model_6/batch_normalization_283/gamma:0', 'img_model_6/batch_normalization_283/beta:0', 'img_model_6/dense_287/bias:0', 'img_model_6/batch_normalization_284/gamma:0', 'img_model_6/batch_normalization_284/beta:0', 'img_model_6/dense_288/bias:0', 'img_model_6/batch_normalization_285/gamma:0', 'img_model_6/batch_normalization_285/beta:0', 'img_model_6/dense_289/bias:0', 'img_model_6/batch_normalization_286/gamma:0', 'img_model_6/batch_normalization_286/beta:0', 'img_model_6/dense_290/bias:0', 'img_model_6/batch_normalization_287/gamma:0', 'img_model_6/batch_normalization_287/beta:0', 'img_model_6/dense_291/bias:0', 'img_model_6/batch_normalization_288/gamma:0', 'img_model_6/batch_normalization_288/beta:0', 'img_model_6/dense_292/bias:0', 'img_model_6/batch_normalization_289/gamma:0', 'img_model_6/batch_normalization_289/beta:0', 'img_model_6/dense_293/bias:0', 'img_model_6/batch_normalization_290/gamma:0', 'img_model_6/batch_normalization_290/beta:0', 'img_model_6/dense_294/bias:0', 'img_model_6/batch_normalization_291/gamma:0', 'img_model_6/batch_normalization_291/beta:0', 'img_model_6/dense_295/bias:0', 'img_model_6/batch_normalization_292/gamma:0', 'img_model_6/batch_normalization_292/beta:0', 'img_model_6/dense_296/kernel:0', 'img_model_6/dense_296/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 21:28:09.584644: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 6s 154ms/step - loss: 0.0000e+00\n",
      "Epoch 2/2\n",
      "22/22 [==============================] - 4s 154ms/step - loss: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 21:28:18.626581: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open logs/price/ensemble/final/log_price_atleast4/checkpoint: Failed precondition: logs/price/ensemble/final/log_price_atleast4/checkpoint; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x3a6f65f40>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_img = Img_model(dropout = 0.2, l2 = 0, nodes1 = 512, nodes2 = 1)\n",
    "model_img.compile()\n",
    "model_img.fit(train_dataset, epochs = 2)\n",
    "model_img.load_weights(\"logs/price/ensemble/final/log_price_atleast4/checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Compound with reloaded weights (frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = load_data_fold(fold=4, drop_id = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SHAPE:  (2710, 77)\n",
      "TEST SHAPE:  (853, 77)\n",
      "VAL SHAPE:  (706, 77)\n"
     ]
    }
   ],
   "source": [
    "# merge data for tabnet and images to get consistent ids\n",
    "data_df_train = pd.merge(final_df, X_train, on=\"id\", how=\"inner\")\n",
    "data_df_train.dropna(inplace = True)\n",
    "print(\"TRAIN SHAPE: \", data_df_train.shape)\n",
    "\n",
    "data_df_test = pd.merge(final_df, X_test, on=\"id\", how=\"inner\")\n",
    "data_df_test.dropna(inplace = True)\n",
    "print(\"TEST SHAPE: \", data_df_test.shape)\n",
    "\n",
    "data_df_val = pd.merge(final_df, X_val, on=\"id\", how=\"inner\")\n",
    "data_df_val.dropna(inplace = True)\n",
    "print(\"VAL SHAPE: \", data_df_val.shape)\n",
    "\n",
    "# extract consistent columns\n",
    "X_train_tab = data_df_train.filter(col_names)\n",
    "X_test_tab = data_df_test.filter(col_names)\n",
    "X_val_tab = data_df_val.filter(col_names)\n",
    "\n",
    "data_df_train.drop(col_names, axis = 1, inplace = True)\n",
    "data_df_test.drop(col_names, axis = 1, inplace = True)\n",
    "data_df_val.drop(col_names, axis = 1, inplace = True)\n",
    "\n",
    "# extract consistent price\n",
    "y_train = data_df_train.pop(\"log_price\")\n",
    "y_test = data_df_test.pop(\"log_price\")\n",
    "y_val = data_df_val.pop(\"log_price\")\n",
    "\n",
    "# drop id\n",
    "X_train = data_df_train.drop(\"id\", axis = 1)\n",
    "X_test = data_df_test.drop(\"id\", axis = 1)\n",
    "X_val = data_df_val.drop(\"id\", axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(X_train.shape[0] * 0.9)\n",
    "batch_size = int(X_train.shape[0] * 0.1)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(features, prices, random_state = 123, test_size = 0.2)\n",
    "# X_train, X_val, y_train, y_val  = train_test_split(X_train, y_train, test_size=0.2, random_state=1) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "data_train = tf.data.Dataset.from_tensor_slices({\"bath\": np.squeeze(np.stack(X_train[\"bath\"])),\n",
    "                                                 \"bed\": np.squeeze(np.stack(X_train[\"bed\"])),\n",
    "                                                 \"dining\": np.squeeze(np.stack(X_train[\"dining\"])),\n",
    "                                                 \"hall\": np.squeeze(np.stack(X_train[\"hall\"])),\n",
    "                                                 \"kitchen\": np.squeeze(np.stack(X_train[\"kitchen\"])),\n",
    "                                                 \"living\": np.squeeze(np.stack(X_train[\"living\"])),\n",
    "                                                 \"features\": X_train_tab,\n",
    "                                                 \"price\": y_train})\n",
    "data_train = data_train.cache()\n",
    "data_train = data_train.shuffle(6000, seed = 13)\n",
    "train_dataset = data_train.take(len(y_train))\n",
    "train_dataset = train_dataset.map(transform)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "data_test = tf.data.Dataset.from_tensor_slices({\"bath\": np.squeeze(np.stack(X_test[\"bath\"])),\n",
    "                                                 \"bed\": np.squeeze(np.stack(X_test[\"bed\"])),\n",
    "                                                 \"dining\": np.squeeze(np.stack(X_test[\"dining\"])),\n",
    "                                                 \"hall\": np.squeeze(np.stack(X_test[\"hall\"])),\n",
    "                                                 \"kitchen\": np.squeeze(np.stack(X_test[\"kitchen\"])),\n",
    "                                                 \"living\": np.squeeze(np.stack(X_test[\"living\"])),\n",
    "                                                 \"features\": X_test_tab,\n",
    "                                                 \"price\": y_test})\n",
    "data_test = data_test.cache()\n",
    "test_dataset = data_test.take(len(y_test))\n",
    "test_dataset = test_dataset.map(transform)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "data_val = tf.data.Dataset.from_tensor_slices({\"bath\": np.squeeze(np.stack(X_val[\"bath\"])),\n",
    "                                                 \"bed\": np.squeeze(np.stack(X_val[\"bed\"])),\n",
    "                                                 \"dining\": np.squeeze(np.stack(X_val[\"dining\"])),\n",
    "                                                 \"hall\": np.squeeze(np.stack(X_val[\"hall\"])),\n",
    "                                                 \"kitchen\": np.squeeze(np.stack(X_val[\"kitchen\"])),\n",
    "                                                 \"living\": np.squeeze(np.stack(X_val[\"living\"])),\n",
    "                                                 \"features\": X_val_tab,\n",
    "                                                 \"price\": y_val})\n",
    "data_val = data_val.cache()\n",
    "val_dataset = data_val.take(len(y_val))\n",
    "val_dataset = val_dataset.map(transform)\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 21:30:31.422937: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - ETA: 0s - loss: 9.4345 - R_squared: -22.3258 - mae: 3.0027"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 21:30:36.576661: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 8s 570ms/step - loss: 9.4345 - R_squared: -22.3258 - mae: 3.0027 - val_loss: 8.0091 - val_R_squared: -17.9431 - val_mae: 2.7875\n",
      "Epoch 2/300\n",
      "10/10 [==============================] - 6s 585ms/step - loss: 8.3593 - R_squared: -19.5954 - mae: 2.8145 - val_loss: 7.2311 - val_R_squared: -16.1126 - val_mae: 2.6440\n",
      "Epoch 3/300\n",
      "10/10 [==============================] - 5s 551ms/step - loss: 7.6460 - R_squared: -17.7314 - mae: 2.6780 - val_loss: 6.7404 - val_R_squared: -14.9769 - val_mae: 2.5475\n",
      "Epoch 4/300\n",
      "10/10 [==============================] - 5s 465ms/step - loss: 7.0294 - R_squared: -16.3749 - mae: 2.5571 - val_loss: 6.1967 - val_R_squared: -13.6994 - val_mae: 2.4357\n",
      "Epoch 5/300\n",
      "10/10 [==============================] - 5s 469ms/step - loss: 6.3974 - R_squared: -14.7609 - mae: 2.4320 - val_loss: 5.6842 - val_R_squared: -12.4846 - val_mae: 2.3261\n",
      "Epoch 6/300\n",
      "10/10 [==============================] - 5s 443ms/step - loss: 5.8126 - R_squared: -13.2360 - mae: 2.3099 - val_loss: 5.1706 - val_R_squared: -11.2600 - val_mae: 2.2127\n",
      "Epoch 7/300\n",
      "10/10 [==============================] - 5s 461ms/step - loss: 5.1183 - R_squared: -11.5924 - mae: 2.1590 - val_loss: 4.7187 - val_R_squared: -10.1722 - val_mae: 2.1077\n",
      "Epoch 8/300\n",
      "10/10 [==============================] - 5s 462ms/step - loss: 4.4230 - R_squared: -9.8715 - mae: 1.9947 - val_loss: 4.2465 - val_R_squared: -9.0347 - val_mae: 1.9919\n",
      "Epoch 9/300\n",
      "10/10 [==============================] - 5s 470ms/step - loss: 3.9158 - R_squared: -8.6329 - mae: 1.8584 - val_loss: 3.8454 - val_R_squared: -8.0667 - val_mae: 1.8846\n",
      "Epoch 10/300\n",
      "10/10 [==============================] - 5s 479ms/step - loss: 3.2750 - R_squared: -7.0231 - mae: 1.6841 - val_loss: 3.7233 - val_R_squared: -7.7637 - val_mae: 1.8387\n",
      "Epoch 11/300\n",
      "10/10 [==============================] - 5s 459ms/step - loss: 2.8538 - R_squared: -6.0076 - mae: 1.5445 - val_loss: 3.3202 - val_R_squared: -6.8056 - val_mae: 1.7219\n",
      "Epoch 12/300\n",
      "10/10 [==============================] - 5s 499ms/step - loss: 2.4912 - R_squared: -5.1014 - mae: 1.4114 - val_loss: 2.5103 - val_R_squared: -4.8913 - val_mae: 1.4875\n",
      "Epoch 13/300\n",
      "10/10 [==============================] - 5s 492ms/step - loss: 2.2013 - R_squared: -4.4121 - mae: 1.2915 - val_loss: 2.1307 - val_R_squared: -3.9958 - val_mae: 1.3578\n",
      "Epoch 14/300\n",
      "10/10 [==============================] - 5s 492ms/step - loss: 1.8540 - R_squared: -3.5484 - mae: 1.1549 - val_loss: 2.0826 - val_R_squared: -3.8803 - val_mae: 1.3278\n",
      "Epoch 15/300\n",
      "10/10 [==============================] - 5s 499ms/step - loss: 1.7099 - R_squared: -3.2118 - mae: 1.0747 - val_loss: 1.8477 - val_R_squared: -3.3241 - val_mae: 1.2352\n",
      "Epoch 16/300\n",
      "10/10 [==============================] - 5s 511ms/step - loss: 1.4834 - R_squared: -2.6303 - mae: 0.9683 - val_loss: 1.5811 - val_R_squared: -2.6935 - val_mae: 1.1279\n",
      "Epoch 17/300\n",
      "10/10 [==============================] - 5s 506ms/step - loss: 1.3731 - R_squared: -2.3535 - mae: 0.9058 - val_loss: 1.2798 - val_R_squared: -1.9798 - val_mae: 0.9994\n",
      "Epoch 18/300\n",
      "10/10 [==============================] - 5s 479ms/step - loss: 1.2879 - R_squared: -2.1583 - mae: 0.8551 - val_loss: 1.0157 - val_R_squared: -1.3548 - val_mae: 0.8759\n",
      "Epoch 19/300\n",
      "10/10 [==============================] - 5s 495ms/step - loss: 1.1204 - R_squared: -1.7515 - mae: 0.7817 - val_loss: 0.9391 - val_R_squared: -1.1745 - val_mae: 0.8306\n",
      "Epoch 20/300\n",
      "10/10 [==============================] - 5s 515ms/step - loss: 1.0713 - R_squared: -1.6517 - mae: 0.7568 - val_loss: 0.7912 - val_R_squared: -0.8236 - val_mae: 0.7495\n",
      "Epoch 21/300\n",
      "10/10 [==============================] - 5s 497ms/step - loss: 1.0298 - R_squared: -1.5253 - mae: 0.7389 - val_loss: 0.7023 - val_R_squared: -0.6136 - val_mae: 0.6963\n",
      "Epoch 22/300\n",
      "10/10 [==============================] - 5s 503ms/step - loss: 1.0387 - R_squared: -1.5497 - mae: 0.7400 - val_loss: 0.6999 - val_R_squared: -0.6062 - val_mae: 0.6902\n",
      "Epoch 23/300\n",
      "10/10 [==============================] - 5s 503ms/step - loss: 0.9945 - R_squared: -1.4485 - mae: 0.7245 - val_loss: 0.7092 - val_R_squared: -0.6287 - val_mae: 0.6914\n",
      "Epoch 24/300\n",
      "10/10 [==============================] - 5s 479ms/step - loss: 0.9058 - R_squared: -1.2157 - mae: 0.6995 - val_loss: 0.5075 - val_R_squared: -0.1527 - val_mae: 0.5697\n",
      "Epoch 25/300\n",
      "10/10 [==============================] - 5s 494ms/step - loss: 0.9020 - R_squared: -1.2215 - mae: 0.6916 - val_loss: 0.4283 - val_R_squared: 0.0362 - val_mae: 0.5160\n",
      "Epoch 26/300\n",
      "10/10 [==============================] - 5s 497ms/step - loss: 0.8570 - R_squared: -1.1031 - mae: 0.6706 - val_loss: 0.5006 - val_R_squared: -0.1350 - val_mae: 0.5629\n",
      "Epoch 27/300\n",
      "10/10 [==============================] - 5s 499ms/step - loss: 0.8433 - R_squared: -1.0709 - mae: 0.6831 - val_loss: 0.5332 - val_R_squared: -0.2139 - val_mae: 0.5822\n",
      "Epoch 28/300\n",
      "10/10 [==============================] - 5s 514ms/step - loss: 0.8730 - R_squared: -1.1370 - mae: 0.7053 - val_loss: 0.4817 - val_R_squared: -0.0925 - val_mae: 0.5484\n",
      "Epoch 29/300\n",
      "10/10 [==============================] - 5s 505ms/step - loss: 0.8568 - R_squared: -1.1024 - mae: 0.6851 - val_loss: 0.4813 - val_R_squared: -0.0890 - val_mae: 0.5482\n",
      "Epoch 30/300\n",
      "10/10 [==============================] - 5s 495ms/step - loss: 0.7993 - R_squared: -0.9615 - mae: 0.6735 - val_loss: 0.3805 - val_R_squared: 0.1498 - val_mae: 0.4795\n",
      "Epoch 31/300\n",
      "10/10 [==============================] - 5s 503ms/step - loss: 0.7789 - R_squared: -0.8991 - mae: 0.6616 - val_loss: 0.2722 - val_R_squared: 0.4074 - val_mae: 0.3963\n",
      "Epoch 32/300\n",
      "10/10 [==============================] - 6s 570ms/step - loss: 0.8083 - R_squared: -0.9816 - mae: 0.6762 - val_loss: 0.2889 - val_R_squared: 0.3654 - val_mae: 0.4103\n",
      "Epoch 33/300\n",
      "10/10 [==============================] - 5s 511ms/step - loss: 0.7439 - R_squared: -0.8355 - mae: 0.6533 - val_loss: 0.2978 - val_R_squared: 0.3447 - val_mae: 0.4168\n",
      "Epoch 34/300\n",
      "10/10 [==============================] - 5s 507ms/step - loss: 0.7746 - R_squared: -0.9102 - mae: 0.6702 - val_loss: 0.3101 - val_R_squared: 0.3160 - val_mae: 0.4261\n",
      "Epoch 35/300\n",
      "10/10 [==============================] - 5s 506ms/step - loss: 0.7699 - R_squared: -0.9009 - mae: 0.6735 - val_loss: 0.3951 - val_R_squared: 0.1136 - val_mae: 0.4899\n",
      "Epoch 36/300\n",
      "10/10 [==============================] - 5s 495ms/step - loss: 0.7488 - R_squared: -0.8342 - mae: 0.6609 - val_loss: 0.3315 - val_R_squared: 0.2654 - val_mae: 0.4426\n",
      "Epoch 37/300\n",
      "10/10 [==============================] - 6s 612ms/step - loss: 0.7628 - R_squared: -0.8745 - mae: 0.6689 - val_loss: 0.3001 - val_R_squared: 0.3417 - val_mae: 0.4182\n",
      "Epoch 38/300\n",
      "10/10 [==============================] - 6s 590ms/step - loss: 0.7677 - R_squared: -0.8777 - mae: 0.6779 - val_loss: 0.3386 - val_R_squared: 0.2481 - val_mae: 0.4481\n",
      "Epoch 39/300\n",
      "10/10 [==============================] - 6s 551ms/step - loss: 0.7319 - R_squared: -0.8081 - mae: 0.6660 - val_loss: 0.3408 - val_R_squared: 0.2423 - val_mae: 0.4498\n",
      "Epoch 40/300\n",
      "10/10 [==============================] - 6s 553ms/step - loss: 0.7568 - R_squared: -0.8497 - mae: 0.6750 - val_loss: 0.3282 - val_R_squared: 0.2729 - val_mae: 0.4399\n",
      "Epoch 41/300\n",
      "10/10 [==============================] - 6s 579ms/step - loss: 0.7632 - R_squared: -0.8731 - mae: 0.6711 - val_loss: 0.3843 - val_R_squared: 0.1397 - val_mae: 0.4819\n",
      "Epoch 42/300\n",
      "10/10 [==============================] - 6s 621ms/step - loss: 0.7619 - R_squared: -0.8741 - mae: 0.6713 - val_loss: 0.3556 - val_R_squared: 0.2072 - val_mae: 0.4604\n",
      "Epoch 43/300\n",
      "10/10 [==============================] - 6s 569ms/step - loss: 0.7140 - R_squared: -0.7425 - mae: 0.6573 - val_loss: 0.2918 - val_R_squared: 0.3586 - val_mae: 0.4117\n",
      "Epoch 44/300\n",
      "10/10 [==============================] - 6s 599ms/step - loss: 0.7147 - R_squared: -0.7489 - mae: 0.6551 - val_loss: 0.2820 - val_R_squared: 0.3823 - val_mae: 0.4045\n",
      "Epoch 45/300\n",
      "10/10 [==============================] - 6s 605ms/step - loss: 0.7000 - R_squared: -0.7230 - mae: 0.6442 - val_loss: 0.3090 - val_R_squared: 0.3178 - val_mae: 0.4260\n",
      "Epoch 46/300\n",
      "10/10 [==============================] - 6s 567ms/step - loss: 0.6838 - R_squared: -0.6798 - mae: 0.6412 - val_loss: 0.3257 - val_R_squared: 0.2787 - val_mae: 0.4383\n",
      "Epoch 47/300\n",
      "10/10 [==============================] - 5s 549ms/step - loss: 0.7003 - R_squared: -0.7151 - mae: 0.6516 - val_loss: 0.3169 - val_R_squared: 0.3003 - val_mae: 0.4314\n",
      "Epoch 48/300\n",
      "10/10 [==============================] - 5s 537ms/step - loss: 0.7092 - R_squared: -0.7348 - mae: 0.6507 - val_loss: 0.3074 - val_R_squared: 0.3218 - val_mae: 0.4250\n",
      "Epoch 49/300\n",
      "10/10 [==============================] - 6s 602ms/step - loss: 0.6746 - R_squared: -0.6558 - mae: 0.6423 - val_loss: 0.3019 - val_R_squared: 0.3354 - val_mae: 0.4205\n",
      "Epoch 50/300\n",
      "10/10 [==============================] - 6s 598ms/step - loss: 0.7264 - R_squared: -0.7822 - mae: 0.6566 - val_loss: 0.2818 - val_R_squared: 0.3846 - val_mae: 0.4045\n",
      "Epoch 51/300\n",
      "10/10 [==============================] - 6s 578ms/step - loss: 0.7047 - R_squared: -0.7291 - mae: 0.6475 - val_loss: 0.3080 - val_R_squared: 0.3211 - val_mae: 0.4261\n",
      "Epoch 52/300\n",
      "10/10 [==============================] - 5s 522ms/step - loss: 0.6858 - R_squared: -0.6858 - mae: 0.6407 - val_loss: 0.3030 - val_R_squared: 0.3339 - val_mae: 0.4222\n",
      "Epoch 53/300\n",
      "10/10 [==============================] - 5s 519ms/step - loss: 0.6822 - R_squared: -0.6752 - mae: 0.6433 - val_loss: 0.3211 - val_R_squared: 0.2899 - val_mae: 0.4362\n",
      "Epoch 54/300\n",
      "10/10 [==============================] - 5s 512ms/step - loss: 0.6798 - R_squared: -0.6729 - mae: 0.6406 - val_loss: 0.3534 - val_R_squared: 0.2139 - val_mae: 0.4600\n",
      "Epoch 55/300\n",
      "10/10 [==============================] - 5s 512ms/step - loss: 0.6884 - R_squared: -0.6840 - mae: 0.6400 - val_loss: 0.2816 - val_R_squared: 0.3846 - val_mae: 0.4041\n",
      "Epoch 56/300\n",
      "10/10 [==============================] - 5s 505ms/step - loss: 0.6774 - R_squared: -0.6723 - mae: 0.6327 - val_loss: 0.2475 - val_R_squared: 0.4672 - val_mae: 0.3753\n",
      "Epoch 57/300\n",
      "10/10 [==============================] - 5s 500ms/step - loss: 0.6791 - R_squared: -0.6834 - mae: 0.6357 - val_loss: 0.2970 - val_R_squared: 0.3468 - val_mae: 0.4171\n",
      "Epoch 58/300\n",
      "10/10 [==============================] - 5s 516ms/step - loss: 0.6559 - R_squared: -0.6050 - mae: 0.6237 - val_loss: 0.2871 - val_R_squared: 0.3716 - val_mae: 0.4092\n",
      "Epoch 59/300\n",
      "10/10 [==============================] - 5s 506ms/step - loss: 0.6605 - R_squared: -0.6235 - mae: 0.6247 - val_loss: 0.2876 - val_R_squared: 0.3704 - val_mae: 0.4095\n",
      "Epoch 60/300\n",
      "10/10 [==============================] - 6s 563ms/step - loss: 0.6532 - R_squared: -0.5992 - mae: 0.6222 - val_loss: 0.2898 - val_R_squared: 0.3640 - val_mae: 0.4116\n",
      "Epoch 61/300\n",
      "10/10 [==============================] - 5s 506ms/step - loss: 0.6446 - R_squared: -0.5784 - mae: 0.6193 - val_loss: 0.3202 - val_R_squared: 0.2914 - val_mae: 0.4357\n",
      "Epoch 62/300\n",
      "10/10 [==============================] - 5s 513ms/step - loss: 0.6736 - R_squared: -0.6518 - mae: 0.6320 - val_loss: 0.2811 - val_R_squared: 0.3869 - val_mae: 0.4048\n",
      "Epoch 63/300\n",
      "10/10 [==============================] - 5s 504ms/step - loss: 0.6290 - R_squared: -0.5440 - mae: 0.6094 - val_loss: 0.2830 - val_R_squared: 0.3810 - val_mae: 0.4071\n",
      "Epoch 64/300\n",
      "10/10 [==============================] - 5s 505ms/step - loss: 0.6294 - R_squared: -0.5474 - mae: 0.6124 - val_loss: 0.2691 - val_R_squared: 0.4140 - val_mae: 0.3956\n",
      "Epoch 65/300\n",
      "10/10 [==============================] - 5s 510ms/step - loss: 0.6222 - R_squared: -0.5344 - mae: 0.6094 - val_loss: 0.2923 - val_R_squared: 0.3588 - val_mae: 0.4137\n",
      "Epoch 66/300\n",
      "10/10 [==============================] - 5s 514ms/step - loss: 0.6314 - R_squared: -0.5449 - mae: 0.6111 - val_loss: 0.2968 - val_R_squared: 0.3475 - val_mae: 0.4174\n",
      "Epoch 67/300\n",
      "10/10 [==============================] - 5s 513ms/step - loss: 0.5996 - R_squared: -0.4689 - mae: 0.5967 - val_loss: 0.2579 - val_R_squared: 0.4423 - val_mae: 0.3848\n",
      "Epoch 68/300\n",
      "10/10 [==============================] - 5s 513ms/step - loss: 0.6426 - R_squared: -0.5703 - mae: 0.6180 - val_loss: 0.2365 - val_R_squared: 0.4964 - val_mae: 0.3648\n",
      "Epoch 69/300\n",
      "10/10 [==============================] - 5s 519ms/step - loss: 0.6115 - R_squared: -0.4991 - mae: 0.5982 - val_loss: 0.2614 - val_R_squared: 0.4335 - val_mae: 0.3882\n",
      "Epoch 70/300\n",
      "10/10 [==============================] - 5s 540ms/step - loss: 0.6353 - R_squared: -0.5584 - mae: 0.6124 - val_loss: 0.3329 - val_R_squared: 0.2616 - val_mae: 0.4450\n",
      "Epoch 71/300\n",
      "10/10 [==============================] - 6s 630ms/step - loss: 0.5683 - R_squared: -0.3944 - mae: 0.5839 - val_loss: 0.2963 - val_R_squared: 0.3483 - val_mae: 0.4170\n",
      "Epoch 72/300\n",
      "10/10 [==============================] - 6s 559ms/step - loss: 0.6311 - R_squared: -0.5491 - mae: 0.6100 - val_loss: 0.2352 - val_R_squared: 0.4987 - val_mae: 0.3644\n",
      "Epoch 73/300\n",
      "10/10 [==============================] - 6s 548ms/step - loss: 0.6196 - R_squared: -0.5164 - mae: 0.6065 - val_loss: 0.2566 - val_R_squared: 0.4448 - val_mae: 0.3839\n",
      "Epoch 74/300\n",
      "10/10 [==============================] - 5s 528ms/step - loss: 0.6184 - R_squared: -0.5128 - mae: 0.5992 - val_loss: 0.3505 - val_R_squared: 0.2202 - val_mae: 0.4583\n",
      "Epoch 75/300\n",
      "10/10 [==============================] - 5s 519ms/step - loss: 0.6260 - R_squared: -0.5379 - mae: 0.6061 - val_loss: 0.2679 - val_R_squared: 0.4181 - val_mae: 0.3938\n",
      "Epoch 76/300\n",
      "10/10 [==============================] - 5s 512ms/step - loss: 0.5772 - R_squared: -0.4230 - mae: 0.5855 - val_loss: 0.2443 - val_R_squared: 0.4761 - val_mae: 0.3722\n",
      "Epoch 77/300\n",
      "10/10 [==============================] - 5s 505ms/step - loss: 0.5841 - R_squared: -0.4335 - mae: 0.5846 - val_loss: 0.2691 - val_R_squared: 0.4145 - val_mae: 0.3950\n",
      "Epoch 78/300\n",
      "10/10 [==============================] - 5s 514ms/step - loss: 0.5783 - R_squared: -0.4288 - mae: 0.5896 - val_loss: 0.3188 - val_R_squared: 0.2955 - val_mae: 0.4344\n",
      "Epoch 79/300\n",
      "10/10 [==============================] - 6s 575ms/step - loss: 0.6139 - R_squared: -0.5023 - mae: 0.6000 - val_loss: 0.2681 - val_R_squared: 0.4174 - val_mae: 0.3938\n",
      "Epoch 80/300\n",
      "10/10 [==============================] - 5s 530ms/step - loss: 0.5754 - R_squared: -0.4168 - mae: 0.5813 - val_loss: 0.3442 - val_R_squared: 0.2350 - val_mae: 0.4539\n",
      "Epoch 81/300\n",
      "10/10 [==============================] - 5s 514ms/step - loss: 0.6093 - R_squared: -0.4954 - mae: 0.5936 - val_loss: 0.2517 - val_R_squared: 0.4562 - val_mae: 0.3807\n",
      "Epoch 82/300\n",
      "10/10 [==============================] - 5s 519ms/step - loss: 0.5708 - R_squared: -0.3983 - mae: 0.5815 - val_loss: 0.2892 - val_R_squared: 0.3827 - val_mae: 0.4167\n",
      "Epoch 83/300\n",
      "10/10 [==============================] - 5s 528ms/step - loss: 0.6048 - R_squared: -0.4830 - mae: 0.5945 - val_loss: 0.2489 - val_R_squared: 0.4721 - val_mae: 0.3787\n",
      "Epoch 84/300\n",
      "10/10 [==============================] - 5s 540ms/step - loss: 0.5697 - R_squared: -0.3970 - mae: 0.5775 - val_loss: 0.2402 - val_R_squared: 0.4854 - val_mae: 0.3701\n",
      "Epoch 85/300\n",
      "10/10 [==============================] - 5s 518ms/step - loss: 0.5725 - R_squared: -0.4108 - mae: 0.5813 - val_loss: 0.2950 - val_R_squared: 0.3521 - val_mae: 0.4165\n",
      "Epoch 86/300\n",
      "10/10 [==============================] - 5s 510ms/step - loss: 0.5411 - R_squared: -0.3260 - mae: 0.5657 - val_loss: 0.3573 - val_R_squared: 0.2040 - val_mae: 0.4637\n",
      "Epoch 87/300\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5777 - R_squared: -0.4232 - mae: 0.5759"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/dmnk/OneDrive - stud.uni-goettingen.de/Dokumente/3. Semester/SeminarDL/DubAir/TabNet.ipynb Cell 38'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dmnk/OneDrive%20-%20stud.uni-goettingen.de/Dokumente/3.%20Semester/SeminarDL/DubAir/TabNet.ipynb#ch0000090?line=3'>4</a>\u001b[0m optimizer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(lr)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dmnk/OneDrive%20-%20stud.uni-goettingen.de/Dokumente/3.%20Semester/SeminarDL/DubAir/TabNet.ipynb#ch0000090?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m , metrics\u001b[39m=\u001b[39m[R_squared,\u001b[39m\"\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/dmnk/OneDrive%20-%20stud.uni-goettingen.de/Dokumente/3.%20Semester/SeminarDL/DubAir/TabNet.ipynb#ch0000090?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_dataset, validation_data \u001b[39m=\u001b[39;49m val_dataset, epochs\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1214\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1199'>1200</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_eval_data_handler\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1200'>1201</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eval_data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39mget_data_handler(\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1201'>1202</a>\u001b[0m       x\u001b[39m=\u001b[39mval_x,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1202'>1203</a>\u001b[0m       y\u001b[39m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1211'>1212</a>\u001b[0m       model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1212'>1213</a>\u001b[0m       steps_per_execution\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution)\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1213'>1214</a>\u001b[0m val_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1214'>1215</a>\u001b[0m     x\u001b[39m=\u001b[39;49mval_x,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1215'>1216</a>\u001b[0m     y\u001b[39m=\u001b[39;49mval_y,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1216'>1217</a>\u001b[0m     sample_weight\u001b[39m=\u001b[39;49mval_sample_weight,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1217'>1218</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mvalidation_batch_size \u001b[39mor\u001b[39;49;00m batch_size,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1218'>1219</a>\u001b[0m     steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1219'>1220</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1220'>1221</a>\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1221'>1222</a>\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1222'>1223</a>\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1223'>1224</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1224'>1225</a>\u001b[0m     _use_cached_eval_dataset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1225'>1226</a>\u001b[0m val_logs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1226'>1227</a>\u001b[0m epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1483\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1480'>1481</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_test_counter\u001b[39m.\u001b[39massign(\u001b[39m0\u001b[39m)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1481'>1482</a>\u001b[0m callbacks\u001b[39m.\u001b[39mon_test_begin()\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1482'>1483</a>\u001b[0m \u001b[39mfor\u001b[39;00m _, iterator \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39menumerate_epochs():  \u001b[39m# Single epoch.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1483'>1484</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_metrics()\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py?line=1484'>1485</a>\u001b[0m   \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py:1199\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=1196'>1197</a>\u001b[0m \u001b[39m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=1197'>1198</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=1198'>1199</a>\u001b[0m   data_iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=1199'>1200</a>\u001b[0m   \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_epoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epochs):\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py?line=1200'>1201</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:486\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=483'>484</a>\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly() \u001b[39mor\u001b[39;00m ops\u001b[39m.\u001b[39minside_function():\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=484'>485</a>\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcolocate_with(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=485'>486</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m iterator_ops\u001b[39m.\u001b[39;49mOwnedIterator(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=486'>487</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=487'>488</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m__iter__() is only supported inside of tf.function \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=488'>489</a>\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mor when eager execution is enabled.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:696\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=693'>694</a>\u001b[0m \u001b[39mif\u001b[39;00m (components \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m element_spec \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=694'>695</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(error_message)\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=695'>696</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_iterator(dataset)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:719\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=713'>714</a>\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcolocate_with(ds_variant):\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=714'>715</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator_resource, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deleter \u001b[39m=\u001b[39m (\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=715'>716</a>\u001b[0m       gen_dataset_ops\u001b[39m.\u001b[39manonymous_iterator_v2(\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=716'>717</a>\u001b[0m           output_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_output_types,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=717'>718</a>\u001b[0m           output_shapes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_output_shapes))\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=718'>719</a>\u001b[0m   gen_dataset_ops\u001b[39m.\u001b[39;49mmake_iterator(ds_variant, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource)\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=719'>720</a>\u001b[0m   \u001b[39m# Delete the resource when this object is deleted\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=720'>721</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_resource_deleter \u001b[39m=\u001b[39m IteratorResourceDeleter(\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=721'>722</a>\u001b[0m       handle\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator_resource,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=722'>723</a>\u001b[0m       deleter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deleter)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3120\u001b[0m, in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py?line=3117'>3118</a>\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py?line=3118'>3119</a>\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py?line=3119'>3120</a>\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py?line=3120'>3121</a>\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMakeIterator\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, dataset, iterator)\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py?line=3121'>3122</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/tabnet_tf/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py?line=3122'>3123</a>\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(2)\n",
    "\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(0.01, decay_steps=100, decay_rate=0.95, staircase=False)\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "model.compile(optimizer, loss='mse' , metrics=[R_squared,\"mae\"])\n",
    "\n",
    "model.fit(train_dataset, validation_data = val_dataset, epochs=1, verbose=1)\n",
    "\n",
    "# set layer weights to pretrained weights\n",
    "model.layers[1:-4] = model_img.layers\n",
    "model.layers[0] = model_tn.layers[0]\n",
    "\n",
    "# freeze layer weights\n",
    "for layer in model.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model.fit(train_dataset, validation_data = val_dataset, epochs=300, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unfreeze weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 21:38:47.148936: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - ETA: 0s - loss: 0.5776 - R_squared: -0.4180 - mae: 0.5802"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 21:38:54.982926: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 18s 671ms/step - loss: 0.5776 - R_squared: -0.4180 - mae: 0.5802 - val_loss: 0.2736 - val_R_squared: 0.4033 - val_mae: 0.3993\n",
      "Epoch 2/300\n",
      "10/10 [==============================] - 6s 548ms/step - loss: 0.5732 - R_squared: -0.4072 - mae: 0.5771 - val_loss: 0.2592 - val_R_squared: 0.4383 - val_mae: 0.3870\n",
      "Epoch 3/300\n",
      "10/10 [==============================] - 7s 701ms/step - loss: 0.5932 - R_squared: -0.4619 - mae: 0.5861 - val_loss: 0.2511 - val_R_squared: 0.4582 - val_mae: 0.3798\n",
      "Epoch 4/300\n",
      "10/10 [==============================] - 7s 667ms/step - loss: 0.5630 - R_squared: -0.3790 - mae: 0.5712 - val_loss: 0.2456 - val_R_squared: 0.4718 - val_mae: 0.3747\n",
      "Epoch 5/300\n",
      "10/10 [==============================] - 7s 603ms/step - loss: 0.5536 - R_squared: -0.3521 - mae: 0.5685 - val_loss: 0.2427 - val_R_squared: 0.4790 - val_mae: 0.3721\n",
      "Epoch 6/300\n",
      "10/10 [==============================] - 7s 686ms/step - loss: 0.5486 - R_squared: -0.3422 - mae: 0.5786 - val_loss: 0.2424 - val_R_squared: 0.4797 - val_mae: 0.3719\n",
      "Epoch 7/300\n",
      "10/10 [==============================] - 7s 648ms/step - loss: 0.5595 - R_squared: -0.3711 - mae: 0.5683 - val_loss: 0.2416 - val_R_squared: 0.4815 - val_mae: 0.3712\n",
      "Epoch 8/300\n",
      "10/10 [==============================] - 6s 584ms/step - loss: 0.5557 - R_squared: -0.3731 - mae: 0.5729 - val_loss: 0.2426 - val_R_squared: 0.4790 - val_mae: 0.3722\n",
      "Epoch 9/300\n",
      "10/10 [==============================] - 6s 552ms/step - loss: 0.5595 - R_squared: -0.3746 - mae: 0.5684 - val_loss: 0.2439 - val_R_squared: 0.4759 - val_mae: 0.3733\n",
      "Epoch 10/300\n",
      "10/10 [==============================] - 6s 587ms/step - loss: 0.5579 - R_squared: -0.3676 - mae: 0.5704 - val_loss: 0.2446 - val_R_squared: 0.4742 - val_mae: 0.3738\n",
      "Epoch 11/300\n",
      "10/10 [==============================] - 7s 681ms/step - loss: 0.5685 - R_squared: -0.3909 - mae: 0.5710 - val_loss: 0.2452 - val_R_squared: 0.4727 - val_mae: 0.3743\n",
      "Epoch 12/300\n",
      "10/10 [==============================] - 6s 565ms/step - loss: 0.5385 - R_squared: -0.3296 - mae: 0.5560 - val_loss: 0.2474 - val_R_squared: 0.4673 - val_mae: 0.3763\n",
      "Epoch 13/300\n",
      "10/10 [==============================] - 6s 562ms/step - loss: 0.5574 - R_squared: -0.3698 - mae: 0.5671 - val_loss: 0.2484 - val_R_squared: 0.4648 - val_mae: 0.3771\n",
      "Epoch 14/300\n",
      "10/10 [==============================] - 6s 565ms/step - loss: 0.5584 - R_squared: -0.3731 - mae: 0.5740 - val_loss: 0.2484 - val_R_squared: 0.4649 - val_mae: 0.3770\n",
      "Epoch 15/300\n",
      "10/10 [==============================] - 6s 583ms/step - loss: 0.5639 - R_squared: -0.3853 - mae: 0.5701 - val_loss: 0.2520 - val_R_squared: 0.4562 - val_mae: 0.3802\n",
      "Epoch 16/300\n",
      "10/10 [==============================] - 6s 578ms/step - loss: 0.5464 - R_squared: -0.3356 - mae: 0.5634 - val_loss: 0.2512 - val_R_squared: 0.4580 - val_mae: 0.3797\n",
      "Epoch 17/300\n",
      "10/10 [==============================] - 6s 559ms/step - loss: 0.5581 - R_squared: -0.3680 - mae: 0.5731 - val_loss: 0.2531 - val_R_squared: 0.4532 - val_mae: 0.3816\n",
      "Epoch 18/300\n",
      "10/10 [==============================] - 6s 546ms/step - loss: 0.5402 - R_squared: -0.3225 - mae: 0.5627 - val_loss: 0.2529 - val_R_squared: 0.4537 - val_mae: 0.3813\n",
      "Epoch 19/300\n",
      "10/10 [==============================] - 7s 622ms/step - loss: 0.5622 - R_squared: -0.3781 - mae: 0.5732 - val_loss: 0.2571 - val_R_squared: 0.4437 - val_mae: 0.3850\n",
      "Epoch 20/300\n",
      "10/10 [==============================] - 7s 671ms/step - loss: 0.5447 - R_squared: -0.3332 - mae: 0.5598 - val_loss: 0.2594 - val_R_squared: 0.4382 - val_mae: 0.3869\n",
      "Epoch 21/300\n",
      "10/10 [==============================] - 6s 612ms/step - loss: 0.5486 - R_squared: -0.3420 - mae: 0.5643 - val_loss: 0.2661 - val_R_squared: 0.4222 - val_mae: 0.3924\n",
      "Epoch 22/300\n",
      "10/10 [==============================] - 6s 581ms/step - loss: 0.5620 - R_squared: -0.3859 - mae: 0.5734 - val_loss: 0.2652 - val_R_squared: 0.4243 - val_mae: 0.3917\n",
      "Epoch 23/300\n",
      "10/10 [==============================] - 6s 555ms/step - loss: 0.5305 - R_squared: -0.3023 - mae: 0.5607 - val_loss: 0.2636 - val_R_squared: 0.4282 - val_mae: 0.3904\n",
      "Epoch 24/300\n",
      "10/10 [==============================] - 6s 582ms/step - loss: 0.5358 - R_squared: -0.3193 - mae: 0.5594 - val_loss: 0.2603 - val_R_squared: 0.4359 - val_mae: 0.3876\n",
      "Epoch 25/300\n",
      "10/10 [==============================] - 6s 543ms/step - loss: 0.5470 - R_squared: -0.3419 - mae: 0.5638 - val_loss: 0.2621 - val_R_squared: 0.4319 - val_mae: 0.3890\n",
      "Epoch 26/300\n",
      "10/10 [==============================] - 6s 564ms/step - loss: 0.5395 - R_squared: -0.3205 - mae: 0.5610 - val_loss: 0.2662 - val_R_squared: 0.4222 - val_mae: 0.3923\n",
      "Epoch 27/300\n",
      "10/10 [==============================] - 6s 554ms/step - loss: 0.5425 - R_squared: -0.3272 - mae: 0.5619 - val_loss: 0.2734 - val_R_squared: 0.4048 - val_mae: 0.3982\n",
      "Epoch 28/300\n",
      "10/10 [==============================] - 6s 552ms/step - loss: 0.5919 - R_squared: -0.4457 - mae: 0.5754 - val_loss: 0.2730 - val_R_squared: 0.4060 - val_mae: 0.3977\n",
      "Epoch 29/300\n",
      "10/10 [==============================] - 6s 583ms/step - loss: 0.5277 - R_squared: -0.2944 - mae: 0.5524 - val_loss: 0.2697 - val_R_squared: 0.4139 - val_mae: 0.3950\n",
      "Epoch 30/300\n",
      "10/10 [==============================] - 6s 542ms/step - loss: 0.5397 - R_squared: -0.3270 - mae: 0.5598 - val_loss: 0.2641 - val_R_squared: 0.4272 - val_mae: 0.3904\n",
      "Epoch 31/300\n",
      "10/10 [==============================] - 6s 552ms/step - loss: 0.5602 - R_squared: -0.3740 - mae: 0.5757 - val_loss: 0.2626 - val_R_squared: 0.4310 - val_mae: 0.3889\n",
      "Epoch 32/300\n",
      "10/10 [==============================] - 6s 543ms/step - loss: 0.5555 - R_squared: -0.3726 - mae: 0.5653 - val_loss: 0.2602 - val_R_squared: 0.4367 - val_mae: 0.3871\n",
      "Epoch 33/300\n",
      "10/10 [==============================] - 6s 553ms/step - loss: 0.5510 - R_squared: -0.3495 - mae: 0.5575 - val_loss: 0.2582 - val_R_squared: 0.4416 - val_mae: 0.3854\n",
      "Epoch 34/300\n",
      "10/10 [==============================] - 6s 575ms/step - loss: 0.5436 - R_squared: -0.3377 - mae: 0.5550 - val_loss: 0.2591 - val_R_squared: 0.4392 - val_mae: 0.3861\n",
      "Epoch 35/300\n",
      "10/10 [==============================] - 6s 583ms/step - loss: 0.5272 - R_squared: -0.2919 - mae: 0.5528 - val_loss: 0.2595 - val_R_squared: 0.4381 - val_mae: 0.3865\n",
      "Epoch 36/300\n",
      "10/10 [==============================] - 6s 548ms/step - loss: 0.5491 - R_squared: -0.3528 - mae: 0.5601 - val_loss: 0.2598 - val_R_squared: 0.4372 - val_mae: 0.3869\n",
      "Epoch 37/300\n",
      "10/10 [==============================] - 6s 553ms/step - loss: 0.5562 - R_squared: -0.3744 - mae: 0.5628 - val_loss: 0.2601 - val_R_squared: 0.4363 - val_mae: 0.3871\n",
      "Epoch 38/300\n",
      "10/10 [==============================] - 6s 560ms/step - loss: 0.5574 - R_squared: -0.3652 - mae: 0.5642 - val_loss: 0.2624 - val_R_squared: 0.4309 - val_mae: 0.3890\n",
      "Epoch 39/300\n",
      "10/10 [==============================] - 6s 548ms/step - loss: 0.5182 - R_squared: -0.2683 - mae: 0.5451 - val_loss: 0.2687 - val_R_squared: 0.4160 - val_mae: 0.3942\n",
      "Epoch 40/300\n",
      "10/10 [==============================] - 6s 567ms/step - loss: 0.5269 - R_squared: -0.2872 - mae: 0.5538 - val_loss: 0.2720 - val_R_squared: 0.4080 - val_mae: 0.3969\n",
      "Epoch 41/300\n",
      "10/10 [==============================] - 7s 614ms/step - loss: 0.5521 - R_squared: -0.3547 - mae: 0.5658 - val_loss: 0.2713 - val_R_squared: 0.4099 - val_mae: 0.3963\n",
      "Epoch 42/300\n",
      "10/10 [==============================] - 6s 575ms/step - loss: 0.5609 - R_squared: -0.3809 - mae: 0.5678 - val_loss: 0.2689 - val_R_squared: 0.4153 - val_mae: 0.3945\n",
      "Epoch 43/300\n",
      "10/10 [==============================] - 6s 554ms/step - loss: 0.5231 - R_squared: -0.2911 - mae: 0.5496 - val_loss: 0.2709 - val_R_squared: 0.4103 - val_mae: 0.3961\n",
      "Epoch 44/300\n",
      "10/10 [==============================] - 6s 554ms/step - loss: 0.5201 - R_squared: -0.2752 - mae: 0.5515 - val_loss: 0.2738 - val_R_squared: 0.4032 - val_mae: 0.3985\n",
      "Epoch 45/300\n",
      "10/10 [==============================] - 6s 554ms/step - loss: 0.5261 - R_squared: -0.2867 - mae: 0.5532 - val_loss: 0.2698 - val_R_squared: 0.4128 - val_mae: 0.3951\n",
      "Epoch 46/300\n",
      "10/10 [==============================] - 6s 553ms/step - loss: 0.5298 - R_squared: -0.2928 - mae: 0.5536 - val_loss: 0.2670 - val_R_squared: 0.4193 - val_mae: 0.3928\n",
      "Epoch 47/300\n",
      "10/10 [==============================] - 6s 564ms/step - loss: 0.5447 - R_squared: -0.3333 - mae: 0.5585 - val_loss: 0.2645 - val_R_squared: 0.4252 - val_mae: 0.3909\n",
      "Epoch 48/300\n",
      "10/10 [==============================] - 6s 563ms/step - loss: 0.5336 - R_squared: -0.3149 - mae: 0.5577 - val_loss: 0.2643 - val_R_squared: 0.4257 - val_mae: 0.3906\n",
      "Epoch 49/300\n",
      "10/10 [==============================] - 6s 551ms/step - loss: 0.5448 - R_squared: -0.3401 - mae: 0.5624 - val_loss: 0.2637 - val_R_squared: 0.4268 - val_mae: 0.3901\n",
      "Epoch 50/300\n",
      "10/10 [==============================] - 6s 587ms/step - loss: 0.5748 - R_squared: -0.4109 - mae: 0.5729 - val_loss: 0.2620 - val_R_squared: 0.4309 - val_mae: 0.3888\n",
      "Epoch 51/300\n",
      "10/10 [==============================] - 6s 579ms/step - loss: 0.5660 - R_squared: -0.3852 - mae: 0.5656 - val_loss: 0.2591 - val_R_squared: 0.4381 - val_mae: 0.3863\n",
      "Epoch 52/300\n",
      "10/10 [==============================] - 6s 589ms/step - loss: 0.5451 - R_squared: -0.3354 - mae: 0.5571 - val_loss: 0.2603 - val_R_squared: 0.4352 - val_mae: 0.3872\n",
      "Epoch 53/300\n",
      "10/10 [==============================] - 6s 570ms/step - loss: 0.5400 - R_squared: -0.3222 - mae: 0.5600 - val_loss: 0.2653 - val_R_squared: 0.4230 - val_mae: 0.3913\n",
      "Epoch 54/300\n",
      "10/10 [==============================] - 6s 557ms/step - loss: 0.5333 - R_squared: -0.3035 - mae: 0.5534 - val_loss: 0.2688 - val_R_squared: 0.4142 - val_mae: 0.3945\n",
      "Epoch 55/300\n",
      "10/10 [==============================] - 6s 536ms/step - loss: 0.5455 - R_squared: -0.3373 - mae: 0.5623 - val_loss: 0.2765 - val_R_squared: 0.3956 - val_mae: 0.4010\n",
      "Epoch 56/300\n",
      "10/10 [==============================] - 6s 555ms/step - loss: 0.5363 - R_squared: -0.3180 - mae: 0.5622 - val_loss: 0.2733 - val_R_squared: 0.4033 - val_mae: 0.3983\n",
      "Epoch 57/300\n",
      "10/10 [==============================] - 6s 560ms/step - loss: 0.5257 - R_squared: -0.2887 - mae: 0.5525 - val_loss: 0.2692 - val_R_squared: 0.4137 - val_mae: 0.3947\n",
      "Epoch 58/300\n",
      "10/10 [==============================] - 6s 581ms/step - loss: 0.5143 - R_squared: -0.2576 - mae: 0.5432 - val_loss: 0.2690 - val_R_squared: 0.4133 - val_mae: 0.3949\n",
      "Epoch 59/300\n",
      "10/10 [==============================] - 6s 568ms/step - loss: 0.5140 - R_squared: -0.2618 - mae: 0.5447 - val_loss: 0.2724 - val_R_squared: 0.4044 - val_mae: 0.3977\n",
      "Epoch 60/300\n",
      "10/10 [==============================] - 6s 573ms/step - loss: 0.5307 - R_squared: -0.3044 - mae: 0.5550 - val_loss: 0.2720 - val_R_squared: 0.4062 - val_mae: 0.3970\n",
      "Epoch 61/300\n",
      "10/10 [==============================] - 6s 560ms/step - loss: 0.5086 - R_squared: -0.2456 - mae: 0.5472 - val_loss: 0.2647 - val_R_squared: 0.4242 - val_mae: 0.3907\n",
      "Epoch 62/300\n",
      "10/10 [==============================] - 6s 570ms/step - loss: 0.5483 - R_squared: -0.3405 - mae: 0.5545 - val_loss: 0.2566 - val_R_squared: 0.4437 - val_mae: 0.3840\n",
      "Epoch 63/300\n",
      "10/10 [==============================] - 6s 566ms/step - loss: 0.5350 - R_squared: -0.3168 - mae: 0.5550 - val_loss: 0.2628 - val_R_squared: 0.4286 - val_mae: 0.3897\n",
      "Epoch 64/300\n",
      "10/10 [==============================] - 6s 551ms/step - loss: 0.5393 - R_squared: -0.3227 - mae: 0.5615 - val_loss: 0.2736 - val_R_squared: 0.4020 - val_mae: 0.3987\n",
      "Epoch 65/300\n",
      "10/10 [==============================] - 6s 558ms/step - loss: 0.5057 - R_squared: -0.2391 - mae: 0.5449 - val_loss: 0.2806 - val_R_squared: 0.3851 - val_mae: 0.4042\n",
      "Epoch 66/300\n",
      "10/10 [==============================] - 6s 579ms/step - loss: 0.5144 - R_squared: -0.2604 - mae: 0.5471 - val_loss: 0.2707 - val_R_squared: 0.4089 - val_mae: 0.3963\n",
      "Epoch 67/300\n",
      "10/10 [==============================] - 6s 597ms/step - loss: 0.5217 - R_squared: -0.2791 - mae: 0.5453 - val_loss: 0.2670 - val_R_squared: 0.4177 - val_mae: 0.3933\n",
      "Epoch 68/300\n",
      "10/10 [==============================] - 6s 549ms/step - loss: 0.4957 - R_squared: -0.2202 - mae: 0.5317 - val_loss: 0.2652 - val_R_squared: 0.4226 - val_mae: 0.3916\n",
      "Epoch 69/300\n",
      "10/10 [==============================] - 6s 568ms/step - loss: 0.5499 - R_squared: -0.3517 - mae: 0.5579 - val_loss: 0.2667 - val_R_squared: 0.4187 - val_mae: 0.3930\n",
      "Epoch 70/300\n",
      "10/10 [==============================] - 6s 571ms/step - loss: 0.5251 - R_squared: -0.2872 - mae: 0.5508 - val_loss: 0.2678 - val_R_squared: 0.4160 - val_mae: 0.3940\n",
      "Epoch 71/300\n",
      "10/10 [==============================] - 6s 528ms/step - loss: 0.5233 - R_squared: -0.2852 - mae: 0.5439 - val_loss: 0.2698 - val_R_squared: 0.4104 - val_mae: 0.3959\n",
      "Epoch 72/300\n",
      "10/10 [==============================] - 6s 579ms/step - loss: 0.4854 - R_squared: -0.1904 - mae: 0.5272 - val_loss: 0.2652 - val_R_squared: 0.4215 - val_mae: 0.3919\n",
      "Epoch 73/300\n",
      "10/10 [==============================] - 7s 662ms/step - loss: 0.5034 - R_squared: -0.2349 - mae: 0.5390 - val_loss: 0.2631 - val_R_squared: 0.4263 - val_mae: 0.3900\n",
      "Epoch 74/300\n",
      "10/10 [==============================] - 6s 605ms/step - loss: 0.4983 - R_squared: -0.2258 - mae: 0.5357 - val_loss: 0.2632 - val_R_squared: 0.4265 - val_mae: 0.3899\n",
      "Epoch 75/300\n",
      "10/10 [==============================] - 6s 574ms/step - loss: 0.5076 - R_squared: -0.2404 - mae: 0.5362 - val_loss: 0.2674 - val_R_squared: 0.4168 - val_mae: 0.3933\n",
      "Epoch 76/300\n",
      "10/10 [==============================] - 6s 579ms/step - loss: 0.5190 - R_squared: -0.2734 - mae: 0.5459 - val_loss: 0.2670 - val_R_squared: 0.4174 - val_mae: 0.3932\n",
      "Epoch 77/300\n",
      "10/10 [==============================] - 6s 548ms/step - loss: 0.5059 - R_squared: -0.2440 - mae: 0.5384 - val_loss: 0.2700 - val_R_squared: 0.4099 - val_mae: 0.3959\n",
      "Epoch 78/300\n",
      "10/10 [==============================] - 6s 576ms/step - loss: 0.5772 - R_squared: -0.4099 - mae: 0.5697 - val_loss: 0.2692 - val_R_squared: 0.4117 - val_mae: 0.3953\n",
      "Epoch 79/300\n",
      "10/10 [==============================] - 6s 555ms/step - loss: 0.5318 - R_squared: -0.2995 - mae: 0.5505 - val_loss: 0.2721 - val_R_squared: 0.4051 - val_mae: 0.3975\n",
      "Epoch 80/300\n",
      "10/10 [==============================] - 6s 571ms/step - loss: 0.5341 - R_squared: -0.3049 - mae: 0.5486 - val_loss: 0.2645 - val_R_squared: 0.4238 - val_mae: 0.3908\n",
      "Epoch 81/300\n",
      "10/10 [==============================] - 6s 539ms/step - loss: 0.5272 - R_squared: -0.2952 - mae: 0.5530 - val_loss: 0.2577 - val_R_squared: 0.4391 - val_mae: 0.3852\n",
      "Epoch 82/300\n",
      "10/10 [==============================] - 6s 550ms/step - loss: 0.5042 - R_squared: -0.2350 - mae: 0.5364 - val_loss: 0.2552 - val_R_squared: 0.4450 - val_mae: 0.3831\n",
      "Epoch 83/300\n",
      "10/10 [==============================] - 6s 565ms/step - loss: 0.4935 - R_squared: -0.2077 - mae: 0.5285 - val_loss: 0.2559 - val_R_squared: 0.4436 - val_mae: 0.3838\n",
      "Epoch 84/300\n",
      "10/10 [==============================] - 7s 630ms/step - loss: 0.5182 - R_squared: -0.2636 - mae: 0.5448 - val_loss: 0.2628 - val_R_squared: 0.4274 - val_mae: 0.3898\n",
      "Epoch 85/300\n",
      "10/10 [==============================] - 7s 599ms/step - loss: 0.5172 - R_squared: -0.2662 - mae: 0.5458 - val_loss: 0.2670 - val_R_squared: 0.4171 - val_mae: 0.3933\n",
      "Epoch 86/300\n",
      "10/10 [==============================] - 6s 554ms/step - loss: 0.5062 - R_squared: -0.2384 - mae: 0.5397 - val_loss: 0.2776 - val_R_squared: 0.3912 - val_mae: 0.4021\n",
      "Epoch 87/300\n",
      "10/10 [==============================] - 8s 758ms/step - loss: 0.5024 - R_squared: -0.2256 - mae: 0.5377 - val_loss: 0.2810 - val_R_squared: 0.3830 - val_mae: 0.4047\n",
      "Epoch 88/300\n",
      "10/10 [==============================] - 10s 969ms/step - loss: 0.5078 - R_squared: -0.2507 - mae: 0.5376 - val_loss: 0.2721 - val_R_squared: 0.4045 - val_mae: 0.3974\n",
      "Epoch 89/300\n",
      "10/10 [==============================] - 7s 681ms/step - loss: 0.4995 - R_squared: -0.2226 - mae: 0.5364 - val_loss: 0.2606 - val_R_squared: 0.4322 - val_mae: 0.3876\n",
      "Epoch 90/300\n",
      "10/10 [==============================] - 7s 652ms/step - loss: 0.5085 - R_squared: -0.2531 - mae: 0.5338 - val_loss: 0.2665 - val_R_squared: 0.4179 - val_mae: 0.3929\n",
      "Epoch 91/300\n",
      "10/10 [==============================] - 6s 591ms/step - loss: 0.5323 - R_squared: -0.3059 - mae: 0.5526 - val_loss: 0.2770 - val_R_squared: 0.3933 - val_mae: 0.4015\n",
      "Epoch 92/300\n",
      "10/10 [==============================] - 6s 592ms/step - loss: 0.5011 - R_squared: -0.2231 - mae: 0.5312 - val_loss: 0.2737 - val_R_squared: 0.4019 - val_mae: 0.3987\n",
      "Epoch 93/300\n",
      "10/10 [==============================] - 7s 641ms/step - loss: 0.5283 - R_squared: -0.3015 - mae: 0.5469 - val_loss: 0.2639 - val_R_squared: 0.4249 - val_mae: 0.3907\n",
      "Epoch 94/300\n",
      "10/10 [==============================] - 6s 555ms/step - loss: 0.4962 - R_squared: -0.2159 - mae: 0.5347 - val_loss: 0.2557 - val_R_squared: 0.4444 - val_mae: 0.3834\n",
      "Epoch 95/300\n",
      "10/10 [==============================] - 6s 562ms/step - loss: 0.5036 - R_squared: -0.2344 - mae: 0.5355 - val_loss: 0.2523 - val_R_squared: 0.4525 - val_mae: 0.3802\n",
      "Epoch 96/300\n",
      "10/10 [==============================] - 6s 570ms/step - loss: 0.5075 - R_squared: -0.2419 - mae: 0.5441 - val_loss: 0.2529 - val_R_squared: 0.4508 - val_mae: 0.3808\n",
      "Epoch 97/300\n",
      "10/10 [==============================] - 7s 647ms/step - loss: 0.5420 - R_squared: -0.3342 - mae: 0.5503 - val_loss: 0.2582 - val_R_squared: 0.4379 - val_mae: 0.3857\n",
      "Epoch 98/300\n",
      "10/10 [==============================] - 6s 581ms/step - loss: 0.4960 - R_squared: -0.2161 - mae: 0.5279 - val_loss: 0.2649 - val_R_squared: 0.4216 - val_mae: 0.3918\n",
      "Epoch 99/300\n",
      "10/10 [==============================] - 6s 565ms/step - loss: 0.5216 - R_squared: -0.2780 - mae: 0.5464 - val_loss: 0.2674 - val_R_squared: 0.4159 - val_mae: 0.3938\n",
      "Epoch 100/300\n",
      "10/10 [==============================] - 6s 570ms/step - loss: 0.5191 - R_squared: -0.2866 - mae: 0.5463 - val_loss: 0.2674 - val_R_squared: 0.4161 - val_mae: 0.3937\n",
      "Epoch 101/300\n",
      "10/10 [==============================] - 6s 563ms/step - loss: 0.5366 - R_squared: -0.3293 - mae: 0.5469 - val_loss: 0.2577 - val_R_squared: 0.4395 - val_mae: 0.3852\n",
      "Epoch 102/300\n",
      "10/10 [==============================] - 6s 588ms/step - loss: 0.4946 - R_squared: -0.2180 - mae: 0.5333 - val_loss: 0.2568 - val_R_squared: 0.4417 - val_mae: 0.3843\n",
      "Epoch 103/300\n",
      "10/10 [==============================] - 6s 565ms/step - loss: 0.5204 - R_squared: -0.2742 - mae: 0.5472 - val_loss: 0.2538 - val_R_squared: 0.4491 - val_mae: 0.3816\n",
      "Epoch 104/300\n",
      "10/10 [==============================] - 6s 555ms/step - loss: 0.5137 - R_squared: -0.2570 - mae: 0.5435 - val_loss: 0.2587 - val_R_squared: 0.4370 - val_mae: 0.3861\n",
      "Epoch 105/300\n",
      "10/10 [==============================] - 6s 571ms/step - loss: 0.4919 - R_squared: -0.2108 - mae: 0.5353 - val_loss: 0.2720 - val_R_squared: 0.4044 - val_mae: 0.3976\n",
      "Epoch 106/300\n",
      "10/10 [==============================] - 6s 562ms/step - loss: 0.5197 - R_squared: -0.2718 - mae: 0.5457 - val_loss: 0.2733 - val_R_squared: 0.4022 - val_mae: 0.3985\n",
      "Epoch 107/300\n",
      "10/10 [==============================] - 6s 574ms/step - loss: 0.4979 - R_squared: -0.2177 - mae: 0.5366 - val_loss: 0.2686 - val_R_squared: 0.4139 - val_mae: 0.3944\n",
      "Epoch 108/300\n",
      "10/10 [==============================] - 6s 555ms/step - loss: 0.4882 - R_squared: -0.1968 - mae: 0.5298 - val_loss: 0.2634 - val_R_squared: 0.4260 - val_mae: 0.3899\n",
      "Epoch 109/300\n",
      "10/10 [==============================] - 6s 580ms/step - loss: 0.4905 - R_squared: -0.2004 - mae: 0.5275 - val_loss: 0.2653 - val_R_squared: 0.4210 - val_mae: 0.3915\n",
      "Epoch 110/300\n",
      "10/10 [==============================] - 6s 546ms/step - loss: 0.5377 - R_squared: -0.3159 - mae: 0.5502 - val_loss: 0.2611 - val_R_squared: 0.4312 - val_mae: 0.3880\n",
      "Epoch 111/300\n",
      "10/10 [==============================] - 6s 586ms/step - loss: 0.5117 - R_squared: -0.2538 - mae: 0.5408 - val_loss: 0.2613 - val_R_squared: 0.4310 - val_mae: 0.3882\n",
      "Epoch 112/300\n",
      "10/10 [==============================] - 6s 559ms/step - loss: 0.4808 - R_squared: -0.1735 - mae: 0.5276 - val_loss: 0.2581 - val_R_squared: 0.4387 - val_mae: 0.3850\n",
      "Epoch 113/300\n",
      "10/10 [==============================] - 6s 562ms/step - loss: 0.5500 - R_squared: -0.3532 - mae: 0.5561 - val_loss: 0.2591 - val_R_squared: 0.4365 - val_mae: 0.3857\n",
      "Epoch 114/300\n",
      "10/10 [==============================] - 6s 585ms/step - loss: 0.4790 - R_squared: -0.1730 - mae: 0.5230 - val_loss: 0.2702 - val_R_squared: 0.4098 - val_mae: 0.3956\n",
      "Epoch 115/300\n",
      "10/10 [==============================] - 6s 588ms/step - loss: 0.4866 - R_squared: -0.1898 - mae: 0.5295 - val_loss: 0.2671 - val_R_squared: 0.4166 - val_mae: 0.3934\n",
      "Epoch 116/300\n",
      "10/10 [==============================] - 6s 571ms/step - loss: 0.5090 - R_squared: -0.2450 - mae: 0.5367 - val_loss: 0.2638 - val_R_squared: 0.4238 - val_mae: 0.3907\n",
      "Epoch 117/300\n",
      "10/10 [==============================] - 6s 576ms/step - loss: 0.5300 - R_squared: -0.3018 - mae: 0.5478 - val_loss: 0.2610 - val_R_squared: 0.4312 - val_mae: 0.3881\n",
      "Epoch 118/300\n",
      "10/10 [==============================] - 6s 552ms/step - loss: 0.5037 - R_squared: -0.2350 - mae: 0.5360 - val_loss: 0.2611 - val_R_squared: 0.4316 - val_mae: 0.3881\n",
      "Epoch 119/300\n",
      "10/10 [==============================] - 7s 596ms/step - loss: 0.5008 - R_squared: -0.2280 - mae: 0.5327 - val_loss: 0.2629 - val_R_squared: 0.4272 - val_mae: 0.3897\n",
      "Epoch 120/300\n",
      "10/10 [==============================] - 6s 578ms/step - loss: 0.5111 - R_squared: -0.2535 - mae: 0.5428 - val_loss: 0.2630 - val_R_squared: 0.4268 - val_mae: 0.3899\n",
      "Epoch 121/300\n",
      "10/10 [==============================] - 6s 560ms/step - loss: 0.5191 - R_squared: -0.2723 - mae: 0.5401 - val_loss: 0.2688 - val_R_squared: 0.4132 - val_mae: 0.3947\n",
      "Epoch 122/300\n",
      "10/10 [==============================] - 6s 572ms/step - loss: 0.4964 - R_squared: -0.2243 - mae: 0.5292 - val_loss: 0.2772 - val_R_squared: 0.3930 - val_mae: 0.4017\n",
      "Epoch 123/300\n",
      "10/10 [==============================] - 6s 565ms/step - loss: 0.4950 - R_squared: -0.2130 - mae: 0.5311 - val_loss: 0.2679 - val_R_squared: 0.4147 - val_mae: 0.3941\n",
      "Epoch 124/300\n",
      "10/10 [==============================] - 6s 562ms/step - loss: 0.5134 - R_squared: -0.2565 - mae: 0.5390 - val_loss: 0.2587 - val_R_squared: 0.4375 - val_mae: 0.3856\n",
      "Epoch 125/300\n",
      "10/10 [==============================] - 6s 562ms/step - loss: 0.4766 - R_squared: -0.1640 - mae: 0.5199 - val_loss: 0.2619 - val_R_squared: 0.4300 - val_mae: 0.3883\n",
      "Epoch 126/300\n",
      "10/10 [==============================] - 6s 567ms/step - loss: 0.5020 - R_squared: -0.2276 - mae: 0.5388 - val_loss: 0.2654 - val_R_squared: 0.4210 - val_mae: 0.3917\n",
      "Epoch 127/300\n",
      "10/10 [==============================] - 6s 577ms/step - loss: 0.4959 - R_squared: -0.2177 - mae: 0.5294 - val_loss: 0.2704 - val_R_squared: 0.4091 - val_mae: 0.3959\n",
      "Epoch 128/300\n",
      "10/10 [==============================] - 6s 572ms/step - loss: 0.4965 - R_squared: -0.2155 - mae: 0.5295 - val_loss: 0.2597 - val_R_squared: 0.4349 - val_mae: 0.3864\n",
      "Epoch 129/300\n",
      "10/10 [==============================] - 6s 598ms/step - loss: 0.5138 - R_squared: -0.2555 - mae: 0.5399 - val_loss: 0.2517 - val_R_squared: 0.4536 - val_mae: 0.3792\n",
      "Epoch 130/300\n",
      "10/10 [==============================] - 6s 581ms/step - loss: 0.5256 - R_squared: -0.2882 - mae: 0.5493 - val_loss: 0.2597 - val_R_squared: 0.4339 - val_mae: 0.3869\n",
      "Epoch 131/300\n",
      "10/10 [==============================] - 6s 572ms/step - loss: 0.4847 - R_squared: -0.1869 - mae: 0.5242 - val_loss: 0.2629 - val_R_squared: 0.4262 - val_mae: 0.3896\n",
      "Epoch 132/300\n",
      "10/10 [==============================] - 6s 575ms/step - loss: 0.4643 - R_squared: -0.1400 - mae: 0.5204 - val_loss: 0.2581 - val_R_squared: 0.4383 - val_mae: 0.3852\n",
      "Epoch 133/300\n",
      "10/10 [==============================] - 6s 577ms/step - loss: 0.5075 - R_squared: -0.2432 - mae: 0.5403 - val_loss: 0.2580 - val_R_squared: 0.4388 - val_mae: 0.3850\n",
      "Epoch 134/300\n",
      "10/10 [==============================] - 6s 577ms/step - loss: 0.4976 - R_squared: -0.2201 - mae: 0.5392 - val_loss: 0.2672 - val_R_squared: 0.4174 - val_mae: 0.3930\n",
      "Epoch 135/300\n",
      "10/10 [==============================] - 6s 570ms/step - loss: 0.5072 - R_squared: -0.2438 - mae: 0.5344 - val_loss: 0.2701 - val_R_squared: 0.4099 - val_mae: 0.3959\n",
      "Epoch 136/300\n",
      "10/10 [==============================] - 6s 571ms/step - loss: 0.4752 - R_squared: -0.1629 - mae: 0.5195 - val_loss: 0.2462 - val_R_squared: 0.4662 - val_mae: 0.3748\n",
      "Epoch 137/300\n",
      "10/10 [==============================] - 6s 569ms/step - loss: 0.4675 - R_squared: -0.1496 - mae: 0.5152 - val_loss: 0.2409 - val_R_squared: 0.4795 - val_mae: 0.3699\n",
      "Epoch 138/300\n",
      "10/10 [==============================] - 7s 642ms/step - loss: 0.5091 - R_squared: -0.2526 - mae: 0.5367 - val_loss: 0.2562 - val_R_squared: 0.4429 - val_mae: 0.3832\n",
      "Epoch 139/300\n",
      "10/10 [==============================] - 6s 557ms/step - loss: 0.4860 - R_squared: -0.1909 - mae: 0.5272 - val_loss: 0.2637 - val_R_squared: 0.4245 - val_mae: 0.3901\n",
      "Epoch 140/300\n",
      "10/10 [==============================] - 6s 587ms/step - loss: 0.4873 - R_squared: -0.1978 - mae: 0.5282 - val_loss: 0.2591 - val_R_squared: 0.4357 - val_mae: 0.3860\n",
      "Epoch 141/300\n",
      "10/10 [==============================] - 6s 569ms/step - loss: 0.5202 - R_squared: -0.2773 - mae: 0.5427 - val_loss: 0.2538 - val_R_squared: 0.4488 - val_mae: 0.3810\n",
      "Epoch 142/300\n",
      "10/10 [==============================] - 6s 582ms/step - loss: 0.4959 - R_squared: -0.2177 - mae: 0.5266 - val_loss: 0.2508 - val_R_squared: 0.4555 - val_mae: 0.3784\n",
      "Epoch 143/300\n",
      "10/10 [==============================] - 6s 574ms/step - loss: 0.4892 - R_squared: -0.1996 - mae: 0.5241 - val_loss: 0.2536 - val_R_squared: 0.4486 - val_mae: 0.3812\n",
      "Epoch 144/300\n",
      "10/10 [==============================] - 6s 593ms/step - loss: 0.4846 - R_squared: -0.1872 - mae: 0.5251 - val_loss: 0.2553 - val_R_squared: 0.4446 - val_mae: 0.3828\n",
      "Epoch 145/300\n",
      "10/10 [==============================] - 6s 584ms/step - loss: 0.4867 - R_squared: -0.1926 - mae: 0.5243 - val_loss: 0.2672 - val_R_squared: 0.4161 - val_mae: 0.3932\n",
      "Epoch 146/300\n",
      "10/10 [==============================] - 6s 588ms/step - loss: 0.5043 - R_squared: -0.2345 - mae: 0.5363 - val_loss: 0.2635 - val_R_squared: 0.4248 - val_mae: 0.3902\n",
      "Epoch 147/300\n",
      "10/10 [==============================] - 6s 590ms/step - loss: 0.5021 - R_squared: -0.2315 - mae: 0.5351 - val_loss: 0.2533 - val_R_squared: 0.4490 - val_mae: 0.3810\n",
      "Epoch 148/300\n",
      "10/10 [==============================] - 6s 555ms/step - loss: 0.4979 - R_squared: -0.2251 - mae: 0.5298 - val_loss: 0.2520 - val_R_squared: 0.4515 - val_mae: 0.3794\n",
      "Epoch 149/300\n",
      "10/10 [==============================] - 7s 669ms/step - loss: 0.5050 - R_squared: -0.2384 - mae: 0.5357 - val_loss: 0.2567 - val_R_squared: 0.4403 - val_mae: 0.3838\n",
      "Epoch 150/300\n",
      "10/10 [==============================] - 6s 564ms/step - loss: 0.5106 - R_squared: -0.2552 - mae: 0.5357 - val_loss: 0.2547 - val_R_squared: 0.4450 - val_mae: 0.3821\n",
      "Epoch 151/300\n",
      "10/10 [==============================] - 6s 558ms/step - loss: 0.4924 - R_squared: -0.2008 - mae: 0.5238 - val_loss: 0.2574 - val_R_squared: 0.4384 - val_mae: 0.3846\n",
      "Epoch 152/300\n",
      "10/10 [==============================] - 6s 591ms/step - loss: 0.5025 - R_squared: -0.2351 - mae: 0.5381 - val_loss: 0.2591 - val_R_squared: 0.4345 - val_mae: 0.3861\n",
      "Epoch 153/300\n",
      "10/10 [==============================] - 6s 572ms/step - loss: 0.4651 - R_squared: -0.1481 - mae: 0.5163 - val_loss: 0.2631 - val_R_squared: 0.4253 - val_mae: 0.3897\n",
      "Epoch 154/300\n",
      "10/10 [==============================] - 6s 572ms/step - loss: 0.4897 - R_squared: -0.1992 - mae: 0.5289 - val_loss: 0.2636 - val_R_squared: 0.4238 - val_mae: 0.3898\n",
      "Epoch 155/300\n",
      "10/10 [==============================] - 6s 578ms/step - loss: 0.4972 - R_squared: -0.2209 - mae: 0.5303 - val_loss: 0.2678 - val_R_squared: 0.4136 - val_mae: 0.3938\n",
      "Epoch 156/300\n",
      "10/10 [==============================] - 6s 587ms/step - loss: 0.4963 - R_squared: -0.2191 - mae: 0.5343 - val_loss: 0.2591 - val_R_squared: 0.4347 - val_mae: 0.3861\n",
      "Epoch 157/300\n",
      "10/10 [==============================] - 6s 580ms/step - loss: 0.4855 - R_squared: -0.1902 - mae: 0.5235 - val_loss: 0.2506 - val_R_squared: 0.4551 - val_mae: 0.3783\n",
      "Epoch 158/300\n",
      "10/10 [==============================] - 6s 553ms/step - loss: 0.4893 - R_squared: -0.1967 - mae: 0.5243 - val_loss: 0.2474 - val_R_squared: 0.4633 - val_mae: 0.3749\n",
      "Epoch 159/300\n",
      "10/10 [==============================] - 6s 572ms/step - loss: 0.5423 - R_squared: -0.3327 - mae: 0.5494 - val_loss: 0.2501 - val_R_squared: 0.4568 - val_mae: 0.3774\n",
      "Epoch 160/300\n",
      "10/10 [==============================] - 6s 567ms/step - loss: 0.4849 - R_squared: -0.1894 - mae: 0.5292 - val_loss: 0.2510 - val_R_squared: 0.4545 - val_mae: 0.3781\n",
      "Epoch 161/300\n",
      "10/10 [==============================] - 6s 584ms/step - loss: 0.5036 - R_squared: -0.2435 - mae: 0.5326 - val_loss: 0.2607 - val_R_squared: 0.4308 - val_mae: 0.3872\n",
      "Epoch 162/300\n",
      "10/10 [==============================] - 6s 584ms/step - loss: 0.4839 - R_squared: -0.1878 - mae: 0.5257 - val_loss: 0.2745 - val_R_squared: 0.3986 - val_mae: 0.3992\n",
      "Epoch 163/300\n",
      "10/10 [==============================] - 6s 580ms/step - loss: 0.4856 - R_squared: -0.1892 - mae: 0.5222 - val_loss: 0.2747 - val_R_squared: 0.3977 - val_mae: 0.3995\n",
      "Epoch 164/300\n",
      "10/10 [==============================] - 6s 574ms/step - loss: 0.4839 - R_squared: -0.1903 - mae: 0.5278 - val_loss: 0.2600 - val_R_squared: 0.4324 - val_mae: 0.3867\n",
      "Epoch 165/300\n",
      "10/10 [==============================] - 6s 590ms/step - loss: 0.4893 - R_squared: -0.2009 - mae: 0.5310 - val_loss: 0.2547 - val_R_squared: 0.4451 - val_mae: 0.3821\n",
      "Epoch 166/300\n",
      "10/10 [==============================] - 7s 611ms/step - loss: 0.5024 - R_squared: -0.2345 - mae: 0.5299 - val_loss: 0.2582 - val_R_squared: 0.4367 - val_mae: 0.3854\n",
      "Epoch 167/300\n",
      "10/10 [==============================] - 6s 591ms/step - loss: 0.4610 - R_squared: -0.1351 - mae: 0.5104 - val_loss: 0.2603 - val_R_squared: 0.4320 - val_mae: 0.3872\n",
      "Epoch 168/300\n",
      "10/10 [==============================] - 6s 565ms/step - loss: 0.4618 - R_squared: -0.1279 - mae: 0.5118 - val_loss: 0.2531 - val_R_squared: 0.4492 - val_mae: 0.3808\n",
      "Epoch 169/300\n",
      "10/10 [==============================] - 6s 570ms/step - loss: 0.4844 - R_squared: -0.1891 - mae: 0.5242 - val_loss: 0.2444 - val_R_squared: 0.4700 - val_mae: 0.3728\n",
      "Epoch 170/300\n",
      "10/10 [==============================] - 6s 590ms/step - loss: 0.5054 - R_squared: -0.2419 - mae: 0.5297 - val_loss: 0.2447 - val_R_squared: 0.4688 - val_mae: 0.3729\n",
      "Epoch 171/300\n",
      "10/10 [==============================] - 6s 580ms/step - loss: 0.4892 - R_squared: -0.1953 - mae: 0.5253 - val_loss: 0.2486 - val_R_squared: 0.4603 - val_mae: 0.3763\n",
      "Epoch 172/300\n",
      "10/10 [==============================] - 6s 582ms/step - loss: 0.4981 - R_squared: -0.2184 - mae: 0.5273 - val_loss: 0.2530 - val_R_squared: 0.4497 - val_mae: 0.3803\n",
      "Epoch 173/300\n",
      "10/10 [==============================] - 6s 553ms/step - loss: 0.5058 - R_squared: -0.2305 - mae: 0.5364 - val_loss: 0.2569 - val_R_squared: 0.4404 - val_mae: 0.3840\n",
      "Epoch 174/300\n",
      "10/10 [==============================] - 6s 573ms/step - loss: 0.4735 - R_squared: -0.1634 - mae: 0.5168 - val_loss: 0.2617 - val_R_squared: 0.4288 - val_mae: 0.3885\n",
      "Epoch 175/300\n",
      "10/10 [==============================] - 6s 571ms/step - loss: 0.4894 - R_squared: -0.1987 - mae: 0.5261 - val_loss: 0.2580 - val_R_squared: 0.4370 - val_mae: 0.3851\n",
      "Epoch 176/300\n",
      "10/10 [==============================] - 6s 570ms/step - loss: 0.4734 - R_squared: -0.1597 - mae: 0.5178 - val_loss: 0.2558 - val_R_squared: 0.4420 - val_mae: 0.3830\n",
      "Epoch 177/300\n",
      "10/10 [==============================] - 6s 571ms/step - loss: 0.4686 - R_squared: -0.1553 - mae: 0.5186 - val_loss: 0.2616 - val_R_squared: 0.4286 - val_mae: 0.3883\n",
      "Epoch 178/300\n",
      "10/10 [==============================] - 6s 562ms/step - loss: 0.4893 - R_squared: -0.2021 - mae: 0.5265 - val_loss: 0.2580 - val_R_squared: 0.4376 - val_mae: 0.3850\n",
      "Epoch 179/300\n",
      "10/10 [==============================] - 6s 577ms/step - loss: 0.4793 - R_squared: -0.1833 - mae: 0.5196 - val_loss: 0.2514 - val_R_squared: 0.4534 - val_mae: 0.3789\n",
      "Epoch 180/300\n",
      "10/10 [==============================] - 6s 565ms/step - loss: 0.4964 - R_squared: -0.2195 - mae: 0.5335 - val_loss: 0.2576 - val_R_squared: 0.4388 - val_mae: 0.3845\n",
      "Epoch 181/300\n",
      "10/10 [==============================] - 6s 576ms/step - loss: 0.4993 - R_squared: -0.2234 - mae: 0.5276 - val_loss: 0.2563 - val_R_squared: 0.4414 - val_mae: 0.3830\n",
      "Epoch 182/300\n",
      "10/10 [==============================] - 6s 566ms/step - loss: 0.4804 - R_squared: -0.1797 - mae: 0.5214 - val_loss: 0.2543 - val_R_squared: 0.4457 - val_mae: 0.3810\n",
      "Epoch 183/300\n",
      "10/10 [==============================] - 6s 567ms/step - loss: 0.4773 - R_squared: -0.1726 - mae: 0.5171 - val_loss: 0.2483 - val_R_squared: 0.4591 - val_mae: 0.3756\n",
      "Epoch 184/300\n",
      "10/10 [==============================] - 6s 576ms/step - loss: 0.4886 - R_squared: -0.2007 - mae: 0.5184 - val_loss: 0.2535 - val_R_squared: 0.4476 - val_mae: 0.3802\n",
      "Epoch 185/300\n",
      "10/10 [==============================] - 7s 591ms/step - loss: 0.4757 - R_squared: -0.1616 - mae: 0.5152 - val_loss: 0.2550 - val_R_squared: 0.4443 - val_mae: 0.3817\n",
      "Epoch 186/300\n",
      "10/10 [==============================] - 6s 559ms/step - loss: 0.4762 - R_squared: -0.1710 - mae: 0.5214 - val_loss: 0.2590 - val_R_squared: 0.4346 - val_mae: 0.3854\n",
      "Epoch 187/300\n",
      "10/10 [==============================] - 6s 566ms/step - loss: 0.4727 - R_squared: -0.1566 - mae: 0.5172 - val_loss: 0.2615 - val_R_squared: 0.4286 - val_mae: 0.3876\n",
      "Epoch 188/300\n",
      "10/10 [==============================] - 6s 571ms/step - loss: 0.4838 - R_squared: -0.1848 - mae: 0.5209 - val_loss: 0.2567 - val_R_squared: 0.4403 - val_mae: 0.3831\n",
      "Epoch 189/300\n",
      "10/10 [==============================] - 6s 581ms/step - loss: 0.4962 - R_squared: -0.2224 - mae: 0.5262 - val_loss: 0.2560 - val_R_squared: 0.4416 - val_mae: 0.3826\n",
      "Epoch 190/300\n",
      "10/10 [==============================] - 6s 572ms/step - loss: 0.4761 - R_squared: -0.1625 - mae: 0.5156 - val_loss: 0.2570 - val_R_squared: 0.4388 - val_mae: 0.3834\n",
      "Epoch 191/300\n",
      "10/10 [==============================] - 6s 597ms/step - loss: 0.4584 - R_squared: -0.1259 - mae: 0.5066 - val_loss: 0.2540 - val_R_squared: 0.4463 - val_mae: 0.3807\n",
      "Epoch 192/300\n",
      "10/10 [==============================] - 6s 579ms/step - loss: 0.4882 - R_squared: -0.1986 - mae: 0.5208 - val_loss: 0.2493 - val_R_squared: 0.4576 - val_mae: 0.3765\n",
      "Epoch 193/300\n",
      "10/10 [==============================] - 6s 579ms/step - loss: 0.4716 - R_squared: -0.1532 - mae: 0.5158 - val_loss: 0.2478 - val_R_squared: 0.4616 - val_mae: 0.3752\n",
      "Epoch 194/300\n",
      "10/10 [==============================] - 6s 549ms/step - loss: 0.4778 - R_squared: -0.1783 - mae: 0.5180 - val_loss: 0.2458 - val_R_squared: 0.4669 - val_mae: 0.3730\n",
      "Epoch 195/300\n",
      "10/10 [==============================] - 6s 563ms/step - loss: 0.4718 - R_squared: -0.1599 - mae: 0.5165 - val_loss: 0.2510 - val_R_squared: 0.4545 - val_mae: 0.3779\n",
      "Epoch 196/300\n",
      "10/10 [==============================] - 6s 562ms/step - loss: 0.4674 - R_squared: -0.1527 - mae: 0.5120 - val_loss: 0.2585 - val_R_squared: 0.4367 - val_mae: 0.3847\n",
      "Epoch 197/300\n",
      "10/10 [==============================] - 9s 869ms/step - loss: 0.4863 - R_squared: -0.1982 - mae: 0.5243 - val_loss: 0.2612 - val_R_squared: 0.4303 - val_mae: 0.3874\n",
      "Epoch 198/300\n",
      "10/10 [==============================] - 7s 601ms/step - loss: 0.4759 - R_squared: -0.1765 - mae: 0.5143 - val_loss: 0.2604 - val_R_squared: 0.4317 - val_mae: 0.3865\n",
      "Epoch 199/300\n",
      "10/10 [==============================] - 6s 554ms/step - loss: 0.4730 - R_squared: -0.1623 - mae: 0.5182 - val_loss: 0.2564 - val_R_squared: 0.4409 - val_mae: 0.3827\n",
      "Epoch 200/300\n",
      "10/10 [==============================] - 6s 617ms/step - loss: 0.4613 - R_squared: -0.1408 - mae: 0.5097 - val_loss: 0.2532 - val_R_squared: 0.4483 - val_mae: 0.3800\n",
      "Epoch 201/300\n",
      "10/10 [==============================] - 6s 561ms/step - loss: 0.4988 - R_squared: -0.2179 - mae: 0.5249 - val_loss: 0.2510 - val_R_squared: 0.4540 - val_mae: 0.3781\n",
      "Epoch 202/300\n",
      "10/10 [==============================] - 6s 581ms/step - loss: 0.4429 - R_squared: -0.0824 - mae: 0.5048 - val_loss: 0.2456 - val_R_squared: 0.4668 - val_mae: 0.3731\n",
      "Epoch 203/300\n",
      "10/10 [==============================] - 6s 594ms/step - loss: 0.4715 - R_squared: -0.1553 - mae: 0.5154 - val_loss: 0.2504 - val_R_squared: 0.4555 - val_mae: 0.3778\n",
      "Epoch 204/300\n",
      "10/10 [==============================] - 6s 586ms/step - loss: 0.4926 - R_squared: -0.2037 - mae: 0.5257 - val_loss: 0.2589 - val_R_squared: 0.4358 - val_mae: 0.3855\n",
      "Epoch 205/300\n",
      "10/10 [==============================] - 6s 571ms/step - loss: 0.4836 - R_squared: -0.1839 - mae: 0.5208 - val_loss: 0.2556 - val_R_squared: 0.4440 - val_mae: 0.3825\n",
      "Epoch 206/300\n",
      "10/10 [==============================] - 6s 570ms/step - loss: 0.4659 - R_squared: -0.1435 - mae: 0.5089 - val_loss: 0.2534 - val_R_squared: 0.4498 - val_mae: 0.3806\n",
      "Epoch 207/300\n",
      "10/10 [==============================] - 6s 572ms/step - loss: 0.5060 - R_squared: -0.2385 - mae: 0.5386 - val_loss: 0.2526 - val_R_squared: 0.4522 - val_mae: 0.3800\n",
      "Epoch 208/300\n",
      "10/10 [==============================] - 6s 566ms/step - loss: 0.4492 - R_squared: -0.1014 - mae: 0.5072 - val_loss: 0.2565 - val_R_squared: 0.4436 - val_mae: 0.3835\n",
      "Epoch 209/300\n",
      "10/10 [==============================] - 7s 628ms/step - loss: 0.4657 - R_squared: -0.1464 - mae: 0.5170 - val_loss: 0.2636 - val_R_squared: 0.4271 - val_mae: 0.3900\n",
      "Epoch 210/300\n",
      "10/10 [==============================] - 7s 656ms/step - loss: 0.4739 - R_squared: -0.1603 - mae: 0.5152 - val_loss: 0.2533 - val_R_squared: 0.4513 - val_mae: 0.3809\n",
      "Epoch 211/300\n",
      "10/10 [==============================] - 7s 683ms/step - loss: 0.4782 - R_squared: -0.1713 - mae: 0.5227 - val_loss: 0.2541 - val_R_squared: 0.4495 - val_mae: 0.3817\n",
      "Epoch 212/300\n",
      "10/10 [==============================] - 6s 593ms/step - loss: 0.4895 - R_squared: -0.2096 - mae: 0.5210 - val_loss: 0.2512 - val_R_squared: 0.4562 - val_mae: 0.3791\n",
      "Epoch 213/300\n",
      "10/10 [==============================] - 6s 572ms/step - loss: 0.4773 - R_squared: -0.1709 - mae: 0.5196 - val_loss: 0.2466 - val_R_squared: 0.4674 - val_mae: 0.3747\n",
      "Epoch 214/300\n",
      "10/10 [==============================] - 6s 550ms/step - loss: 0.4719 - R_squared: -0.1555 - mae: 0.5163 - val_loss: 0.2494 - val_R_squared: 0.4606 - val_mae: 0.3774\n",
      "Epoch 215/300\n",
      "10/10 [==============================] - 7s 623ms/step - loss: 0.4513 - R_squared: -0.1090 - mae: 0.5050 - val_loss: 0.2495 - val_R_squared: 0.4602 - val_mae: 0.3775\n",
      "Epoch 216/300\n",
      "10/10 [==============================] - 7s 637ms/step - loss: 0.4737 - R_squared: -0.1600 - mae: 0.5108 - val_loss: 0.2468 - val_R_squared: 0.4667 - val_mae: 0.3749\n",
      "Epoch 217/300\n",
      "10/10 [==============================] - 6s 573ms/step - loss: 0.4632 - R_squared: -0.1360 - mae: 0.5048 - val_loss: 0.2467 - val_R_squared: 0.4671 - val_mae: 0.3747\n",
      "Epoch 218/300\n",
      "10/10 [==============================] - 6s 556ms/step - loss: 0.4684 - R_squared: -0.1490 - mae: 0.5149 - val_loss: 0.2504 - val_R_squared: 0.4585 - val_mae: 0.3780\n",
      "Epoch 219/300\n",
      "10/10 [==============================] - 6s 554ms/step - loss: 0.4640 - R_squared: -0.1391 - mae: 0.5143 - val_loss: 0.2476 - val_R_squared: 0.4651 - val_mae: 0.3756\n",
      "Epoch 220/300\n",
      "10/10 [==============================] - 6s 548ms/step - loss: 0.4896 - R_squared: -0.1992 - mae: 0.5215 - val_loss: 0.2493 - val_R_squared: 0.4611 - val_mae: 0.3771\n",
      "Epoch 221/300\n",
      "10/10 [==============================] - 6s 559ms/step - loss: 0.4431 - R_squared: -0.0806 - mae: 0.4986 - val_loss: 0.2471 - val_R_squared: 0.4661 - val_mae: 0.3751\n",
      "Epoch 222/300\n",
      "10/10 [==============================] - 6s 579ms/step - loss: 0.4773 - R_squared: -0.1714 - mae: 0.5201 - val_loss: 0.2542 - val_R_squared: 0.4495 - val_mae: 0.3815\n",
      "Epoch 223/300\n",
      "10/10 [==============================] - 6s 577ms/step - loss: 0.4666 - R_squared: -0.1482 - mae: 0.5122 - val_loss: 0.2659 - val_R_squared: 0.4218 - val_mae: 0.3919\n",
      "Epoch 224/300\n",
      "10/10 [==============================] - 7s 642ms/step - loss: 0.4684 - R_squared: -0.1524 - mae: 0.5152 - val_loss: 0.2614 - val_R_squared: 0.4326 - val_mae: 0.3880\n",
      "Epoch 225/300\n",
      "10/10 [==============================] - 6s 568ms/step - loss: 0.4729 - R_squared: -0.1632 - mae: 0.5174 - val_loss: 0.2621 - val_R_squared: 0.4311 - val_mae: 0.3887\n",
      "Epoch 226/300\n",
      "10/10 [==============================] - 6s 547ms/step - loss: 0.4627 - R_squared: -0.1353 - mae: 0.5120 - val_loss: 0.2617 - val_R_squared: 0.4321 - val_mae: 0.3884\n",
      "Epoch 227/300\n",
      "10/10 [==============================] - 7s 648ms/step - loss: 0.4858 - R_squared: -0.1937 - mae: 0.5224 - val_loss: 0.2562 - val_R_squared: 0.4451 - val_mae: 0.3833\n",
      "Epoch 228/300\n",
      "10/10 [==============================] - 6s 618ms/step - loss: 0.4424 - R_squared: -0.0875 - mae: 0.4992 - val_loss: 0.2532 - val_R_squared: 0.4523 - val_mae: 0.3805\n",
      "Epoch 229/300\n",
      "10/10 [==============================] - 6s 561ms/step - loss: 0.4589 - R_squared: -0.1237 - mae: 0.5081 - val_loss: 0.2537 - val_R_squared: 0.4511 - val_mae: 0.3810\n",
      "Epoch 230/300\n",
      "10/10 [==============================] - 6s 576ms/step - loss: 0.4713 - R_squared: -0.1558 - mae: 0.5208 - val_loss: 0.2557 - val_R_squared: 0.4464 - val_mae: 0.3829\n",
      "Epoch 231/300\n",
      "10/10 [==============================] - 6s 573ms/step - loss: 0.4577 - R_squared: -0.1188 - mae: 0.5052 - val_loss: 0.2535 - val_R_squared: 0.4515 - val_mae: 0.3809\n",
      "Epoch 232/300\n",
      "10/10 [==============================] - 6s 574ms/step - loss: 0.4794 - R_squared: -0.1758 - mae: 0.5194 - val_loss: 0.2529 - val_R_squared: 0.4530 - val_mae: 0.3804\n",
      "Epoch 233/300\n",
      "10/10 [==============================] - 6s 573ms/step - loss: 0.4711 - R_squared: -0.1565 - mae: 0.5141 - val_loss: 0.2497 - val_R_squared: 0.4608 - val_mae: 0.3775\n",
      "Epoch 234/300\n",
      "10/10 [==============================] - 6s 577ms/step - loss: 0.5079 - R_squared: -0.2374 - mae: 0.5235 - val_loss: 0.2513 - val_R_squared: 0.4568 - val_mae: 0.3789\n",
      "Epoch 235/300\n",
      "10/10 [==============================] - 6s 564ms/step - loss: 0.4609 - R_squared: -0.1258 - mae: 0.5055 - val_loss: 0.2541 - val_R_squared: 0.4503 - val_mae: 0.3814\n",
      "Epoch 236/300\n",
      "10/10 [==============================] - 6s 584ms/step - loss: 0.4991 - R_squared: -0.2233 - mae: 0.5267 - val_loss: 0.2581 - val_R_squared: 0.4411 - val_mae: 0.3850\n",
      "Epoch 237/300\n",
      "10/10 [==============================] - 6s 573ms/step - loss: 0.4728 - R_squared: -0.1570 - mae: 0.5132 - val_loss: 0.2565 - val_R_squared: 0.4451 - val_mae: 0.3836\n",
      "Epoch 238/300\n",
      "10/10 [==============================] - 6s 549ms/step - loss: 0.4429 - R_squared: -0.0901 - mae: 0.5049 - val_loss: 0.2592 - val_R_squared: 0.4387 - val_mae: 0.3859\n",
      "Epoch 239/300\n",
      "10/10 [==============================] - 6s 585ms/step - loss: 0.4890 - R_squared: -0.2070 - mae: 0.5270 - val_loss: 0.2626 - val_R_squared: 0.4304 - val_mae: 0.3890\n",
      "Epoch 240/300\n",
      "10/10 [==============================] - 6s 555ms/step - loss: 0.4682 - R_squared: -0.1561 - mae: 0.5147 - val_loss: 0.2663 - val_R_squared: 0.4208 - val_mae: 0.3924\n",
      "Epoch 241/300\n",
      "10/10 [==============================] - 6s 571ms/step - loss: 0.4747 - R_squared: -0.1671 - mae: 0.5173 - val_loss: 0.2574 - val_R_squared: 0.4416 - val_mae: 0.3844\n",
      "Epoch 242/300\n",
      "10/10 [==============================] - 6s 567ms/step - loss: 0.4856 - R_squared: -0.1910 - mae: 0.5205 - val_loss: 0.2536 - val_R_squared: 0.4508 - val_mae: 0.3809\n",
      "Epoch 243/300\n",
      "10/10 [==============================] - 6s 585ms/step - loss: 0.4700 - R_squared: -0.1535 - mae: 0.5152 - val_loss: 0.2540 - val_R_squared: 0.4495 - val_mae: 0.3815\n",
      "Epoch 244/300\n",
      "10/10 [==============================] - 6s 571ms/step - loss: 0.4497 - R_squared: -0.1098 - mae: 0.5026 - val_loss: 0.2491 - val_R_squared: 0.4613 - val_mae: 0.3769\n",
      "Epoch 245/300\n",
      "10/10 [==============================] - 6s 588ms/step - loss: 0.4718 - R_squared: -0.1631 - mae: 0.5139 - val_loss: 0.2477 - val_R_squared: 0.4647 - val_mae: 0.3756\n",
      "Epoch 246/300\n",
      "10/10 [==============================] - 6s 581ms/step - loss: 0.4541 - R_squared: -0.1185 - mae: 0.5043 - val_loss: 0.2510 - val_R_squared: 0.4559 - val_mae: 0.3789\n",
      "Epoch 247/300\n",
      "10/10 [==============================] - 6s 559ms/step - loss: 0.4860 - R_squared: -0.1945 - mae: 0.5231 - val_loss: 0.2502 - val_R_squared: 0.4581 - val_mae: 0.3782\n",
      "Epoch 248/300\n",
      "10/10 [==============================] - 6s 616ms/step - loss: 0.4753 - R_squared: -0.1597 - mae: 0.5148 - val_loss: 0.2526 - val_R_squared: 0.4526 - val_mae: 0.3802\n",
      "Epoch 249/300\n",
      "10/10 [==============================] - 6s 571ms/step - loss: 0.4822 - R_squared: -0.1766 - mae: 0.5194 - val_loss: 0.2497 - val_R_squared: 0.4597 - val_mae: 0.3773\n",
      "Epoch 250/300\n",
      "10/10 [==============================] - 6s 561ms/step - loss: 0.5006 - R_squared: -0.2251 - mae: 0.5269 - val_loss: 0.2487 - val_R_squared: 0.4622 - val_mae: 0.3763\n",
      "Epoch 251/300\n",
      "10/10 [==============================] - 6s 564ms/step - loss: 0.4701 - R_squared: -0.1535 - mae: 0.5164 - val_loss: 0.2486 - val_R_squared: 0.4623 - val_mae: 0.3763\n",
      "Epoch 252/300\n",
      "10/10 [==============================] - 6s 602ms/step - loss: 0.4526 - R_squared: -0.1076 - mae: 0.5046 - val_loss: 0.2483 - val_R_squared: 0.4627 - val_mae: 0.3761\n",
      "Epoch 253/300\n",
      "10/10 [==============================] - 6s 568ms/step - loss: 0.4322 - R_squared: -0.0579 - mae: 0.4926 - val_loss: 0.2487 - val_R_squared: 0.4615 - val_mae: 0.3766\n",
      "Epoch 254/300\n",
      "10/10 [==============================] - 6s 566ms/step - loss: 0.4493 - R_squared: -0.1018 - mae: 0.4974 - val_loss: 0.2484 - val_R_squared: 0.4623 - val_mae: 0.3763\n",
      "Epoch 255/300\n",
      "10/10 [==============================] - 6s 584ms/step - loss: 0.4846 - R_squared: -0.1901 - mae: 0.5196 - val_loss: 0.2510 - val_R_squared: 0.4561 - val_mae: 0.3786\n",
      "Epoch 256/300\n",
      "10/10 [==============================] - 6s 564ms/step - loss: 0.4647 - R_squared: -0.1410 - mae: 0.5083 - val_loss: 0.2534 - val_R_squared: 0.4503 - val_mae: 0.3807\n",
      "Epoch 257/300\n",
      "10/10 [==============================] - 6s 555ms/step - loss: 0.4521 - R_squared: -0.1045 - mae: 0.5072 - val_loss: 0.2491 - val_R_squared: 0.4605 - val_mae: 0.3767\n",
      "Epoch 258/300\n",
      "10/10 [==============================] - 6s 573ms/step - loss: 0.4685 - R_squared: -0.1472 - mae: 0.5112 - val_loss: 0.2518 - val_R_squared: 0.4540 - val_mae: 0.3791\n",
      "Epoch 259/300\n",
      "10/10 [==============================] - 6s 565ms/step - loss: 0.4621 - R_squared: -0.1318 - mae: 0.5153 - val_loss: 0.2584 - val_R_squared: 0.4385 - val_mae: 0.3853\n",
      "Epoch 260/300\n",
      "10/10 [==============================] - 6s 591ms/step - loss: 0.4933 - R_squared: -0.2064 - mae: 0.5234 - val_loss: 0.2604 - val_R_squared: 0.4332 - val_mae: 0.3870\n",
      "Epoch 261/300\n",
      "10/10 [==============================] - 7s 628ms/step - loss: 0.4922 - R_squared: -0.2044 - mae: 0.5241 - val_loss: 0.2540 - val_R_squared: 0.4479 - val_mae: 0.3811\n",
      "Epoch 262/300\n",
      "10/10 [==============================] - 6s 567ms/step - loss: 0.4680 - R_squared: -0.1484 - mae: 0.5119 - val_loss: 0.2565 - val_R_squared: 0.4419 - val_mae: 0.3831\n",
      "Epoch 263/300\n",
      "10/10 [==============================] - 6s 603ms/step - loss: 0.4697 - R_squared: -0.1627 - mae: 0.5060 - val_loss: 0.2538 - val_R_squared: 0.4480 - val_mae: 0.3806\n",
      "Epoch 264/300\n",
      "10/10 [==============================] - 6s 577ms/step - loss: 0.4773 - R_squared: -0.1660 - mae: 0.5166 - val_loss: 0.2517 - val_R_squared: 0.4531 - val_mae: 0.3787\n",
      "Epoch 265/300\n",
      "10/10 [==============================] - 6s 575ms/step - loss: 0.4474 - R_squared: -0.0964 - mae: 0.5038 - val_loss: 0.2542 - val_R_squared: 0.4476 - val_mae: 0.3811\n",
      "Epoch 266/300\n",
      "10/10 [==============================] - 6s 576ms/step - loss: 0.4491 - R_squared: -0.1017 - mae: 0.5032 - val_loss: 0.2539 - val_R_squared: 0.4484 - val_mae: 0.3808\n",
      "Epoch 267/300\n",
      "10/10 [==============================] - 6s 560ms/step - loss: 0.4738 - R_squared: -0.1607 - mae: 0.5137 - val_loss: 0.2536 - val_R_squared: 0.4488 - val_mae: 0.3806\n",
      "Epoch 268/300\n",
      "10/10 [==============================] - 6s 558ms/step - loss: 0.4670 - R_squared: -0.1522 - mae: 0.5093 - val_loss: 0.2505 - val_R_squared: 0.4559 - val_mae: 0.3779\n",
      "Epoch 269/300\n",
      "10/10 [==============================] - 6s 562ms/step - loss: 0.4703 - R_squared: -0.1537 - mae: 0.5127 - val_loss: 0.2522 - val_R_squared: 0.4513 - val_mae: 0.3794\n",
      "Epoch 270/300\n",
      "10/10 [==============================] - 6s 576ms/step - loss: 0.4559 - R_squared: -0.1194 - mae: 0.5018 - val_loss: 0.2570 - val_R_squared: 0.4400 - val_mae: 0.3837\n",
      "Epoch 271/300\n",
      "10/10 [==============================] - 6s 583ms/step - loss: 0.4536 - R_squared: -0.1107 - mae: 0.5053 - val_loss: 0.2579 - val_R_squared: 0.4381 - val_mae: 0.3846\n",
      "Epoch 272/300\n",
      "10/10 [==============================] - 6s 593ms/step - loss: 0.4363 - R_squared: -0.0693 - mae: 0.4955 - val_loss: 0.2594 - val_R_squared: 0.4346 - val_mae: 0.3858\n",
      "Epoch 273/300\n",
      "10/10 [==============================] - 6s 596ms/step - loss: 0.4584 - R_squared: -0.1241 - mae: 0.5089 - val_loss: 0.2561 - val_R_squared: 0.4415 - val_mae: 0.3826\n",
      "Epoch 274/300\n",
      "10/10 [==============================] - 6s 569ms/step - loss: 0.4646 - R_squared: -0.1370 - mae: 0.5125 - val_loss: 0.2607 - val_R_squared: 0.4294 - val_mae: 0.3864\n",
      "Epoch 275/300\n",
      "10/10 [==============================] - 6s 594ms/step - loss: 0.4848 - R_squared: -0.1905 - mae: 0.5198 - val_loss: 0.2578 - val_R_squared: 0.4366 - val_mae: 0.3836\n",
      "Epoch 276/300\n",
      "10/10 [==============================] - 6s 569ms/step - loss: 0.4800 - R_squared: -0.1772 - mae: 0.5158 - val_loss: 0.2565 - val_R_squared: 0.4401 - val_mae: 0.3824\n",
      "Epoch 277/300\n",
      "10/10 [==============================] - 6s 569ms/step - loss: 0.4906 - R_squared: -0.2078 - mae: 0.5204 - val_loss: 0.2519 - val_R_squared: 0.4518 - val_mae: 0.3784\n",
      "Epoch 278/300\n",
      "10/10 [==============================] - 6s 582ms/step - loss: 0.4569 - R_squared: -0.1206 - mae: 0.5044 - val_loss: 0.2538 - val_R_squared: 0.4468 - val_mae: 0.3801\n",
      "Epoch 279/300\n",
      "10/10 [==============================] - 6s 572ms/step - loss: 0.4796 - R_squared: -0.1830 - mae: 0.5199 - val_loss: 0.2604 - val_R_squared: 0.4312 - val_mae: 0.3861\n",
      "Epoch 280/300\n",
      "10/10 [==============================] - 6s 565ms/step - loss: 0.4728 - R_squared: -0.1670 - mae: 0.5113 - val_loss: 0.2610 - val_R_squared: 0.4297 - val_mae: 0.3867\n",
      "Epoch 281/300\n",
      "10/10 [==============================] - 6s 604ms/step - loss: 0.4358 - R_squared: -0.0685 - mae: 0.4947 - val_loss: 0.2620 - val_R_squared: 0.4271 - val_mae: 0.3874\n",
      "Epoch 282/300\n",
      "10/10 [==============================] - 7s 611ms/step - loss: 0.4557 - R_squared: -0.1171 - mae: 0.5052 - val_loss: 0.2622 - val_R_squared: 0.4269 - val_mae: 0.3875\n",
      "Epoch 283/300\n",
      "10/10 [==============================] - 6s 556ms/step - loss: 0.4613 - R_squared: -0.1312 - mae: 0.5085 - val_loss: 0.2606 - val_R_squared: 0.4303 - val_mae: 0.3861\n",
      "Epoch 284/300\n",
      "10/10 [==============================] - 6s 580ms/step - loss: 0.4687 - R_squared: -0.1471 - mae: 0.5079 - val_loss: 0.2617 - val_R_squared: 0.4283 - val_mae: 0.3871\n",
      "Epoch 285/300\n",
      "10/10 [==============================] - 6s 581ms/step - loss: 0.4712 - R_squared: -0.1539 - mae: 0.5140 - val_loss: 0.2621 - val_R_squared: 0.4278 - val_mae: 0.3876\n",
      "Epoch 286/300\n",
      "10/10 [==============================] - 6s 568ms/step - loss: 0.4445 - R_squared: -0.0891 - mae: 0.5016 - val_loss: 0.2560 - val_R_squared: 0.4425 - val_mae: 0.3820\n",
      "Epoch 287/300\n",
      "10/10 [==============================] - 6s 589ms/step - loss: 0.4634 - R_squared: -0.1360 - mae: 0.5076 - val_loss: 0.2554 - val_R_squared: 0.4440 - val_mae: 0.3816\n",
      "Epoch 288/300\n",
      "10/10 [==============================] - 6s 570ms/step - loss: 0.4790 - R_squared: -0.1783 - mae: 0.5154 - val_loss: 0.2512 - val_R_squared: 0.4532 - val_mae: 0.3778\n",
      "Epoch 289/300\n",
      "10/10 [==============================] - 6s 591ms/step - loss: 0.5047 - R_squared: -0.2371 - mae: 0.5266 - val_loss: 0.2498 - val_R_squared: 0.4564 - val_mae: 0.3765\n",
      "Epoch 290/300\n",
      "10/10 [==============================] - 6s 587ms/step - loss: 0.4554 - R_squared: -0.1158 - mae: 0.5002 - val_loss: 0.2541 - val_R_squared: 0.4459 - val_mae: 0.3803\n",
      "Epoch 291/300\n",
      "10/10 [==============================] - 6s 560ms/step - loss: 0.4732 - R_squared: -0.1596 - mae: 0.5160 - val_loss: 0.2561 - val_R_squared: 0.4410 - val_mae: 0.3821\n",
      "Epoch 292/300\n",
      "10/10 [==============================] - 6s 570ms/step - loss: 0.4588 - R_squared: -0.1317 - mae: 0.5030 - val_loss: 0.2612 - val_R_squared: 0.4292 - val_mae: 0.3867\n",
      "Epoch 293/300\n",
      "10/10 [==============================] - 6s 577ms/step - loss: 0.4544 - R_squared: -0.1256 - mae: 0.5037 - val_loss: 0.2661 - val_R_squared: 0.4174 - val_mae: 0.3910\n",
      "Epoch 294/300\n",
      "10/10 [==============================] - 6s 579ms/step - loss: 0.4352 - R_squared: -0.0697 - mae: 0.4988 - val_loss: 0.2572 - val_R_squared: 0.4387 - val_mae: 0.3829\n",
      "Epoch 295/300\n",
      "10/10 [==============================] - 6s 554ms/step - loss: 0.4633 - R_squared: -0.1402 - mae: 0.5040 - val_loss: 0.2516 - val_R_squared: 0.4522 - val_mae: 0.3780\n",
      "Epoch 296/300\n",
      "10/10 [==============================] - 6s 575ms/step - loss: 0.4383 - R_squared: -0.0758 - mae: 0.4992 - val_loss: 0.2507 - val_R_squared: 0.4546 - val_mae: 0.3771\n",
      "Epoch 297/300\n",
      "10/10 [==============================] - 6s 576ms/step - loss: 0.4476 - R_squared: -0.1016 - mae: 0.5006 - val_loss: 0.2552 - val_R_squared: 0.4438 - val_mae: 0.3812\n",
      "Epoch 298/300\n",
      "10/10 [==============================] - 6s 563ms/step - loss: 0.4701 - R_squared: -0.1516 - mae: 0.5084 - val_loss: 0.2551 - val_R_squared: 0.4439 - val_mae: 0.3811\n",
      "Epoch 299/300\n",
      "10/10 [==============================] - 7s 615ms/step - loss: 0.4717 - R_squared: -0.1610 - mae: 0.5121 - val_loss: 0.2556 - val_R_squared: 0.4426 - val_mae: 0.3816\n",
      "Epoch 300/300\n",
      "10/10 [==============================] - 6s 588ms/step - loss: 0.4768 - R_squared: -0.1686 - mae: 0.5202 - val_loss: 0.2540 - val_R_squared: 0.4463 - val_mae: 0.3803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x58bb54700>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for layer in model.layers[:-4]:\n",
    "    layer.trainable = True\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(0.0001, decay_steps=100, decay_rate=0.95, staircase=False)\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "model.compile(optimizer, loss='mse' , metrics=[R_squared,\"mae\"])\n",
    "\n",
    "model.fit(train_dataset, validation_data = val_dataset, epochs=300, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f0e252e38867c5c5dbaa4d840f6f52d4785511881d0242d037299812a0aec8ad"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('tensorflow_m1': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
