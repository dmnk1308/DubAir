{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/statsmodels/compat/pandas.py:65: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import Int64Index as NumericIndex\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "from load_data import *\n",
    "from helpers import *\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import squareform\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn import ensemble\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from hyperopt.pyll.base import scope\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final prediction models\n",
    "Use several different models with the selected features and compare their results in terms of R^2 and MSE\n",
    "\n",
    "Models:\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "- Neural Network\n",
    "- SVR (?)\n",
    "- Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings = pd.read_csv(\"/Users/dmnk/OneDrive - stud.uni-goettingen.de/Dokumente/3. Semester/SeminarDL/DubAir/Final_listings.csv\")\n",
    "listings.drop(listings.columns[0], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have fun implementing your models.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "price, listings = load_data()\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(listings, price[\"log_price\"], random_state = 123, test_size = 0.2)\n",
    "model_score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings.rename(columns = {\"property_type_Private room in residential home\": \"property_type_private_room_residential_home\",\"property_type_Entire rental unit\": \"property_type_entire_rental_units\"}, inplace = True)\n",
    "listings[\"property_type_private_room_residential_home\"] = listings[\"property_type_private_room_residential_home\"].values.astype(np.int8)\n",
    "listings[\"property_type_entire_rental_units\"] = listings[\"property_type_entire_rental_units\"].values.astype(np.int8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings[\"log_price\"] = price[\"log_price\"]\n",
    "listings.to_csv(\"colab.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_squared(y, y_pred):\n",
    "  residual = tf.reduce_sum(tf.square(tf.subtract(y, y_pred)))\n",
    "  total = tf.reduce_sum(tf.square(tf.subtract(y, tf.reduce_mean(y))))\n",
    "  r2 = tf.subtract(1.0, tf.math.divide(residual, total))\n",
    "  return r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE:                                                 \n",
      "0.20827706082930889                                    \n",
      "SCORE:                                                                            \n",
      "0.22588902655672416                                                               \n",
      "SCORE:                                                                            \n",
      "0.24565732335228402                                                               \n",
      "SCORE:                                                                            \n",
      "0.2073041469721654                                                                \n",
      "SCORE:                                                                            \n",
      "0.24465746171678654                                                              \n",
      "SCORE:                                                                           \n",
      "0.2043852476700559                                                               \n",
      "SCORE:                                                                           \n",
      "0.20374361660495768                                                              \n",
      "SCORE:                                                                            \n",
      "0.20059096159928214                                                               \n",
      "SCORE:                                                                            \n",
      "0.20371755415353746                                                               \n",
      "SCORE:                                                                            \n",
      "0.20302214850422004                                                               \n",
      "SCORE:                                                                             \n",
      "0.20443535842484925                                                                \n",
      "SCORE:                                                                             \n",
      "0.20305176049355594                                                                \n",
      "SCORE:                                                                             \n",
      "0.2042673632228518                                                                 \n",
      "SCORE:                                                                             \n",
      "0.20651959135485004                                                                \n",
      "SCORE:                                                                             \n",
      "0.2021128503529549                                                                 \n",
      "SCORE:                                                                             \n",
      "0.21138281557709762                                                                \n",
      "SCORE:                                                                             \n",
      "0.20411382283886206                                                                \n",
      "SCORE:                                                                             \n",
      "0.20249856478692654                                                                \n",
      "SCORE:                                                                             \n",
      "0.2043132271672413                                                                 \n",
      "SCORE:                                                                             \n",
      "0.20454209976651644                                                                \n",
      "SCORE:                                                                             \n",
      "0.2038326097992331                                                                 \n",
      "SCORE:                                                                             \n",
      "0.20927619438918826                                                                \n",
      "SCORE:                                                                             \n",
      "0.20318965051584253                                                                \n",
      "SCORE:                                                                             \n",
      "0.20364531888482224                                                                \n",
      "SCORE:                                                                             \n",
      "0.20247135239977992                                                                \n",
      "SCORE:                                                                             \n",
      "0.2052490426335165                                                                 \n",
      "SCORE:                                                                             \n",
      "0.20225208006692255                                                                \n",
      "SCORE:                                                                             \n",
      "0.20396388617676023                                                                \n",
      "SCORE:                                                                             \n",
      "0.20711597246487606                                                                \n",
      "SCORE:                                                                             \n",
      "0.20605380614723623                                                                \n",
      "SCORE:                                                                             \n",
      "0.20715595687181962                                                                \n",
      "SCORE:                                                                             \n",
      "0.2058202805390884                                                                 \n",
      "SCORE:                                                                             \n",
      "0.20372930303433934                                                                \n",
      "SCORE:                                                                             \n",
      "0.2027723555767121                                                                 \n",
      "SCORE:                                                                             \n",
      "0.205985909047732                                                                  \n",
      "SCORE:                                                                             \n",
      "0.3894520306259598                                                                 \n",
      "SCORE:                                                                             \n",
      "0.20442073060626015                                                                \n",
      "SCORE:                                                                             \n",
      "0.2054601223856799                                                                 \n",
      "SCORE:                                                                             \n",
      "0.20395592208433966                                                                \n",
      "SCORE:                                                                             \n",
      "0.217568834011276                                                                  \n",
      "SCORE:                                                                             \n",
      "0.38980663162025814                                                                \n",
      "SCORE:                                                                             \n",
      "0.2054427755540947                                                                 \n",
      "SCORE:                                                                             \n",
      "0.2072979389084045                                                                 \n",
      "SCORE:                                                                             \n",
      "0.2061192517917305                                                                 \n",
      "SCORE:                                                                             \n",
      "0.20607441858124925                                                                \n",
      "SCORE:                                                                             \n",
      "0.20742644438795774                                                                \n",
      "SCORE:                                                                             \n",
      "0.20276296500630428                                                                \n",
      "SCORE:                                                                             \n",
      "0.20411488730902724                                                                \n",
      "SCORE:                                                                             \n",
      "0.20483164019673453                                                                \n",
      "SCORE:                                                                             \n",
      "0.2036430558592582                                                                 \n",
      "SCORE:                                                                             \n",
      "0.25009873875758115                                                                \n",
      "SCORE:                                                                             \n",
      "0.20640268388878527                                                                \n",
      "SCORE:                                                                             \n",
      "0.20458347021503082                                                                \n",
      "SCORE:                                                                             \n",
      "0.204275716419606                                                                  \n",
      "SCORE:                                                                             \n",
      "0.2046175904206221                                                                 \n",
      "SCORE:                                                                             \n",
      "0.20530244557303695                                                                \n",
      "SCORE:                                                                             \n",
      "0.20478521961129767                                                                \n",
      "SCORE:                                                                             \n",
      "0.20072873462025115                                                                \n",
      "SCORE:                                                                             \n",
      "0.204973043720606                                                                  \n",
      "SCORE:                                                                             \n",
      "0.2057634627415407                                                                 \n",
      "SCORE:                                                                             \n",
      "0.2070933068376728                                                                 \n",
      "SCORE:                                                                             \n",
      "0.20650786146589964                                                                \n",
      "SCORE:                                                                             \n",
      "0.20633727195557047                                                                \n",
      "SCORE:                                                                             \n",
      "0.2090837969327372                                                                 \n",
      "SCORE:                                                                             \n",
      "0.2058273094297469                                                                 \n",
      "SCORE:                                                                             \n",
      "0.20547306729416231                                                                \n",
      "SCORE:                                                                             \n",
      "0.20345971092499907                                                                \n",
      "SCORE:                                                                             \n",
      "0.20903142105294883                                                                \n",
      "SCORE:                                                                             \n",
      "0.20692274420863013                                                                \n",
      "SCORE:                                                                             \n",
      "0.20408575172769025                                                                \n",
      "SCORE:                                                                             \n",
      "0.20417564105036032                                                                \n",
      "SCORE:                                                                             \n",
      "0.20526644096784546                                                                \n",
      "SCORE:                                                                             \n",
      "0.2040381830306785                                                                 \n",
      "SCORE:                                                                             \n",
      "0.20351518528847254                                                                \n",
      "SCORE:                                                                             \n",
      "0.2054314438165524                                                                 \n",
      "SCORE:                                                                             \n",
      "0.2059407435814534                                                                 \n",
      "SCORE:                                                                             \n",
      "0.2078434753481817                                                                 \n",
      "SCORE:                                                                             \n",
      "0.2055776866716659                                                                 \n",
      "SCORE:                                                                             \n",
      "0.20593182632981416                                                                \n",
      "SCORE:                                                                             \n",
      "0.2043431996321409                                                                 \n",
      "SCORE:                                                                             \n",
      "0.2058202816960081                                                                 \n",
      "SCORE:                                                                             \n",
      "0.20663729179871015                                                                \n",
      "SCORE:                                                                             \n",
      "0.20460425049367342                                                                \n",
      "SCORE:                                                                             \n",
      "0.2042198053705223                                                                 \n",
      "SCORE:                                                                             \n",
      "0.22136835898567117                                                                \n",
      "SCORE:                                                                             \n",
      "0.20580654327314868                                                                \n",
      "SCORE:                                                                             \n",
      "0.20684055276767702                                                                \n",
      "SCORE:                                                                             \n",
      "0.2037037975786155                                                                 \n",
      "SCORE:                                                                             \n",
      "0.20231179164961374                                                                \n",
      "SCORE:                                                                             \n",
      "0.20389160092419495                                                                \n",
      "SCORE:                                                                             \n",
      "0.20410710262448378                                                                \n",
      "SCORE:                                                                             \n",
      "0.2034797793152311                                                                 \n",
      "SCORE:                                                                             \n",
      "0.2051369326462451                                                                 \n",
      "SCORE:                                                                             \n",
      "0.2042700645018457                                                                 \n",
      "SCORE:                                                                             \n",
      "0.20860945063381237                                                                \n",
      "SCORE:                                                                             \n",
      "0.20434496821620357                                                                \n",
      "SCORE:                                                                             \n",
      "0.20605900864785973                                                                \n",
      "SCORE:                                                                             \n",
      "0.2032824313725374                                                                 \n",
      "SCORE:                                                                             \n",
      "0.20754855395175273                                                                \n",
      "SCORE:                                                                             \n",
      "0.2797165736082738                                                                 \n",
      "100%|██████████| 100/100 [14:01<00:00,  8.42s/trial, best loss: 0.20059096159928214]\n"
     ]
    }
   ],
   "source": [
    "space={'max_depth': hp.quniform(\"max_depth\", 2, 100, 2),\n",
    "        \"min_samples_split\": hp.quniform(\"min_samples_split\", 2, 1000, 2),\n",
    "        \"max_features\": hp.quniform(\"max_features\", 2,24,2),\n",
    "        'n_estimators': 200,\n",
    "        'seed': 0\n",
    "    }\n",
    "    \n",
    "def objective(space):\n",
    "    model = RandomForestRegressor(\n",
    "                    n_estimators =space['n_estimators'], max_depth = int(space['max_depth']))\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, pred)\n",
    "    print (\"SCORE:\", mse)\n",
    "    return {'loss': mse, 'status': STATUS_OK }\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best_hyperparams = fmin(fn = objective,\n",
    "                        space = space,\n",
    "                        algo = tpe.suggest,\n",
    "                        max_evals = 100,\n",
    "                        trials = trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparams\n",
    "best_hyperparams[\"max_depth\"] = int(best_hyperparams[\"max_depth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "waterfall_plot requires a scalar base_values of the model output as the first parameter, but you have passed an array as the first parameter! Try shap.waterfall_plot(explainer.base_values[0], values[0], X[0]) or for multi-output models try shap.waterfall_plot(explainer.base_values[0], values[0][0], X[0]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/97/j215pw6x7sq158bvx1ktlhf80000gn/T/ipykernel_19427/870970173.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mshap_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplots\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaterfall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/tensorflow_m1/lib/python3.9/site-packages/shap/plots/_waterfall.py\u001b[0m in \u001b[0;36mwaterfall\u001b[0;34m(shap_values, max_display, show)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# make sure we only have a single output to explain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_values\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_values\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_values\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         raise Exception(\"waterfall_plot requires a scalar base_values of the model output as the first \" \\\n\u001b[0m\u001b[1;32m     57\u001b[0m                         \u001b[0;34m\"parameter, but you have passed an array as the first parameter! \"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                         \u001b[0;34m\"Try shap.waterfall_plot(explainer.base_values[0], values[0], X[0]) or \"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: waterfall_plot requires a scalar base_values of the model output as the first parameter, but you have passed an array as the first parameter! Try shap.waterfall_plot(explainer.base_values[0], values[0], X[0]) or for multi-output models try shap.waterfall_plot(explainer.base_values[0], values[0][0], X[0])."
     ]
    }
   ],
   "source": [
    "# train a Random Forest\n",
    "forest = RandomForestRegressor(n_estimators = 200, max_depth = int(best_hyperparams['max_depth'])).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6472476816005108"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=0.6472476816005109>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = forest.predict(X_test)\n",
    "R_squared(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "space={'max_depth': hp.quniform(\"max_depth\", 2, 50, 2),\n",
    "        'gamma': hp.uniform ('gamma', 0, 9),\n",
    "        'reg_alpha' : hp.quniform('reg_alpha', 0,180,1),\n",
    "        'reg_lambda' : hp.uniform('reg_lambda', 0,100),\n",
    "        'colsample_bytree' : hp.uniform('colsample_bytree', 0,1),\n",
    "        'min_child_weight' : hp.quniform('min_child_weight', 0, 50, 1),\n",
    "        'n_estimators': 500,\n",
    "        'seed': 0\n",
    "    }\n",
    "    \n",
    "def objective(space):\n",
    "    model=xgb.XGBRegressor(\n",
    "                    n_estimators =space['n_estimators'], max_depth = int(space['max_depth']), gamma = space['gamma'],\n",
    "                    reg_alpha = int(space['reg_alpha']),min_child_weight=int(space['min_child_weight']),\n",
    "                    colsample_bytree=int(space['colsample_bytree']))\n",
    "    \n",
    "    #evaluation = [(X_train, y_train), (X_test, y_test)]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "\n",
    "    pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, pred)\n",
    "    print (\"SCORE:\", mse)\n",
    "    return {'loss': mse, 'status': STATUS_OK }\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best_hyperparams = fmin(fn = objective,\n",
    "                        space = space,\n",
    "                        algo = tpe.suggest,\n",
    "                        max_evals = 100,\n",
    "                        trials = trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparams\n",
    "best_hyperparams[\"max_depth\"] = int(best_hyperparams[\"max_depth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr0AAAGrCAYAAADNQy/DAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAACo3ElEQVR4nOzdd1hUV/oH8O+dYQrDzNCkiAhSBAULGlERLIgiCiQx2fRk0zYniUk2ZfPblE3fJOtusim7m+LNbnrfNYkB7F0s0dgFBUEUQUF6n35/f8wMjkhncJjh/TzPPJ6599xz3zs4w8uZc87lBEEAIYQQQgghrkzk6AAIIYQQQggZbJT0EkIIIYQQl0dJLyGEEEIIcXmU9BJCCCGEEJdHSS8hhBBCCHF5lPQSQgghhBCXR0kvIYQQQghxeZT0EkIIIYQQl0dJLyFkyOM4zo/juA0cx9VxHPcfjuP+wnHcY708di/HcbGDHCIhhJAhjqM7shFChjqO494CIBcEYRnHcX4ADgGIFAShrRfH3gjgJkEQrrdHLDzPfwkgBYAHgAoAf2OM/buLumMAvA8gAYAWwP8APMYYM1j2jwfwHoCrAFQB+D/G2I82x28FMBOAwbKpnDEWbY/rIISQ4YZ6egkhzmABgP9ayncBWN2bhNfiZwDJHMeNtFMsfwEwhjGmBnA1gFd5nr+qi7rvA7gAYCSAOABzASwDAJ7n3QCsApANwAcAA/Alz/NRHdp4mDGmtDwo4SWEkH6ipJcQMmRxHCflOK4BwEQAWRzHHQWwGMA2mzp/4zjuR5vnb3Act4njOAkACIKgAbAfQKo9YmKM5THGtJanguUR0UX1MADfM8Y0jLEKAGsBWIdajAMQBOBtxpiRMbYZwE4Ad9gjTkIIIZdyc3QAhBDSFUEQdBzHJQDYIghCAABwHFcFoMCm2l8BFHMcFwfzUIA0AEmCIOht6hwHMLmzc/A8nw0gqYsQchljGZ0c8z7MPc7uAA4CWN3F8e8CuNkyTMEb5oT9ecs+rpP6HIAJHbb9hef55TBf858YY1u7OBchhJBuUNJLCBnq4gActnnuBaDJ+kQQhBqO494B8DkAT5gT3oYObTTBPMTgMp0ltT1hjC3jef4RmMfqzoN5vG5ntgG4D0AjADGAzwD8ZNl3AuahD//H8/zbAJJhHv6wxeb4pwDkA9ABuBlAFs/zcYyx4r7GTAghwx0NbyCEDHVxuDTprQOg6lDnIMxDIJ4RBOFsJ22oANTbMyjLkIRcAMEAHuy4n+d5EYB1AH6AedLbCJh7e/9qOV4P4FoA6TBPiPsDgO8BlNmc4xfGWBNjTMsY+wzm4Q9L7HkdhBAyXFBPLyFkqJsM4Aub50cARAHYBwAcx00E8AHMvaj3APi6kzbGA/iys8Z5nl8DYHYX597BGFvcQ3xu6HxMrw+A0QD+ZRkDrOV5/hMArwL4IwAwxo7A3LtrjWWX5Tq6IqDzYRGEEEJ6QEkvIWSomwzgSZvnq2FOFL/iOG4UgCwADwDYCKCE47h5giBstVbmOE4G85Jgd3bWeC+S2nY8z/sDmA/zigttMK8qcQuAWztpt5rn+RIAD/I8/yYApSWGwzbtTQJQCPO3bstgHoLxqWWfF4AZMA+RMAC4CcAcAI/1Nl5CCCEX0fAGQsiQxXFcIMxDAk7YbP4cwBKO4zxhToDfEgThZ0EQWgG8AeC1Ds1cDWCrIAjn7BCSAPNQhjKYh1m8CfO6u6usFXieX8Pz/LOWp9fBPLGuCkARzMnr4zbt3QHgPMxje1MALLRZGUICc69wFYBqAI8AuJYxZjuJjxBCSC/RzSkIIU6H47jXAVwQBOGdXtT9BcC9giAcG/TACCGEDFmU9BJCCCGEEJdHwxsIIYQQQojLo6SXEEIIIYS4PEp6CSGEEEKIy6OklxBCCCGEuDxKegkhhBBCiMujpJcQ4tQ4jvui51qEEEKGO1qyjBDi1DiOaxEEwcPRcRBCCBnaqKeXEEIIIYS4PDdHB0AIGVQu/1XODTfcAAyD6ySEENIrXJc7aHgDIS7N5d/gWq0WMpnM0WEQQggZGrpMeml4AyHEqZWVlTk6BEIIIU6Akl5CiFPz8KA5bIQQQnpGSS8hxKlJpVJHh0AIIcQJUNJLCHFqdXV1jg6BEEKIE6CklxDi1AICAhwdAiGEECdASS8hxKnV1tY6OgRCCCFOgJJeQohT0+v1jg6BEEKIE6B1eglxbS7/Bqd1egkhhNigdXoJIa6J1uklhBDSG5T0EkKcmlKpdHQIhBBCnAAlvYQQp+bm5uboEAghhDgBSnoJIU6tvr7e0SEQQghxAjSRjRDX5vJv8JaWFroVMSGEEKsuJ7LR94KEEKdWXV19WdK78YwJZxsdFJAL85ID10Zy4Lguf6cQQsiQRUkvIcSpmUymS54X1wvI/MEEMeVldqczAbuua8K0Md6ODoUQQvqMkl5CiFMLDAy85Pnf9ppgMAEalx/YceUpJYC7u8LRYRBCSL/QRDZCiFM7d+5ce7lBK+CLfAEGSngHTUVFhaNDIISQfqGklxDi1NRqdXv5oyOmrmcwELugJeIIIc6Kkl5CiEswmgT8ba+AVoOjI3Ftnp5ejg6BEEL6hZJeQohTa2w0L9OwqkhAGyW8g66mptrRIRBCSL9Q0ksIcWpBQUEAgFd2m9Csd3Aww4Ba7enoEAghpF9ocBYhxKlVVFSgxn0MTtY5OpLhQa/XOToEQgjpF+rpJf2WlJT0vo+Pz4C+UJ44cWLepEmTjtgrpv5ISkp6LzAwsM2RMZD+E4lEeH2PacgNbfCWAz9cI0Lzo2KcZmLcMq7rKXaxI4C114tQtUwM4cnL+yLG+QCbbhSh/hExTt4rxrWRjpuu19ZGbxVCiHOya08vz/MKAP8CsBTm28CtBPAwY6zTT0me5+8C8DGAVpvNWYyxW2zqXAfgJQDhAMoBPMcY+6/N/tMAAgHY/spLYIwdHfgVEXvy8fExxMTE8Lm5ucus244ePRrryJgAIDc39yEADzk6DtI/evcRWFMiDLn7Lb+XIoLOCAS8b0ScP5BznRiHq4zIr7m8rt4IfF8g4P1DAlYtFV+yT8wBq64V48PDJiz8rwlzgzlkXSfClM+NDund7rguMiGEOAt7D294F8A4y0MA8BOAtwA82M0xpxhjkZ3t4Hl+JoAvAWQA2AogHcBKnudLGWO/2FT9HWPsywFHTwhxOn/brYEAmaPDuIRCAlwfxWHCp0a06IGd5cDPRQLuiBHhmR2my+oX1gGFdQIivC5va5wvEKQE3t5vTuu3nBWws9zc1gs7L29rsFVUVCA2IPSKn5cQQgbKbsMbeJ53B3A7gOcZY5WMsQsAngdwJ8/z8n42ex2AdYyxzYwxE2MsC8BOAPfbJ+q+mT179ruBgYFtcrlcUKvVxgkTJuSnp6f7Wfenp6cHxMXF7fPx8dHL5XIhICBAk5KS8iAAZGRkuMfHx6/z8/PTyeVywdfXVz9nzpw3LPtk06ZN2+Tj46P38PAwjRkzpjY1NfVaa7sxMTFF48ePPxUbG1uoUCgET09P4+zZs/+5cOHCm0aNGtUil8uF0NDQusWLF0+xHuPj42OIj4/fEBoaWieTyYSAgADNwoULr589e/Y/fX19de7u7kJsbOyJjIyM9mxh4cKF148ZM6ZWoVCYfHx89PHx8RsyMzMl1v0LFiy4KygoqEUmkwkhISENGo0m2vb1mTlz5ko/Pz+tXC4XvL29DVOnTt1lPT4yMrKivr5e/Msvvzwok8mEsLCwauu1xcbGnrC2kZaWNmvs2LHnlUqlycvLyzB58uSDGRkZ7fc85ThOSEhI+NZ63cHBwU2LFi1a3NPPbsmSJWOio6PPenh4mNzd3a0/m4cAIDEx8d++vr46yzXeKZPJBNsHx3HC3Llz/2JpJzI2NrbAy8vLoFQqTdHR0WcXL17cY2+19RzTp09frVarjSqVyhQXF/drRkaGu821J0VHR59Vq9VGhUIhjB49unHJkiWRADB16tSdPj4+eplMJvj6+upnzpz5Y0/nHA7a9AK+LlFDa3R0JJeK8gaMJlzSE3u4SkDsiL631dlABg7AhH60ZQ9SqdQxJyaEkAGyZ09vNAA5gP022w4AcAcQBaCrcZujeZ6vAKCHOaF9hjFWYtknwuWf+SIAcR22vcXz/D8AlAL4gDG2oqsgeZ5/GsDT3VzH14yxZZ3tEIvF1ZMmTbpJJpPl6HS6uQcPHlwjlUpXAZgFAEVFRfva2to8p0+fPl8ikeTqdLoUACYAKC8v31JeXj4lLi7uFrlc/oPBYLjKYDCEAkBlZeWqkpKS2fHx8VdLJJLcioqKrD179qxMT08PycnJKQeA4uLisFmzZr0cHh7+55qams/37t378OjRo2+YNGnSXJFIVJKXl3fy7NmzXwMYb423pKRk7tSpU5dOnDhxc3Fxcd6BAwe+HTlyZNHMmTNHGY3GiJ07d+7y8fH5O4CH09PTR+/Zs+f76OjoLRMmTFiq0+nm/Prrrz9xHPcjgIz09PTRu3fv/jg2Nnbd1KlTf6PRaH6zd+/ej93c3Nq/VZZKpaeuuuqqBRKJJFej0dy0Z8+er+Ry+ScAbi8qKgrsbHiDrYyMDNnBgwc3+/n5nZo7d+4kk8kUcuTIke2CIGwHMNFar7S0NGPixIkpbm5ueSdPnswvLCz8EoBvNz9TVFRUfGUwGGTz5s0bKRKJLuh0uoUcx1027Gbjxo2fAfjM+jw+Pn59SUnJPA8Pj08yMzO5Y8eO7VepVOcTExPDALSePn161/Hjx3cA8Onu/ABQX18v0Wg0o+fMmTNCr9fH7d+/f51EIvkRQFp6errv3r17twQHB5+YPXv2LI7jLrS1tf2W47gWAFAoFIdnzJjxsJub26GWlpbHc3Nz/56cnPzsli1bXu/pvK7s83zTkBvWAJhv19vQYb5XgxZQSfs+FvdELXChFfi/eA5v7xeQPJrD3NEctpQ65sqVSqVDzksIIQPVY9LL8/ynAO7spsprjLHnAKgszxts9lnLanTOmswUAfAHsBzABp7nJzPGWgBkA1jL8/xCAFsAZAJIBHDapo07YU60tQDmAfiW53l0lfgyxpZbztNnW7du/bPN080JCQmrSktLlwDAkiVLYgoLC0cvXLjwmrVr1+6w1NkIAJmZmVx+fv6MhISElzds2LDSsu9XywMlJSXzo6Ojv123bt0aS/2UoqIibWNj49MAHgGA0aNHV23duvUlAEhPT/+9Tqe7NSgo6Ks1a9b8CgAJCQnrT58+fbVtvOHh4evXr1+fAwDz5s379MSJEy/PmDFjaU5OThWAqujo6PLGxsZZANDY2PiUWCwWRo4cuTArK0sAkDNr1qzvTp48eSMANDQ0PC2VSk0BAQFLLPs/mzp16n2nT5+eaT3f9u3b/8/m9N/GxcX9oaamZm5vX9+2trY7a2trZdOmTZtrjTE5Ofm1nTt3vpaZmclZzouIiIgVa9eu3WO5rv/s3bv3hZ7aFolEeq1W667T6RbI5fJv161bt76nY5KSkj48ceLEglmzZi1YvXp14YIFC+6oqKhQT5o0KTg7O7sJAJYsWZK+Zs2ak4sXL55m/Vl0EwNGjx49Lzs7uw7AllmzZn1dWFh4KwA0NTU9ZzAYRCEhIdOys7O1lkM+sh7b4Q+Ft6Kiop6or6+/BkC3SW9TUxNUKpXLlg9fADQOmMC25SYx5o3uPIHNLRPwyGYj1B06RNUyoEnX90TVYAKu/cmIf6aI8dR04NcKAd8XCA7r3a6trUWTF4bM/wEqU5nKVO5Y7kpvenofBvBkN/utk9CaLP96Aqi3KQNAY2cHMsZO2Tyt4Hn+PpgT5ZkANjHGtvI8/wDM44JHAdgB4FsAkTZtbLNpYwPP82/BPMyiy97e/kpOTn6quLj42draWpXBYOAEQYCHh4cBAHQ63XQAkEqlGzoeZzQax+t0OshkstzO2m1sbJTIZLL2nvCsrCxjSEhIi1arDbduk8vltdZyTk5ODcdxkEgkRdZtIpGoWafTXfLzlEqlpTb7G0UiEVavXt0+lEAsFmsNBoMCALRabbharW6xJpYAIJPJDjc0NNxmub4wlUp1yX65XF4E888KADB79ux/njp16p76+np3k8nEGY1GjBw5sr6r17MjnU4X4+HhYbQkvNZr2KPX62E0GscDyAfQ8brrtVptj91nAQEBN5hMph+PHDnyn+bm5i9iYmJOhYaGXrNmzZq8zuonJyc/eeDAgftnzZp177p16zZb4oszGAzYvHlzo0KhaK8rkUig0+mmwvJHTFcs11ZjfS6TyfIaGxslAKDRaKI8PT1bbBLeS8ycOfN/JSUlGU1NTVIAnF6vR1RUVI/T6G0/AFyx/PAUET45ZoLmCieAyd91f0KFBHATAZFeQFG9edtkPw55/byvw9FqYJ7NOXfeIsZneVd+PC8AeHl5OfznTmUqU5nK3ZW70mPSyxhrBtDcY0tAAQANgKkANlu2TQHQBqCwF8cD5slvAmyGNDDGPgXwqfU5z/P7AGzqpg0TOh8GZz3+WQDPdnP8l4yxBzpuzMjI8Ni1a9fyqVOnfj9p0qQHsrOz6xISEr4vKCi4DgCkUuleANDpdAsAZNkeKxaLj0ulUmi12lmw9P7aUqvVeq1WO8H6PDMzU9zY2OgRGhp6qmPdwSKTyU41NTWl2vaoarXaSZ6ennoAkEqlpzvu12g07X98pKWlTd+5c+fDs2fPfl2lUr2enZ3dEhcXt6+urm6szWm67eaSSqX5LS0t4vT0dF9rcqjT6aZLJBKIxeKCgVyfJZFOAoDFixdPPHHixNYzZ878DCCiY92FCxdev3fv3r/Fx8e/vnHjxo9t4jsqlUqRkpLilpWV1ec0q+O1abXaWLVarQcAuVxe2NjYmJaZmSnJysq65BYLKSkpDxw4cOD62bNn369QKD7JysrSR0VFlQmC4Lh1q4aImBEcYjy1OFDb32kDg6NVD/xwUsAriSL8br0JcX7ANZEcZn3d9X8bmRiQii+WBQA6S/WJI8yT3UQcsCyOw0gl8GmeY4Y3tLVpcLE/gxBCnIfdxvQyxtp4nv8SwCs8zx+zbH4FwOeMMU1nx/A8nw7gMMxLkXkD+AuAagB7LPvdAEyy1FHC3OM8GsDblv2hMC9lthvmMcFJAB4HYDsMoWOcr6OHr4Q7IwiCUq/Xw83NrSo7O7suNTU1s6io6Brr/tWrV+dHR0eXFRQUfJGWlpYhkUh26nS6ZABYt27d5ri4uH3Hjx9/NjU19YhMJltlMBimGo3G0PXr1/8QFha2pbCw8NZFixZ9LZFIdlZUVKwymUycWq3u1zCM/lCr1X81GAzLKioq1mVkZCzV6/WzCwoKbo6IiFgPAJ6enn/V6XQPVFZWZmdkZPxGq9UuLSoqSrCO6TWZTH6CIMDNze0Mx3GtCxYsuLe4uHiqj4+P9RsAeHh4tLW1tcV0FYO7u/tnPj4+/ygtLd2Wnp4+12QyhZw8efK56OjovP4kmbbmzp37ikwm+0Umk60XiUQXxGKxgeO4y9pcvHjxVfv27ftu0qRJ32/btu1PHeL7wt/f/50zZ84cWLJkyfWrV68uWrJkybimpqYHd+zY8WhPMZhMJpw9e3ZTRkZGssFgmHTy5MlbwsLCtgKASqV6XSwWP3LmzJl96enpmSKRqKKtre23MplstdFo9OM4Dm5ubqcAmObNm/fSmTNnRkVERBQP5DVxFY/GtuChPfIhdze2ZRtN+HiRCBeWiVHTBjy4wdS+XNloFZB/txgxnxhxtgkIVQOn2cWPY83jbjjdICDsI/N/0TtiRfjdRA4SEbCjXMDC/xrbE+IrTavt9OOcEEKGPHsvWfYozOv0Wnt2V8KchAJo72W9jTFmne0+D+Zxi54wD4HYCWChpXcZAMQAeJgnyQkwj+tNYoxVWvZ7wDz0IdKyvxTAK4yxf9n5upCTk1M5a9asL48ePfqATCZ7KDAwsD40NDT31KlT7WNWIyMjp5eVla3+5Zdftra2toq9vLy0EyZMeBzA5uDg4GQ3N7ef9+/f/31zc7NEqVTqY2Nj3wbwQ0BAwLUmk2nNvn37Vms0GpGfn1/DjBkzbrROYrsScnJyzi5cuPCWwsLCD/Py8prkcrkpPDx8u7+//3WW/WdSUlLuP378+DuHDx9uDQgIaIyKitpqvf7169fnTJs2bfMvv/zyodFoXBEcHFwRHh5+pL6+Psx6joiIiDePHj36vEKhMAUGBtacOnXKzzaG7OxsbVpa2sKioqL/btu2rcrNzc0UEhJybPTo0SkDvT6tVjvx2LFjTzc3N0skEokwatSo8tDQ0Gs71mtsbHywoaFBfODAgZtkMtlN1u0JCQmvbN269cUlS5ZMLSsrW7Vnz54TcrlcrFAoDKNGjerVNxleXl56uVx+bvv27dUmk4mLiIg4HBAQcA1g7olOS0tLKS4u/mb79u1njEYjN2LEiMYJEyZsUalUr0dFRd20Y8eO9RzHYcyYMcXh4eElPZ1vuFga44HnDmHIJb11GmDpqs6HIJxtAlT/uJi1nmkEuDe7Hpz8x20m/HFbl7uvKFqnlxDirDhBGIpznwlxLYmJif8+ceLEb2tqaq70ek8u/wYvLi7G+qYx+L9tAlqGWOLrapQS4Kd55UiZTOv0EkKGrC6H/tFtiAkhTs3b2xt3xoq6/pQjdiWTDa3x04QQ0luU9BKX0fGmEtZHeHh4Vc9HD0xaWtrMrs4/ceLETleIIPah0+mgkHC4fzLXPhGMDB53d0p6CSHOiYY3EOLaXP4NXlxcjIiICJQ3CYj8t/GKL182nNDwBkKIE6DhDYQQ1xQcHAwAGKXisCiMo2EOg8zHp8ebDxJCyJBESS8hxKmVlZW1l/80QwR3e69JQy7R3NybZdsJIWTooaSXEOLUZDJZezl+JIdIbwcGMwzodDpHh0AIIf1CSS8hxKmp1epLnj83UwQPiYOCcXEmgdbpJYQ4L/oikBDi1Kqqqi5JfJeO5fDufqCkwYFBuShvOaCrLQcCwh0dCiGE9Bmt3kCIa3P5N3h9fT28vLwcHcawUVFRQb29hJChjFZvIIS4ptbWVkeHMKxIpVf6poKEEGIflPQSQpxaW1ubo0MYVurq6hwdAiGE9AsNbyDEtbn8G1yr1V6yggMZXM3NzVAqlY4OgxBCukLDGwghrsl2nV4y+Gprax0dAiGE9Aut3kAIcWru7u6ODmFY0ev1lzxv1Qv4v20mNGgdFNAV8uwMEWJG0P3+CHFmlPQSQpyaQqFwdAjDivW2z1af5pnw8VEBGqODAroCOABjFC14NZmGdRDizGh4AyHEqdXU1Dg6hGHFdjiJSRDw+h7XTngBc9IrldCqFYQ4O0p6CSFOzc/Pz9EhDCu2k9jWlQguP6zBqqGR7nZCiLOjpJcQ4tQaGxsdHcKw4uZ2cVTcK7tNaNZ3U9mFiEViR4dACBkgSnoJIU5Nqx0mXY1DRH19PQAgr1rA4SrHxnIluStowiQhzo6SXkKIU+s4sYoMLustiP/yiwk6Fx/La6u5udnRIRBCBoiSXkKIU6N1eq+s6upqVLUKWFkowOjytz65iJbGI8T5UdJLCHFqHh4ejg5hWDGZTHjvoMnRYVxxRuMw6tYmxEVR0kvahYaG1sfHx29wdBwA4OPjY0hKSnofANLS0mbKZDJh8eLFU/rTVlJS0nuBgYFt9o2QDBVSKS0ldSX5+AXinQODs0yZtxz44RoRmh8V4zQT45ZxXd8MInYEsPZ6EaqWiSE8efmS818sEeHcA2I0PCJGwT1i3DtxYDeW0Ol0AzqeEOJ4dHOKPuB5XgHgXwCWwrx040oADzPGOk2oeJ7/LYAHAIwHYASwD8AfGWNHLfvdAXwOIA5ABIAXGGOvdmhjEoC3AUwFoAHwEYAXGWNO9cViTExMEcdxhry8vHF9PXbt2rV70M29tHs6T25u7kMAHurreYlzqKurg4+Pj6PDGDY+2tsAo2nEoLT9XooIOiMQ8L4Rcf5AznViHK4yIr+TpZj1RuD7AgHvHxKwaunlKyv85RcT7l0H6IxAtA+w9SYxDl4w4kBl/2LzVHv270BCyJBBPb198y6AcZZHFMzJ7Fvd1FcBeBFAMIBRAA4AWG9JdgFAALALAAOwt+PBPM97AlgLYB0APwDzAdwF4A8DvxRCXENAQICjQxg2BEHA+ye9B2WZMoUEuD6Kw/M7TWjRAzvLgZ+LBNwR0/mvqcI64ONjAvJqOv/7P78G7RPtBMH8iPDqf28vrdNLiPOjpLeXLInq7QCeZ4xVMsYuAHgewJ08z8s7O4Yx9h5jbANjrIUxpgXwOoBAmJNmMMY0jLG3GWNbYO7F7SgRgBzAG4wxA2PsOID/AFhm9wu00Ov13lFRUeVyuVzw9fXVz5079zXb/QkJCV/7+fnp3N3dheDg4KaUlJT2HtSFCxfeHBIS0uDu7i54eHiYRo8e3bhkyZIx06dPzykoKIg4ceJEtEwmE2QymZCZmSnpbUxpaWlJHMcJixcvntaf8yQmJv7b19e3/bvJ0NDQ+ri4uF+7us7MzEwuPj5+nZeXl8HDw8M0efLkA2PGjKmdNm3aVgBYsmTJmOjo6LMeHh4md3d3ISAgQGP7OnSF4zhh5syZP44cObJNLpcLoaGhdYsWLZpv3Z+RkeEeHx+/zs/PT2eNa86cOW8AQHJy8pPBwcHNCoXCpFQqTePGjTuzZMmSmN6+hq6strbW0SEMGzvKgMq2wfm1EeUNGE3AybqL2w5XCYgdQKfyewtEaHlUjIJ73XC+BVh9qv9fkLmJ6YtRQpwdvYt7LxrmBHS/zbYDANxh7vU90os2UgC0Aijq5TlFuPxrfRGAMJ7n1Yyxy1bl53n+aQBPd9Pm14yxLpPmoqKiqTNmzFgWHR390YULF1bu27fv2fT09LdycnJqZs+e/c+jR4/ePHPmzN/J5fKv6urqPszNzf1XWlrawbVr1+46ceIEHxAQsG/y5MlpgiC4aTSaWzmOa9m7d2/6QIY3dNTX8yQmJvbpOuvq6j48ceLEgpkzZ94mk8lWXbhw4X9nz56dMmKE+bdvRUXFVwaDQTZv3ryRIpHogk6nW8hxXK/GDJ86dWrJlClTFkskkoOlpaXbDh48uCYzM1OZlZWlLy8v31JeXj4lLi7uFrlc/oPBYLjKYDCEAoBIJGoZN27cQ3K5/Fuj0RiVl5e349SpU2sAhA709XR2ev0wuTvCEPD6L0a0GganbaUEaOgwbLZBC6ik/e+dfWijCY9sAhKCgHmjOWgHMA5ZJpf1/2BCyJAw7JNenuc/BXBnN1VeY4w9B/NQBQCw/Y7LWlb34jxRAP4N4A+MsaZehrcLgAnAMzzPvwlzcn2PzTkvS3oZY8sBLO9l+5cJDw/P27Rp04cAkJ6efl9bW9s1Op1uPoD/lpWV3RIVFbVr48aNH1uq3x0cHPybmpqaFwCkiUQiU1tb2yi9Xj9j7dq1uTD3StudPc7T3XWWl5f/JioqKnfDhg3fAkBmZmaGWq1uz6xEIpFeq9W663S6BXK5/Nt169at7+15x44d+/W6des2W86bfPz48erW1ta7MzMzP8rPz5+RkJDw8oYNG1Zaqv9qeWDTpk0f2DRzdN68ee/u37//+Z7O19TUBJVK5dJlLy+vYXW9jiybBHf0cnj9ZbbcJMa80Z0fm1sm4JHNRqg7zElUy4Am3cCmL5gE81CJ28dzeHAy8M+D/WlPQEtLCwDFkPlZUJnKVO663BUa3gA8DPN42a4er1vqWRNV29kM1nK390HleT4GwBYAbzLGPuxtYIyxWgDpANIAnId50tvHMCfCdd0c2m8ymazCWs7JyakCAJPJPGulpaVF5e7uftK2vkqlqtJoNMEAMH78+EwAoj179mz18fHRX3XVVdszMjLs3j1ij/P0cJ1KuVx+yro/KytLUKlULdbnAQEBN/j7+x8+cuTIfzZv3qyPiYkpWrx4cWxvziuVSvNszlvj4eFh1Ov1441G43idTgeZTJbb2XELFy68LTw8vFqtVhvlcrmwZ8+eF5qbm3vMPmw/AFy1bL1D2FCJx5XLz84QQSHu33Jlyd8Zwb1p6PQx+1sjCusANxEQ6XXxmMl+HPKq+3W6y7iJBjKml4NCoQAwdH4WVKYylbsud2XY9/QyxpoB9OZWOwUwj7udCmCzZdsUAG0ACrs6iOf5qTBPRvszY+yf/YhvD4A5Nu29AWAfY6yls/o8zz8L4NlumvySMfZAX+MAAA8Pj6a2trZI223Nzc0jAgMDdwHA2rVrd8DcG43U1NTr9uzZ81+5XP4BgHs4jrPbwp6DfR4PD49mjUYTbn2emZnJNTU1efj7+wNoT5KTAGDx4sUTT5w4sfXMmTM/w7wCR7d0Ol17cpyenu7b0tIilkgkx8Vi8XGpVAqtVjsLwMaOxx08ePCTkJCQX8ePH39DTk5O+bx5817etm3bCwO9VlegVCodHcKwMXc0Bz+5CWda7N9f0qoHfjgp4JVEEX633oQ4P+CaSA6zvjZ2eYxMDEjFF8sCzJPX/BTA/NEcsk8JaDMAC0I53DKew63Z/f94oGE0hDi/YZ/09hZjrI3n+S8BvMLz/DHL5lcAfM4Y62wSGnieTwSQDeApxhjfRR0ZzN8XigC4WSbFGRljesv+qQDyYf48vxrmlR6u6ybO13Gxd9qugoODvz148OCyBQsW3CWXy7+pq6t7r7KyUhUbG/sqACQlJfEqleqDNWvWHBSJRGUikUjgOE4PADKZrKq2tnZ8ZmamOCsrq+vfYr0w2OcZNWrUyiNHjty7cOHCG2QyWXZVVdV3jY2N7WsizZ079xWZTPaLTCZbLxKJLojFYgPHcb06V1FR0S2LFi36XCKRHDh79uwmb29vnUKh+CQrK0uIi4vbd/z48WdTU1OPyGSyVQaDYarRaAxdv379D1qtVuzm5tYgEonOpaWlzTx16tRj/bk2V+TmRh9jVwrHcXhmuoAnczEoKzgs22jCx4tEuLBMjJo24MENpvblykargPy7xYj5xIizTUCoGjjNLv7sNY+74XSDgLCPjBAE4ME4ET5cCIg44Ewj8NhmE34u7v9QCUp6CXF+9Nuibx6FeZ1ea8/uSgCPW3daellvY4xZe/NehXkIxFs8z9subbaYMbbDUi7AxclIs2Fe4uwzmJcmA8zr/N4AQArgGIAbGGOb7HhNvbZjx46HExIS/A8dOsS3tLR84uPj05KUlPS4ZVwtamtr5x85cuRemUwmcnd3N4aFhf3q7e39MAD4+/s/e+HChZxNmzbpFQoFl5KSIs3KyurXb5G+nqev7Xt7e98fFRUVvnfv3m8NBgMXGRl5aNSoUY0cx2kAQKvVTjx27NjTzc3NEolEIowaNao8NDT02t60HRYWtubw4cOr6+vr5QEBAQ1TpkzJtL4OwcHByW5ubj/v37//++bmZolSqdTHxsa+DeCHqVOnLj927Nj/HT582OTr69saEhKy9uzZs13+8TOc1NfXw9fX19FhDBuzVecg5gZn/mSdBli6qvPe2LNNgOofF/+2PNMIcG92Pquuug2Y992A/ra+DK3TS4jz4wTBqe5xQMgVl5mZKd6xY4d24sSJH+7YsePh/rbDcZwwf/78ZR0mpQ02l3+Dt7S00K2Ir6Camhq8e9wLb+wbnLuyDUUiAI/F1OHvS/wcHQohpGddDt6nnl5COjF79ux3PD09XxQEwa2iouJHnU4nUqlUbzo6LnK56upqSnqvIIPBgIeniPDGvmGS8VpIJL1eWpwQMkRR0kscIjw8vKq8vLzTZee1Wm3/F+a0kzNnztxdVVX1KACMGDGiKSEh4f7Vq1ef7u6YoX5NrspkstscSdILzc3NiAgIwNKxHL4rEGBy+e8SzCjpJcT50fAGQlyby7/B29ra4O7u3nNFYhdarRYymQxHqwTM+MqItkG6WcVQQsMbCHEqXXYy0Tq9hBCndu7cOUeHMKyUlZUBACb6cZg4gFsEOxsaQkOI86OklxDi1NTqHm+ISOzI9mv+FxJEUA6Tb/21Gq2jQyCEDBAlvYQQQnrNx8envbw4nIPa7vdcHJoMxmEwjoMQF0dJLyHEqTU2dnsXcGJnlZWV7WURx+GZ6Rzk4m4OcAECaJ1eQlwBrd5ACHFqQUFBjg5hWPH29r7k+d0TRDhSZUKdi3/7H+9RASDE0WEQQgaAVm8gxLW5/Bu8pKQEYWFhjg6DuLiKigoEBgY6OgxCSM9o9QZCiGsSiehjjAw+qbTPdzQnhAwx9NuCEOLURowYRutmEYepq6tzdAiEkAGipJcQ4tQqKiocHQIZBgICAhwdAiFkgCjpJYQ4NS8vL0eHQIaB2tpaR4dACBkgWr2BEOLUDAZaP5UMPr1ef9m2O3KM2F7mXHNFZW7A+t+IMcazy7k+hLgsSnoJIU6tubmZvnomgy44OPiS50eqBKw8KaDNyf7mUkqA/aWNGDOR1h0mww8NbyCEOLWOyQghg6GsrOyS56/vMUFrdFAwAyDmALlc7ugwCHEISnoJIU6tYzJCyGBQKpXt5QstAlYVCTA518iGdtXV1Y4OgRCHoKSXEOLUJBKJo0Mgw4Cb28XRgP88aHLqu764iWlkIxmeKOklhDg1Hx8fR4dAhoH6+noAgNYg4J8HBKcc2mClUqkcHQIhDkFJLyHEqVVWVjo6BDIMWG9B/M0JEwzO3M0LoK6ebrRBhidKegkhTs3b29vRIZBhoLq6GoIg4M+7BbRcvnqZU7Edn0zIcEJJLyHEqel0OkeHQIYBk8mEbWcFVLbar01vOfDDNSI0PyrGaSbGLeO6Xjs3dgSw9noRqpaJITw5sDG5tLY1Ga4o6SVDSlpa2kyZTCYsXrx4CgAkJib+29fXt9dZjY+PjyEpKen9rvbHx8evi4yMbL9vbWhoaH18fPwG63OZTCakpKTc19/4yZXX0tLi6BDIMBAYGIhX95js2sv7XooIOiMQ8L4Rt+UY8cFCEWJ8O6+rNwLfFwi4d51pwOfVaDQDboMQZ0RTOAcBz/MKAP8CsBQAB2AlgIcZY21d1L8LwMcAbPsQshhjt9jU8QDwFwA3AFABOAvgVsbYQcv+VwGkA4gFsJ0xtsDOl3VFrF27dg/Mr9mg2Ldv36Lu9mu12vZzz58//7Ft27a9bTQa6dZFQxit00uuhF2FF7CzfJTd2lNIgOujOEz41IgWPbCzHPi5SMAdMSI8s+PyxLawDiisExDhNfBzjxgxYuCNEOKEKOkdHO8CGGd5CAB+AvAWgAe7OeYUYyyysx08z3OWNtoATGeMneV5PhyAbRdXMYAXACyynJeQYaGsrAwRERGODoO4uM/OjIBh4J2s7aK8AaMJOGkzp+xwlYC5owf/b+zq6mpgrP0SeEKcBQ1vsDOe590B3A7gecZYJWPsAoDnAdzJ83x/b4OTCiARwF2MsbMAwBg7xRhrn7bOGPuEMZYFYNBXHZ89e/a7gYGBbXK5XFCr1cYJEybkp6en+wFAQkLC9yNHjrxk1FtaWtpskUgkpKWlJQFAbGzsSS8vL4NcLhf8/f21s2fP/pdN3SSO44TFixdP6+u5rTQaTXRISEiDTCYTgoKCWhYsWHCndd+0adO2hoWF1XR1bRzHCSkpKQ8uXrx4Sm5u7tsmkwkymUyQyWRCUlISHx0dXTp58uSDtsckJiZ+7Ovrq8vMzOz2t5WPj49h2rRpm7qKLTMzk5s1a9YXAQEBGrlcLnh5eRkSEhK+A4CFCxfeEBoaWq9UKk0KhcIUHh5etWjRouTuzjdcyGQyR4dAXFyDVsB/i6V2XbVBKQEaOgzcatACKungJ720tjUZrqin1/6iAcgB7LfZdgCAO4AoAEe6OG40z/MVAPQAdgJ4hjFWYtmXDHNP7lM8z98NoAnAdwBeZIz1a4QZz/NPA3i6mypfM8aWdbZDLBZXT5o06SaZTJaj0+nmHjx4cI1UKl0FYJa3t/cff/311xsWLlx404YNG74DgKqqqj+HhITUrV27NhcAvLy8ckNDQxeLxeIz9fX1b+7evfv3qamp69avX5/VU9zdndta5+TJk/OmT5/+20mTJv104cKF/+3Zs+eT9PT0jTk5OeW9fX3WrFlzcP78+Y9v27bt7Q5DHp745Zdf3szIyFBlZ2c3AUBpaelvIiIiNmVlZfX4K7G72Kqrq7/Kz8+/adq0aQ8rFAreaDRG6HQ663WZIiIiXp44ceK/BUHwKioq2pWXl/czzENdhjW1Wu3oEIiL+/q4CRAE9GXk1ZabxJjXRa9tbpmARzYboZZeul0tA5p0g78emkKhGPRzEDIUUdLbSzzPfwrgzm6qvMYYew4Xk5AGm33Wcle/nbcDmAigCIA/gOUANvA8P5kx1gJgBIAJANYBCAEwGsBqmIc3vNbniwHAGFtuOU+fbd269c82TzcnJCSsKi0tXQIAq1evPh0dHX32woULTwP4LjMzkyspKUmKiYlZYT1g586dd9sc/2hQUNDvmpqabgPQY9Lb3bmtIiMjd27YsOErAMjMzFxy8uRJfWNj4zMAHu771V7Kw8PjbXd39+UNDQ2vAng0NTU1/fz586oJEyb8sTfHdxdbcXHx9TExMf/btGnTB5bqhZYHNmzYsNKmmabU1NRHNmzY8GN6erpfTk5OVVfna2pqal+I3lXL5eXl7YnvUIiHyq5XHi3XQCySAn0Y3pD8Xfd3r1BIADcREOkFFNWbt03245A3yN/VCRDQ0NAAQDkkXlsqU3kwyl2hpLf3HgbwZDf7rV/pN1n+9QRQb1MGgMbODmSMnbJ5WsHz/H0wJ8ozAWyytGkE8CfGmBbASZ7n3wNwK/qZ9A5EcnLyU8XFxc/W1taqDAYDJwgCPDw82tfAGTly5Ipff/31zxkZGR4tLS0PajQasZeX13MAkJmZKT537tzm0tLShObmZgnHcdBqtQgKCvK3x7kBQC6XF1vLWVlZQkhISItWqw2zx7VnZWUJ06dPX19WVnY7gEcvXLjwcnh4+Lk1a9Yc7c3x3cVWX18vlcvlBzo7btGiRcklJSVfVlZWBmq1WhHHmXuQjEbjWABdJr22HwCuWh45cqTDY6Cya5fToxXw2axBi8F+wwJa9cAPJwW8kijC79abEOcHXBPJYdbXxi6PkYkBqfhiWQCg67p6pzhwUKvMfyQOhdeWylQejHJXKOntJcZYM4DmXlQtAKABMBXAZsu2KTBPQivs5ekEy8P63dihbur1C8/zzwJ4tpsqXzLGHui4MSMjw2PXrl3Lp06d+v2kSZMeyM7OrktISPi+oKDgOmsdlUq13M3N7eXGxsYXqqqqboiIiMjPzs6uA4C6urp/FBUVJc2YMeN6mUyWlZWVZQwKCmpBL8aX9+bcAKDRaNpnNWVmZnJNTU0eoaGhJZe32KNOF7McMWLEk4cOHTq+aNGitOLi4qlXXXXVi71tsLvYvLy8dBqNZkpnxxUWFv7P3d29LjExMXr16tVFqamp127YsOFH0Lh8tLa2wsvLy9FhEBfGcRweiWnGy4e97bpk2bKNJny8SIQLy8SoaQMe3GBCfo1532gVkH+3GDGfGHG2CQhVA6fZxV/ZmsfdcLpBQNhHfcx6AWi1GoBGRpFhiJJeO2OMtfE8/yWAV3ieP2bZ/AqAzxljnS6OyPN8OoDDAMoBeMO8NFk1gD2WKj/APBThZZ7nXwAwCuaVID62aUMCQAzzz1RkmTQnWHqGO4vzdQCv9/X6BEFQ6vV6uLm5VWVnZ9elpqZmFhUVXWNbJysryzh16tRfzpw5c39FRYXn7Nmz24czGI1Gb5FIJIjF4pOCILglJiZ+VFlZqQgKCrLLuQGgqKgoceHChbfKZLIfL1y48D+9Xi9Sq9V/7eu1SiSSQpPJhLS0tNlr167dYd2+evXqE+PGjSs9evToD25ubiaVStXr17G72CIiIn46fvz4b1JSUu5TKBQfW8b0JmzcuPEzvV4vUyqVGrFYfGbJkiVRp0+ffq+v1+Oq2to6XQmQELu6Z4oCr3Y1I6Of6jTA0lWdj5k42wSo/nExoT3TCHBv2uemElq6oQsZpoZ9L9EgeRQXx2MWwtz7+7h1J8/zz/I8n2dTfx6AvTD3JOcB8AWw0NK7DMZYE8xLkSUCqAOwFcA3AN60aeMjmHuT/wTzxLc2y3ntKicnp3LmzJlfHj169AGZTCYUFBR8Hhoamtuxnp+f34ulpaWeXl5e2o0bN35q3e7j4/OIv7//+S1bthzbtm1ba0tLy8TRo0f36kbwvT332LFjtxUUFHywYcOG1rKysnkzZ878XU5Oztm+Xuu6devWT5w48Vhubu42hUIhJCUlfWjdFxQU9M758+fdIyIitmdlZfW6q6W72EaMGHHz+PHjVx49evRfGzZsMOzevTu/paVlMQBER0c/ceHChcj169frfv3116N+fn7r+no9rorW6SVXQn11BR6ewkEmdnQkA0fr9JLhihOEwZ8pSoirSUtLm71hw4btCxcunLF27dq9vTnGx8fHEBMTw+fm5na6KsYgcfk3eHFxMa3TSwZdTU0N9HIfhPFGaPo+omDI8JQC78ZX4s4EWqeXuKwul1mhnl5C+igjI0NWXl7+YWRkZFlvE14yeNzd3R0dAhkGDAYDAj04ZEZwEDn5PRplUmnPlQhxQZT0EtIHCxYsuGPDhg2aCxcuRISFhd1ku2/ixIl51htZdHykpaXNdFTMro7WHCVXQnOzeR7zszNFTj/EQSbr732SCHFuNLyBENfm8m9wGt5ArgStVtt+979pXxiwv7KHA4YoGt5AhgEa3kAIcU1+fn49VyJkgMrKytrLLySIoHTiO/l6enr2XIkQF0RJLyHEqTU2dnrPF0LsSiK5mOWmh3NOnfS2trb2XIkQF0RJLyHEqWm1nS5FTYhd+fj4tJfFIg5Pz+CgcNKV7vV6O95hgxAn4qRvWUIIMaN1esmVUFlZCaVS2f78nokiPLvDCHcn+y2qNQJ+tE4vGaac7O1KCCGXKisro4lsZNB5e3tf8lwl5bDxRjGOVjnXXFGpGIjAGQD0niHDD63eQIhrc/k3eEVFBQIDAx0dBiFOg94zxMXR6g2EENckpYX2CekTes+Q4YqSXkKIU6urq3N0CIQ4FXrPkOGKkl5CiFMLCAhwdAiEOBV6z5DhiiayEUKcWm1t7SWz6gkh3evsPaM1CNCbHBRQPymlXQ7dJKRTlPQSQpwarTlKSN90fM9UtwoI/7cRGoODAuoHowDwc1tx7zS1o0MhToSSXkKIU6N1egnpm47vmfcOmaA3wql6eqUioNYgd3QYxMnQmF5CiFMrKytzdAiEOBXb94zOKOCd/QI0RgcG1E/1DfWODoE4GUp6CSFOjcbzEtI3tu+Z706YYHCiHl5bYpHY0SEQJ0NJLyHEqbm50SgtQvrC+p4RBAF/3iOg2UmHxSsUCkeHQJwMJb2EEKdWX1/v6BAIcSrW90xuOXCu2bGxDERTc5OjQyBOhpJeQohTo9upEtI31vfMq3uMaHHSXl4AULhTTy/pG0p6CSFOrbq62tEhEOJUqqurUVIvYPtZR0cyMAajE62xRoYESnoJIU7NZHLSWTiEOIjJZMKb+0wwCvZr01sO/HCNCM2PinGaiXHLuK5vHBE7Alh7vQhVy8QQnuz/mHydTtfvY8nwREkvcSkxMTFFsbGxJ67U+ZKSkt4LDAxss3e78fHxG0JDQ+vt3a4rouENhPSNwicQn+bZ9w5s76WIoDMCAe8bcVuOER8sFCHGt/O6eiPwfYGAe9cNLAAvT68BHU+GH5r2PMzwPP97ALcBmAjgHGMsshfH/BbAiwBGAjgKYBljbL9l30wAzwOYBkAOoAjAnxljP9kcPwPAGwAmAdACWA/gMcZYjf2uzDFyc3MfAvCQ9XlMTEwRx3GGvLy8cQ4Ma1g5d+4cIiIiHB0GIU7jH7ubAPjYrT2FBLg+isOET81jhHeWAz8XCbgjRoRndlye2BbWAYV1AiK8BnZe8zq9/gNrhAwr1NM7/JwD8DcAr/WmMs/zSQA+APAgAG8AKwGs5nneeu9HHwDfAZhg2f9nAN/wPB9vOV4MIBvATgB+AMYDCALwDztdDxnm1Gq6DSkhvWU0CfioyAutdhwOG+UNGE3AybqL2w5XCYgdYb9zdIaWKyR9RUnvMMMY+x9jbCWA8l4ech+AHxhj6xljWph7bLUAllraW80Y+5wxVsUYM1l6eI8BSLIc7wlgBIBPGGN6xlgtgO8BTLbfVXUtLS1t1tixY88rlUqTl5eXYfLkyQczMjK8rfs5jhMSEhK+HTVqVItcLheCg4ObFi1atNi6Pz09fWRMTEyxh4eHydvb25CUlLRCLBYL8+fPfwwAEhMT/+3r66sDgOnTp+cUFBREnDhxIlomkwkymUzIzMyUTJs2bWtYWNglvdqhoaH18fHxG6zP582b95K/v79WJpMJkZGRlTqd7pJfF0uWLImMjY0t8PLyMiiVSlN0dPTZxYsXxw7Sy0YIcVFZxQJaDV2Pt+0PpQRo6DC8tkELqKT2PU9Hchndhpj0Df2ZRHoyGcCn1ieMMYHn+YPoImnleT4QQCyAI5b6tTzPrwBwH8/zzwFQA7gZwI9dnZDn+acBPN1NTF8zxpb1FHhGRobs4MGDm/38/E7NnTt3kslkCjly5Mh2QRC2wzy8AwBQWlqaMXHixBQ3N7e8kydP5hcWFn4JwBcATp8+va25uXnE3LlzJ3Ac11BUVLS7q4lTe/fuTe9seMO0adO6jTMtLW3uzp07X5wxY8YHXl5ej7e0tDy+a9euvwQGBjYAQGZmJnfs2LH9KpXqfGJiYhiA1tOnT+86fvz4DtjzO0on1djYCD8/P0eHQYhT+OSYgJY+Jr1bbhJj3ujOj8ktE/DIZiPU0ku3q2VAk86OM+U60dzSDICWLSO9Rz29LoDn+U95nhe6ebw6gOZVABo6bKuHOXntGIcHzMMffmaMbbLZ9V+Ye4ZbAFwAYALwl65OyBhbzhjz6ubRY8ILAG1tbXfW1tbKQkND5+bk5FStWbNmf1RU1GsnTpyYkJmZ2f4JHhERsWLt2rV7srOzm0aOHPmfyspKbwDIzMwUFxYWjo2MjHx59erV+Tk5OeVjxoy5szfn7ouampo/jRw5sjE3N3dZdna2dsuWLcvDw8NP21zH7RUVFeqwsLD4nJycszk5OTWhoaHpZ86c8V68eHG3GXVTU5PLlz09PR0eA5Wp7CzlJaO18JD0LRlN/s4I7k1Dp4/Z3xpRWAe4iYBIr4vHTPbjkDfIqwlKJRcz7aHw2lJ56JS7Qkmva3gY5vGyXT1eH0DbTTAPUbDlBaDRdgPP8yoAa2BOan9rs32sZfurANwtxxYDWDuAmHpFp9PFeHh4GHNycqqs26RS6R69Xg+j0Tjeuk0ikRRZyyKRqF6r1XIAYDAYxhkMBkgkkoM2dXfZO06NRhPk4eFxyfAHd3f3MpvriDMYDNi8eXOjQqEQFAqFsHXr1pMSiQQ6nW5qd22rVCqXL9t+0A2FeKhM5aFcvivOHSLBvj2wrXrgh5MCXkkUQSEBZgUB10Ry+CK/69UZZGJAKr683BcCLl7HUHhtqTx0yl2h4Q0ugDHWDGCwbiZ5GEB7YsXzPAcgDsAPNtt8YE5iTwG4nTFmO0ViMoA6xtinlucNPM//E8ARnuc9GWMde5HB8/yzAJ7tJqYvGWMP9BS4VCrNb2lpEaenp/vm5OTUAIBOp5sukUggFosLejrezc3thJubG/R6/RQAWwDAYDDM7O4YjuMu+5QXi8VNer3+ki//Wlpa2r+Tk8vl5ysrK6fb7m9rawu2uY6jUqkUKSkpbllZWcae4h5uRCL6252Q3pK5cbh7bDP4QjU0dvw0WbbRhI8XiXBhmRg1bcCDG0zIrzHvG60C8u8WI+YTI842AaFq4DS7mH5oHnfD6QYBYR/1LSC93olvJ0ccgpLeYYbneTeYf+4SABzP83IAYIxpujjkIwBreZ7/DMAOAL+HeWmyHy3tBQLYAOAAgHsYYx0/tX4F4Mnz/O0AvoF5ANbDAE51lvBaYnkdA+udBgC4u7t/5uPj84/S0tJt6enpc00mU8jJkyefi46OzutN8piVlWWMjY0tKioqemHJkiWrOY5rOH369OfdHSOTyapqa2vHZ2Zmiq3n8PDw2FRZWZmxcOHC22Qy2f9qamq+qKurk4SFhQEAfHx8Xj948ODm2bNn/9PT0/PJlpaW3586dWqMdUyvu7v7F/7+/u+cOXPmwJIlS65fvXp10ZIlS8Y1NTU9uGPHjkcH+jo5uxEjBnmKOCEu5rHpMvCF9m2zTgMsXdV5z+7ZJkD1j4sfuWcaAe7NgS8fQev0kr6iLpLh5zkAbQB4AOGWcvvNFXief5bn+Tzrc8ZYLoBlMCe/DQBuBLCEMWYd3nA/zMuV/QbmXtxmy+NZy/GnAVwP4FEANQBOAwgFcM3gXaJZdna2dsqUKQvb2tpGbNu2rWr37t37fHx8ikJCQub2to0xY8bM8/DwqN26dWv+rl27zvr6+m7iOA4cx7V0Vt/f3/9Zg8Eg3bRpk16hUAiZmZmSzZs3vzNx4sRdu3fv/mLr1q1tOp0uMDg4uD3hX7du3ZbExMTXCgoK2IYNGzSlpaX/N27cuEPW/VlZWcYJEyZMBSDas2fPCblcLuzZs+dofX39goG8Pq6ioqLC0SEQ4lRMjRW4biwH8eAurjDozOv0EtJ7nGDnsT2EuLJFixalrV+/fk1aWtrUNWvWHOz5CIdz+Td4TU0NfH19HR0GIU6jpqYG50w+mPGVEW12XK/3SpKKgGfimvDSfO+eK5Phpss/52h4AyHdSEtLm63X68crFIpPDAZDVFFR0echISENTpLwDgsGg5P+1ibEQQwGAyYGcJg4AtjrxF+U2K7eQEhv0PAGQrphMpm8jh079o/169frcnNzj4rFYl1MTEyKo+MiFzU3D9YcTkJck/U980KCCEqJg4MZgJbWTkeZEdIl6uklpBvr16/PgnniHhmigoODe65ECGlnfc8sDueglgHNTroIgtJD6egQiJOhnl5CiFMrKyvruRIhpJ31PSPiODwznYOHk/b2arRdLTpESOco6SWEODWJxEl/YxPiILbvmbsmOG8aQOP5SV857/92QggB4OPj4+gQCHEqtu8ZpZTDfZM4SJ0wG6B1eklf0ZheQohTq6yshFJJY/sI6a2O75knrhLhw0NGqJ1oMQSNEdC31AEIcHQoxIlQ0ksIcWre3rROJyF90fE9M1rN4eTvxKhzsiGyvgaXX4ac2BklvYQQp6bT6RwdAiFOpbMhQcEqDsEqBwQzALW1TtQ1TYYEJxzFQwghF7W00FqdhAxHdXV1jg6BOBm6DTEhrs3l3+BarRYymczRYRBCrrDm5mYaz0860+VtiKmnlxDi1GidXkKGp9raWkeHQJwMjeklhDg16uUlZHjS6y+/lVxRnYCShiv3BddVARx83LvsWCRDDA1vIMS1ufwbvLGxEWq12tFhEEKusI5Dm1p0AkZ+YIToCuWgOhOQGarDd0sVV+aEpLe6/B9APb2EEKdWVVVFSS8hw1BZWRkiIiLan3+Wb4JJAJou7wAeNEaO0ihnQmN6CSFOzdfX19EhEEIcwHYSm0kQ8PoeAS1X+M7EbW1tV/aEZEAo6SWEOLXW1lZHh0AIcQA3t4u9rOtKBDRor3wMHEdplDOhnxYhxKlRTwshw1N9fX17+c+7TWi+gsMarKRSyZU/Kek3SnoJIU4tODjY0SEQQhwgMDAQAJBXLeBQlWNi0God0L1M+o2SXkKIU6N1egkZnqqrqwEAy38xQWd0TAwSCd0K2ZlQ0ksIcWru7u6ODoEQ4gAmkwnVrQL+VyjA6KDFGU0mk2NOTPqFkl4yKCZNmnRYqVSaZDKZsGTJkhhHx3OlpKSkPKRQKFx+bdyhRKGgNTIJcZTaNgFLfzLC4x0DQlcY8PXx7pPAt381IfB9Azz/YcA9a43QGi5+XCrfNVzyEP/dgEc2dd2FGxgYiPcOOTbpNBqv8HIRZEBcYoE5nud/D+A2ABMBnGOMRfbimAgAbwKYb9l0HMBsxpjesn8agPcBTABwHsCLjLEvLftCAOR3aFIKQMMYG/YLhqakpLCCgoJJKSkp41evXn3C0fFcSZs2bXoPwHuOjmM4qampgZeXl6PDIGRYemiTCVIxULlMjEMXgPQfjJjsxyF2xOX3B1hXYsLyvSZsvlGMICWw9CcjXtxlwvI5YgBA86MXU5IWnYCAD4y4IarrvrnTZefwzv5QaBw0tAGgb5qcjav09J4D8DcAr/WmMs/zfgB2ADgMIASAD4CHARgt+z0BrAGwEoA3gAcAfMjzfAIAMMZKGWNK2weAfQC+tOtV9VNGRoZD34VarTZOqVTqnS3hdfTrRvrHz8/P0SEQMiy16ASsLBTw50QRlFIOScEcro7k8EV+572vn+UJuHeiOSH2lnN4PkGET491/sXY/woF+CuA2d3MU91SMwIGB48uoNVjnItLJL2Msf8xxlYCKO/lIU8AKGWMvcQYa2CMGRljvzLGrG+f6wC0AfgbY0zLGNsA4EcArLPGeJ6fAGAWgA8HdiWdCw0NrZ88efLByMjISplMJgQEBGjmzZv3vHX/tGnTto4ZM6Y2Li5un0qlMubn55cCwJw5c94IDAxsc3d3FwIDA9vmzp273HpMYmLiv319fXXTp09frVarjSqVyhQXF/drbxK/jIwM2bRp0zb5+PjoPTw8TGPGjKlNTU29FgCmT5+e88svvzxYV1cnkclkwpgxY2p7am/u3Ll/CQoKalEoFIK/v782KSmpvafUGufMmTP/5+XlZVAoFKYJEybkZ2Zmtq8Ts2DBgnuCgoJa5HK5EBIS0mCJrcfvnGxeg2wvLy/Dvn376gAgNTX12vDw8GqlUmny9vY2TJ06dZf1dYmKiiqbPHnygQ7t/MfX11efmZnJzZ8//zGxWHzJp/isWbM+DwgI0Li7uwsjR45sTU5OfhoAMjMzJQqFwpSSknIfAKSlpc3lOE646qqrtlqP9ff3186ZM+fvADBz5sz/+fj46OVyueDp6WmcOnXqzp6ucThobGx0dAiEDEuFdYBYBET5XOzVnezHIa+68/p5NQIm+11at7IVqGm7PPH9LE/Ab2M4cFznd5QVBAF/Pyx3yDJltkQil0ijhg2XGN7QD8kATvI8vwrAbABlAP7KGPvKsn8ygAOMMdt34gEAd3TR3gMAdjPGjnR1Qp7nnwbwdDcxfc0YW9bVzoKCgriEhISnx40b9059ff1bO3fufCUtLW3L2rVrcwHg7Nmz3gEBAefnzp2rFgRBnpKSct/u3bufTExMfEGpVP6tubn5mZ07d764YMGCwo0bN34MAPX19RKNRjN6zpw5I/R6fdz+/fvXSSSSHwGkdRMnKisrV5WUlMyOj4+/WiKR5FZUVGTt2bNnZXp6esjevXvTExMT/33ixInf1tTU9DitNTk5+alff/31qZkzZz6iUCg+bGtru2PXrl0fz58///jmzZv/ZY1Tp9MFJiUleev1+vjc3NyNXl5e7wB4KD09ffTu3bv/HRsbu3rq1Kk3aLXaa/bu3ftVx8SzK/X19RKtVhuUlJTkJwiCaMmSJTE7d+78YfLkyV/HxMTcbzQaxx0+fHi7SCTKBpASFBTE//rrry9lZGS4Z2dntwFAWVnZ9WFhYduysrKE+fPnX9L+rFmzvigoKLhp6tSpt8nl8h+ampr+tGvXrr8sWrRo77p16zaPHTu2oqGh4WYAHzU2Nt7n6+urv3DhwlQAWLx48ZTq6mqpUqn8YNGiRakHDhy4ft68edesX7/+5/T09FCtVrugN9fo6mjJIEIco1kPeHb4lPeUAU26zj9+m3Xm/bZ1AaBJB/jadLeUNgrYVibgP4vEXZ47txw439p5QnwluUlonV5nMmSTXp7nPwVwZzdVXmOMPdfP5kcAiAdwE4DrYU6Cs3ieP8MYywWgAtDQ4Zh6AJeN1+V5XgHgdgCPdndCxthyAMu7q9OdsLCwki1btvzV8vShkJCQ22tqap4HsAgAPD09Db/88svVlv0tEyZMeCIyMvL01q1b/2zZ9tK4cePurqioeBLAx4D5L9TRo0fPy87OrgOwZdasWV8XFhbe2lMsJSUl86Ojo79dt27dGgDIzMxMKSoq0jY2Nj4N4JG+XNfZs2efGD9+/BbLWFgA+HTSpEmPV1ZWPgzgXwAgkUgwatSo5KysLD2AzdHR0eVNTU0JANDQ0PCMVCo1BQQEZGZlZQkAvr3qqquWlZSUzOrN+cViMUaPHj03Ozu7CQBmzJjxqZ+fX/2uXbtut1TZP2fOnPfz8vIeBQCVSvUXsVj8YlNT04sAnl60aFFyWVmZ5/jx45/trP3i4uIbxo8f/8mGDRv+a9n0SmRk5LLa2tpnAGweMWLEjvLy8kUAUFNTMzc6OvqrgwcP3pmenj66qanpwcDAwNbVq1cXpaWljQIAjUaTnJ6e/ktOTs4ZAP/p6fqampqgUqlcumw7nncoxENlKrtKed63BmzrYkXAmYFGvL9QikbdpcdWNWqhkko7bVMpdUej9uL2Rp25LZUlcbZu/zxPQEKgEWFebl3GtqPMBIPg+KRX09YGQA7A8T8vKl8sd2XIJr0wj7F9spv9A7n3aBPMPbP/szzfwPP8WgBXA8i17B/T4RgvAJ19j3ozABOA7wYQT48UCsUlHz0eHh41Go1mlPW5Wq1ust3f2to6wtvb+3iHNkobGhqibdow5uTk1Fify2SyvMbGxh7/bG1sbJTIZLL2Xu2srCxjSEhIi1arDe/bVQFNTU2eZ8+enW+74oHJZMKoUaPalxr38PDQWxJeAIBYLNYaDAYFAOh0ujEqlarFkvBar+MUzMNNeqRUKg3WhBcA2traxpSXl3vbxiMIQvuyNFlZWfopU6bsLS8vvwvA09XV1S+HhobWrF27dm9n7dfX18t+/fVXplAo2ofGGI1GuLu7VwOAWq3+ZN++fTemp6ePLC8vHxUeHv6PkSNHpjc3Ny+rqalZEBAQcBQA1q5du23OnDl/O3v27O/Onz//WEhISGNERMRftmzZ0u0fUrYfAK5arq+vh6+v75CJh8pUdpXy1pu7SxHc0KITYDABFQYlrEefaJQidkTnbcb6GnG4SsCN48zbD1cJCFAAvu7cJfU/zzfh6emSS47tWL49RoSXdzl+uTC5XN5edvTPi8oXy10ZskkvY6wZQPMgNX8IQGcrPFgTncMAlnbYN8WyvaMHAHzGGNN0d0Ke558F0GlvoMWXjLEHutrZ2tp6yXD+lpYW34CAgJPW5xzHXfJ9kkKhqO54TGtr62hrsmVpQ5yenu5rTXy1Wm2sWq3ucYSUWq3Wa7XaCdbnmZmZ4sbGRo/Q0NBTPR3bkVKpbAwNDd27d+/ejL4eCwBSqfR0U1NTamZmJmdNfLVabVhvj+/4usnl8tKQkJCA4uJi/66O8ff3f/no0aNrFi9ePPnUqVOzJkyY0OVqDZ6enrpx48b9a/v27X/obP+6devWent7GysrK7/w8PDQrVmz5uCMGTN21dXVZZw7dy506tSpK6x1t2/f/hSApzIyMjyqq6s/27lz51/S09M/sv3DZTjy8PBwdAiEDEseUg7XjeXwwk4T/p0qwqEqYFWRgF23dj4s4bexHO5aY8Jt4wWMVAKv7hZw14RLe2t3lQsobwJuiO6+FzdEzSEpQIMt593hyDUijUajA89O+mrIJr19wfO8G8zXIgHA8TwvB4BuEtEVAHbwPH8tgJ8BzAWQCsA6fOBHAH/jef7/ALwL87jf6wAs7HDeKTAPk/htTzEyxl4H8HqfLsxGSUlJWHJy8pNKpfLd+vr6t86fP6+OiYnpsr2AgIC3d+zYsSI5OflZpVL5RlNT09PFxcWhc+fO/Z21jslkwtmzZzdlZGQkGwyGSSdPnrwlLCxsa0+xhIWFbSksLLx10aJFX0skkp0VFRWrTCYTp1ar+zx8IyQk5K0DBw68ZlnflhcEQarVaq8VBEG0cePGL3o63tPT8686ne6BCxcurMrIyLhJq9VmFBcXJ/Z3coGvr+/Tx48fP5SYmPixt7f34xzHNev1+iSNRpO0devW1wBzojp69OimwsLCDTqdTuTp6dnlMJvIyMjvCgoKHlq4cOFeuVz+vSAIXm1tbTe7ubmdtg4PCQoKKsnLy0uJioo6CABqtfqLI0eO/M9gMMDDw4MHgEWLFqVptdqrPDw8PhSJRLVubm61AMBx3LBfJFIqpTsiEeIo7y8Q4Z51Jvi/b4SvO/DBQlH7cmWljQJiPjEi/24xQtQc0sJE+ON0IPl7I9oMwPVjObw869LP6s/yTLguioNK2vPQhedmirEnC2h14KcgJb3OxSWSXgDPAXjR5rl1DREOaO9lvY0xFgsAjLE9PM/fCnOS+xWAEgB3MsZ2W/bX8zy/BOb1Vl+BeZ3eB6z7bdwPYCtjbNCX5oqOjj5UWlr6x7Kysjc8PT11iYmJL69du3ZbV/U3bdrEz5kzZ3x+fv4LDQ0Nr3l6emoTEhL+vnHjxv9Y63h5eenlcvm57du3V5tMJi4iIuJwQEDANT3FEhAQcK3JZFqzb9++1RqNRuTn59cwY8aMG3Nycnq7eka7LVu2LJ83b568oKBgeW1t7T85joOvr29jZGTkS705Picn50xKSsr9x48ff/fQoUOtAQEBjREREbtKS0tn9jUWAFizZs3R1NTUa0+dOvXR0aNH79Tr9SJPT09NWFjYKtt6o0eP/mn37t13TJw48ajt8IiOdu3a9dukpCTNsWPHPmloaPhWJBIJ/v7+tZGRke3DHXx8fNbl5+dHent7/wQAcrn8B4lEYgoICGiwjLeGIAiKoqKip2tqal4FAG9v79ZZs2a9kJ2d3XHs+bBTV1cHHx8fR4dByLDk487hp2s779kNUXOXrL0LAE9ME+GJaV13SqxI7Xry2mXt4zzGeIYi34HfddE6vc6FEwS6edRQFxoaWu/v779v3759C3uu3Tt9WWHB2UydOnVXXV1dVElJyQhHxzIEuPwbvLm5GUql0tFhEEKusJqaGmy44IX71gsOW7psyagW5Nzi6ZiTk650+TWBq/T0kmEsOTn5KblcvtbNze1Ya2vrAwUFBTPj4uI+cXRc5Mqora2lpJeQYchgMOD6KBEe3mR0WNIrFve+Z5o4Hq2qTC6TlJT0nkwmEzp72N44ojdSUlIe6qqt+Pj4dfaIt7W1ddbOnTsPrFu3znDo0KG3Y2Nj1/n4+LC0tLSZXZ174sSJefY4N3E8vd7Bq9MTQhyiubkZEjGHJ6ZxcHdQFx4lvc6FhjcQ4tpc/g2u1Wohk8l6rkgIcSnW935tm4BRHxqhccCcMhreMCR1ObyBenoJIU6trKyL1fMJIS7N+t73cedwyzgOYgfcq0JKf3A7FUp6CSFOjcbzEjI8SWxuAfzUDBEkDshoDDS8yqlQ0ksIcWpubjQfl5DhyHapwmgfDtNHXvkYrHfrJM6Bkl5CiFOrr693dAiEEAeorKy85PnzCSJ4SLqoPEhonV7nQl0khBCnFhgY6OgQCCEO4O3tfcnzlBAO432AI1VXLoZRbg0A/K7cCcmA0OoNhLg2l3+DnzlzBqGhoY4OgxAyDFVUVNAf3kMPrd5ACHFNNKaOEOIoUqnL3dTUpVHSSwhxatTLQghxlLq6OkeHQPqAkl5CiFM7d+6co0MghAxTAQEBjg6B9AElvYQQp6ZWqx0dAiFkmKqtrXV0CKQPaPUGQgghhJB+0Hdyc4qd5QI2nbHfHOIbozmM83XA7eZcECW9hBCn1tjYCD8/WjKIEHLlBQcHX/LcYBJw7U9GVLfZ7xynarT4NNPDfg0OYzS8gRDi1IKCghwdAiFkmCorK7vk+U8nBWgM9j2Hm9sVvuOGC6OklxDi1CoqKhwdAiFkmFIqlZc8f2W3Cc2Xj3gYkJaWFvs2OIxR0ksIcWoiEX2MEUIcw83t4ijRvecFFNfb/xz0GWc/9EoSQpzaiBEjHB0CIWSYqq+vby+//osJbXYe2gAAMpnM/o0OU5T0EkKcGg1vIIQ4ivXmOOVNAtaVCINy3/e2NjvOihvmKOklhDg1Ly8vR4dACBmmqqurAQBv7zdhsG6ILpPRrY7thZJeQohTMxgG4ftEQgjpBZPJhFa9gBWHBeiMg3QO42Cl08MPJb1kSIuPj18XGRk5KN9fz58//zGxWNz+bVTHcy1ZsiQ8PDy82t3dXQgKCqLps0NUc3Ozo0MghDhQbZuApT8Z4fGOAaErDPj6ePdJ4tu/mhD4vgGe/zDgnrVGaA0XByXM+9YA+dsGKN81P6L/0/0f1YGBgfgszzQowxqs9PSHvd3QzSmcAM/zCgD/ArAUAAdgJYCHGWOdDvThef5DALd32OwB4A+MsbcsdX4L4CkAIQBqAXwC4GXGmNChrZEA8gHUMMYi7XZRvbRv375Fts9DQ0Pr/f399+3bt2/hYJ+rqqrqA71eL01JSZFnZ2dr7X0+Yh8dF4cnhAwvD20yQSoGKpeJcegCkP6DEZP9OMSOuPwuZutKTFi+14TNN4oRpASW/mTEi7tMWD5H3F7nXyki/G5S7/oEy8rP4fU9oWix8zJltjw86MYU9kI9vc7hXQDjLI8oAOMBvNVVZcbYA4wxpfUBc7JsAPAtAPA8PxnAxwCeAaAGsAjA/QB+10lzKwDst9+lOI/W1tZQtVp9jhLeoa3j4vCEkOGjRSdgZaGAPyeKoJRySArmcHUkhy/yO+/t/SxPwL0TzQmxt5zD8wkifHqs//20B5p9UD/IvyFonV77oaR3iON53h3mXtvnGWOVjLELAJ4HcCfP8/JeNnM/gCzG2DnL8wgAFxhjPzPGBMbYCQBbAUzucO47YP424Es7XEqXMjIy3OPj49f5+fnp5HK54Ovrq58zZ84bADBt2rStYWFhNQAwadKkI2fPnvU8dOjQAplMJvj5+elSU1PT3dzchCVLlsRY28vMzOR8fHwMSUlJH/YlDttzRUZGVpw4cSL6xIkT0TKZTLjqqqu2AkBqauq14eHh1Uql0uTt7W2YOnXqroyMDPee2o6JiSkaP358yfjx409ZrzExMfHftnXmz5//+5CQkAYPDw+TUqk0xcbGngSA9PR036ioqHK1Wm2Uy+VCUFBQa3Jy8lN9uTZXJpHQ3YoIGa4K6wCxCIjyudirO9mPQ1515/XzagRM9ru0bmUrUNN2MfF9ZocJI94zIPFrA7aWdj9U4u+H3e1+M4qOxGJxz5VIr9DwhqEvGoAcl/a2HgDgDnOv75HuDuZ5PhDA1QAybDavA3CO5/mlAFYBiAEwB8DdHY57FcBsAPN7CpLn+acBPN1Nla8ZY8s621FeXr6lvLx8Slxc3C1yufwHg8FwlcFgCO1Y78iRI5M6G94QEhLSUF1d/TfrNTY3Nz+l0WhEXl5ez/QUd1eKiooCY2JiijiOM+Tl5Y0DgCVLlsTs3Lnzh8mTJ38dExNzv9FoHHf48OHtIpEoG0BKT20WFhaOSUhI+CAiIuLxlpaWx3bu3Lk8JSVl76ZNm/iFCxdev2PHjnenT5/+n0mTJj0OwK21tfUuy6Fu/v7+q8aOHftnjuPqL1y48N/du3cvX7JkyarVq1ef6O81ugofHx9Hh0AIcZBmPeDZYXEDTxnQpOu897ZZZ95vWxcAmnSArzvw17lixPgCUhHw7QkBmT+acOhODhFelw+VOF4j4EjN4PcdSqT0h729UNLrIDzPfwrgzm6qvMYYew6AyvK8wWaftazuxanuBXAWwAbrBsZYC8/znwD4DIACgBjAXxlj622O+xDAG4yxUp7nezwJY2w5gOW9iOcSmZmZXH5+/oyEhISXN2zYsNKy+VfLo1dCQkK+LigouNf6/Ny5c8siIyOPZmdn1/U1nu7U1NT81c/Pr37Xrl3W8dL758yZ835eXt6jvTl+1KhRjbm5udbE/6/jx4+/v7Ky8nEAfEVFxcvh4eFlO3futB1i8jYA5OTkVAKw/YMhQ6lUmlpbW28E8Ep352xqaoJKpXLp8tmzZzF+/PghEw+VqUxl+5XnfWvAti5GMM0MNOL9hVI06i49tqpRC5VU2mmbSqk7GrUXtzfqzG2pLIlzjLIZKqm5/nWhzfhmlAKrTwm4a2zzZbFVtQJiEYdBW6vMQtOmAeDu8J+FM5W7Qkmv4zwM4Mlu9rda/m2y/OsJoN6mDACN3Z2A53kRgPsAfGA7QY3n+bsBvARgIYB9ME9m+5bn+VcYYy/wPH8rAD8A7/f2YvrLaDSO1+l0kMlkuf1tw8vL65nW1tYHUlJSHpTJZBtOnTo1et68eY/bM04AaGtrG1NeXu6tUCjaX0tBEGAy9e4Tz8PDo8b2ubu7e3lDQ0M0ALS2tvp7enoWdnZcRkaGd2lp6daysrKY1tZWN5FIBI1GA4PB0OMMLtsPAFctBwQEODwGKlOZyoNT3npzd2mKG1p0AgwmoMKgbO8hOtEoReyIztuM9TXicJWAG8eZtx+uEhCgAHzduU7rczBC6CK22cGAj9SIVsPgplJyuazT2KjcdbkrlPQ6CGOsGUBv1loqAKABMBXAZsu2KQDaAHSaJNlIAzAS5klrtq4CsJkx9ovl+Wme578CcA+AFwCkwjy+94Kll1cGQMHzfDWAFMbY4Y4n4nn+WQDPdhPLl4yxBzpuFIvFx6VSKbRa7SwAG3u4HnAcd1mGmZ2d3RAXF3ewoqLiCZVKlRoQENBi02tsN3K5vDQkJCSguLjYvz/Ht7S0+No+b2trG+Xu7l4DAAqF4kJLS8tlQzoAoLKy8rsLFy5ETZ8+fb5EIsnNysoSlEqlSRAEGpMPQKfTOToEQoiDeEg5XDeWwws7Tfh3qgiHqoBVRQJ23dr5ONjfxnK4a40Jt40XMFIJvLpbwF0TzAlvvUbAL+cFzB3NwU0EfHdCwPYyAe/M7/yjluM4PDy+CX8+4j2oqzfQWuT2Q0nvEMcYa+N5/ksAr/A8f8yy+RUAnzPGND0cfj+AHxhjVR227wTwLs/zVzHG9vM8PxrmyXIHLPsfB/CcTf0bAPwe5vG9lV3E+TqA13t7XVZZWVlCXFzcvuPHjz+bmpp6RCaTrTIYDFONRmPo+vXrf+hYXy6XN7a0tIR13B4QEPDc1q1bV6vV6tCxY8d+3dc4esPX1/fp48ePH0pMTPzY29v7cY7jmvV6fZJGo0naunXraz0dX15erp49e/Y/vby8nmhpaXmkuLg4bPbs2csAIDAw8OWtW7d+n5SU9KGXl9cfAEhbW1vv3Lx58zsGg0EtFotNYrH4lCAIymnTpv3U1tZ2+QCzYYpmNhMyvL2/QIR71png/74Rvu7ABwtF7cuVlTYKiPnEiPy7xQhRc0gLE+GP04Hk741oMwDXj+Xw8ixzUqs3Ac/lmnCi1jw5bpwP8NO1IkT7dP1x+7upCrx2dHCvz2AwDu4JhhFKep3DozCv02vt2V0Jc2IKoL2X9TbGWKzNtlEA0tHJBCvG2Dc8zwfDPKQhEOYe59UAnrDsrwNQZ9NWHQAjY2xQ1oYKDg5OdnNz+3n//v3fNzc3S5RKpT42NvZtAJclvcHBwS8ePXp0hUKhEFQqlbayslIOAOvWrVsTGhraUFlZ6ent7T0oKxusWbPmaGpq6rWnTp366OjRo3fq9XqRp6enJiwsbFVvjo+KijpdXV2dvm/fvocVCoUhPj7+k02bNn0AABs2bPjv/Pnz/3Dy5MmXDhw4cL9IJBJCQ0NPAngnMDDw3vr6+u0bN24sc3d3N0ZHR6/x8vIa5PnCzoPW6SVkePNx5/DTtZ337IaoOTQ/emmq88Q0EZ6YdnnvrZ+Cw747+pYW1VdX4KG4MXh7vwDtIOWmtE6v/XCCMJj3ESHkyomJiSkSiUT6Y8eOjXd0LB11XAniCnL5N3hxcTEiIiIcHQYhZBiqqamBXu6DMN4IzSAlvTePacI3v/EenMZdU5dd8zQmkLiERYsWpRYXF0cEBQX1e5ky4pxkMlnPlQghZBAYDAYEenDIiOAgGqRBZ25utE6vvVDSS5xeVFRU2bZt29ZNmjRp4/r163+ybk9LS5spk8mEzh4TJ07Ms2cMSUlJ73V1rqSkpPfseS5yKbW6Nyv3EUKI/TU3m+ejPztDBPkg5aZubjQS1V5oeAMhrs3l3+A0vIEQ4iharbb926arPjfgwAX7n4OGN/QZDW8ghLgmX1/fnisRQsggKCu7OL/7+QQRlINw8zS5u9z+jQ5TlPQSQpxaa2trz5UIIWQQSCQXs9zMCA4eg5D06nW0WI+9UNJLCHFqbW1tjg6BEDJM+fj4tJfFIg5/nM5BYechuEYjrdNrL5T0EkKcGq3TSwhxlMrKS+/XdN8k+6dVtE6v/dCUQEKIUysrK6OJbIQQh/D2vnSCmUrKYfkcDp/n2W8O8XRVFYBRdmtvOKPVGwhxbS7/Bj937hyCgoIcHQYhhAyKiooKBAYGOjoMZ0KrNxBCXJNCoXB0CIQQMmikUqmjQ3AZlPQSQpxaTU2No0MghJBBU1dX5+gQXAYlvYQQp+bn5+foEAghZNAEBAQ4OgSXQUkvIcSpNTY2OjoEQggZNLW1tY4OwWXQ6g2EEKem1WodHQIhhAwavf7ym1O8uc+IlYV9n6cc48vhP2lie4TllGj1BkJcm8u/wbVaLWQymaPDIISQQdHxM66mTUDwh0Zo+nHPCjEnwPCHQbht3NBCqzcQQlxTWVmZo0MghJBB0/Ez7r2Dpn631WU2OExQ0ksIcWp0tyJCiCtTKpXtZZ1RwNv7hX718gLD4Ku/HlDSSwhxarSGJSHElbm5XZx+9X2BCYb+d/QOe5T0EkKcGq1hSQhxZfX19QAAQRDw6m4BzZfPayO9REkvIcSp0RqWhBBXZr0F8c5yoKzZwcE4OUp6CSFOjdawJIS4surqagDAq3uMaKFe3gGhpJcQ4tQ6W8OSEEJchclkQkm9gG1nHR2J86OklxDi1IKDgx0dAiHExdW2CVj6kxEe7xgQusKAr493P5vs7V9NCHzfAM9/GHDPWiO0hsvXTThZJ0D+tgG353S/FENgYCD+/qsJxuG+9IId0B3ZiN3xPK8A8C8AS2FeFnAlgIcZY229OPavAP4I4A7G2JeWbWMAlABoxcUVV+oZY8E2x00C8DaAqQA0AD4C8CJjzOU+JjiOE+bPn79s06ZNHzg6lqGgrKwMERERjg6DEOLCHtpkglQMVC4T49AFIP0HIyb7cYgdcfnKt+tKTFi+14TNN4oRpASW/mTEi7tMWD7n0juhPbTRhPjAns9deOY8PjkWAr0dVm0Y7uv0UtJLBsO7AMZZHgKAnwC8BeDB7g7ieX46gMUAzndRJZoxdtmdCHie9wSwFsA7ABYBGAtgHYBGAG/25wKI87Bdw5IQQuytRSdgZaGAY3eJoZRySAoGro7k8EX+5YksAHyWJ+DeiRcT4ucTRLgtx4Tlcy7W+faECV4yYFYQh6L67s//c8UIu12Ly/UC9RENbyB2xfO8O4DbATzPGKtkjF0A8DyAO3mel3dznAzAfwDcD0DXx9MmApADeIMxZmCMHbe0taw/19CTjIwM9/j4+HV+fn46uVwu+Pr66ufMmfOGZZ9s2rRpm3x8fPQeHh6mMWPG1Kampl5rPTYmJqYoNjb2hG17Pj4+hqSkpPcBIDEx8d++vr66mTNn/s/Ly8ugUChMEyZMyM/MzJQAQGBgYBsA7Nix432ZTCZ0bGs4sl3DkhBC7K2wDhCLgCifi/2kk/045FV3Xj+vRsBkv0vrVraabx8MAI1aAS/sNOHv83pOwYwmAf86JkerYWDXQMzotwWxt2iYE9D9NtsOAHAHEAXgSBfHvQRgM2NsN8/zXbX9C8/zUgB5AF5ijG21bBfh8m9tRADCeJ5XM8YaOzbE8/zTAJ7u5jq+Zox1mjSXl5dvKS8vnxIXF3eLXC7/wWAwXGUwGEIBoLKyclVJScns+Pj4qyUSSW5FRUXWnj17Vqanp4fk5OSUd3O+dvX19RKdTheYlJTkrdfr43Nzczd6eXm9A+ChiooKd47jhNmzZ9PwBov6+nr4+vo6OgxCiItq1gOeHe6B4ykDmnSd95s268z7besCQJMO8HUHnt9pwr0TRBit7nmwwfrTtC6vPVHSS3qF5/lPAdzZTZXXGGPPAVBZnjfY7LOW1V20PQ3ADQDiumi7GkACzMmzBMA9ANbwPD+DMXYEwC4AJgDP8Dz/JszJ9T0257ws6WWMLQewvJvr6VRmZiaXn58/IyEh4eUNGzastGz+1fJASUnJ/Ojo6G/XrVu3xlI/paioSNvY2Pg0gEd6cw6JRIJRo0YlZ2Vl6QFsjo6OLm9qakroa6wA0NTUBJVK5dJl679DJR4qU5nKzlWe/ZUWuecvH6YAAImjgH/OF6PRJsFtampCo9YDKinXaZtKKVDZoAGgAACU1zYDcIdKCuwsacbGM3Ic/K0ITU1N7XW6ik1magU4mwzaDobCaz7Y5a5Q0kt662EAT3azv9Xyb5PlX08A9TZloJPk09Jz+wmAhxhjnS67bdm+x/JUB+CfPM9fDXOifIQxVsvzfDqAvwH4A4AzAD4G8BwAu96uy2g0jtfpdJDJZLmd7W9sbJTIZLL23uysrCxjSEhIi1arDe/tOTw8PPSWhBcAIBaLtQaDQdGfeG0/AFy13NbW1qf6VKYylalsW95xW/dJZYtOgMHE4WSdgLHeHFQqFQ5XGRE7ovM2Y305FDZdbPNUmwcCFCb4unPYV6vA6QYTQlYYAbijWS/AKAD5Ne448NvLY5sfqcQo91acbLLf7daHwms+2OWuUNJLesWSePbmXjAFMK+eMBXAZsu2KQDaABR2Uj8IQCyAr2yGNXgD+IDn+cWMsdu6OI8JNkMaGGN7ALRPE+B5/g0A+xhjLZ0dzPP8swCe7eY6vmSMPdBxo1gsPi6VSqHVamcB2Nhxv1qt1mu12gnW55mZmeLGxkaP0NDQUwDg5ubWqtPprH8EICMjQ9bc3Nx5F0MXOG64z7+9lMlEN6InhAweDymH68ZyeGGnCf9OFeFQFbCqSMCuWzv/6P5tLIe71phw23gBI5XAq7sF3DXB/LnNJnG4edzF497cZ8LpBuCDhV2P7/19TDOeOeBDwxzsgJJeYleMsTae578E8ArP88csm18B8DljTNPJIWcBhHTYthvmXtuvAYDn+ZkwJ9wnYP4/+1sAc2GTtPI8PxVAPsyTU68GwABc102crwN4va/Xl5WVJcTFxe07fvz4s6mpqUdkMtkqg8Ew1Wg0hq5fv/6HsLCwLYWFhbcuWrToa4lEsrOiomKVyWTi1Gr1cgBQqVR7jxw5ck9aWtpsNze3Q+Xl5RuMRmOfYlCpVCaNRjOtr7G7KustOgkhZLC8v0CEe9aZ4P++Eb7u5iTVujpDaaOAmE+MyL9bjBA1h7QwEf44HUj+3og2A3D9WA4vzzIntQoJB4XkYrtKCSB3A/wUXXdm3BHngZcOg5JeO6CklwyGR2Fep9fas7sSwOPWnZZe1tsYY7GMMSOAS5Yh43neCKCOMVZj2RQG4M8ARsLci5wPIJMxtt/msAdgHu4gBXAMwA2MsU32vjAACA4OTnZzc/t5//793zc3N0uUSqU+Njb2bQA/BAQEXGsymdbs27dvtUajEfn5+TXMmDHjRuskNm9v70dCQkKStm/fvk0qlZrGjx+/0tPTs08J7IQJEz7Ny8u7U6FQ3B0eHn7i2LFjMYNxnc7i3LlztE4vIWRQ+bhz+Onaznt2Q9Qcmh+9NJ16YpoIT0zreXWGlxJ7/qKv+kIFnpg2Bq/uEdA2wFUchvv3hJwgDPdV2whxaS7/Bq+qqoKfn5+jwyCEkEFRU1MDkYcPRn1oHHDSK+YEGP4g6bmic+syt6d1egkhhBBChiiDwQBvOYdbxnEQD/eu2gGipJcQ4tQaGy9bFIQQQlxGc7N5DvlT00WQUNY2IPTyEUKcWlBQkKNDIISQQRMcHAzAfEe4GSMdHIyTo6SXEOLUKioqHB0CIYQMmrKyi3O9n0sQQenyQ3IHDyW9hBCnJhLRxxghxHVJJBez3JQQDiPcHRiMk6PfFoQQpzZixAhHh0AIIYPGx8envcxxHP40k4MH9fb2CyW9hBCnRsMbCCGurLKy8pLnt8eIIOrnKg7DffEHujkFIcSpeXl5OToEQggZNN7e3pc8l7txWHm1CNvL+r4Mu1pXDWD43sWSbk5BiGtz+Td4ZWUlAgICHB0GIYQMeRUVFcPh1u10cwpCiGuyrmFJCCGke1Kp1NEhOBQlvYQQp2Zdw5IQQkj36urqHB2CQ1HSSwhxarZrWBJCCOnacB8KRkkvIcSp2a5hSQghpGu1tbWODsGhaPUGQohTs13DkhBCXE7hOWD6HwGtoffHSN2AjS8C8WMv2azX6+0cnHOhnl5CiFPruIYlIYS4lLIamEwmQKPr/aOxFXjx28uaGu5zICjpJYQ4tY5rWBJCiKvh+nO79S3HgDMXLtk03OdAUNJLCHFqOp3O0SEQQsigMplMfT/IKAB///mSTUql0k4ROSdKegkhTq2lpcXRIRBCyNCjNwD/2QQ0t7VvcnMb3lO5KOklhDi14T5GjRDi+kTcANK1jze1F+vr6wcejBOjpJcQ4tSG+xg1QojrMwn9GN4AAK1aYPkPgGV4xDC4BXG3KOklhDg1mUzm6BAIIWRQcRzX/4ObNEDOfgBAdXW1nSJyTpT0EkKcmlqtdnQIhBAydDVrgFe+B9DPCXG2apuApcsBj1uAUAZ8vb37+m9nAYH3AJ63A/f8C9B2sk7wyXOA/Cbg9ncGFlsvUNJLHGbSpEmvjBgxQrgS5xoxYkTT4sWLv+xtfY7jhOuvv/6VwYyJ2EdVVZWjQyCEkEElCAP8VZl/Fjhc0vvhDS99a3509NBH5htfVH4MfPU48CAP5JV23sa6g+ahFZteAk5/CJyq7HTtYDz0ERAf2etLGYghO42P5/mbATwEYDIABWOs21h5np8N4F0AYwCIARQDeJUx9oNNnSUA/gwgEkALgJUA/o8xpulw3mcBRABoAvBPxthr9rsy4gjV1dUqR8dABoevr6+jQyCEkEEl6s86vbY0BuAvP+Dca9ciIiKif220aICVe4Bj7wBKdyBpPHB1PPDFNmD5HZfX/2wrcG8KEBtifv78DcBt71xa99tcwMsDmBUNFFX0L64+GMo9vXUA3gfwWC/rFwBYCsAXgJfluC95nh8PADzP+wP4AcB/AHgDmA5gHoDnrQ3wPH8HgLcBPAHAE8BYAJcuckfIEJKRkeHu6BgcrbW11dEhEELIoBrwsASTCVi1F56anqt2qfAcIBYBUUEXt00OBfLOdl4/rxSYPMam7higsh6oaTI/b2wFXvgG+PtdAwiqb4Zs0ssYW8cY+wbAqV7Wv8AYO8MYEwBwAEwwX5+1zzwYgAzAfxhjJsZYGYBsmHuSwfO8CMByAC8zxjYyxgyMsSbG2FH7XplZUFCQZPLkyT+OGDFCK5fLBV9fX0NaWtpHAMBxnNukSZOyfHx8dB4eHqbw8PCG66677rfWYyMjIw9NmjTpfHR09BF3d3eTl5eXMTU19Yv4+PjFI0eObJTL5UJYWFjTrbfeOsd6jKenpyYhIWF3cHBwrUwmE0aNGtV27bXX/t66vzfnnDJlSrntNXh6emoWLVr0XwCYNGnScn9/fyEuLu4TT09Pg0KhEOLi4oqff/55T2v96dOnXz9y5MgmmUwmhIaGNotEomm9ea160/aMGTMmRUZGlqjVaqNarTZOnjy5+K677grvLFYAiI+Pf9zPz69NJpMJUVFRtbGxsbsiIiIu+TioqqqabH09R48e3XrDDTfc3Jt4Z8yYcXdQUFCDu7u74O/vr0tOTl7F87wYAAICAmqTk5O32NaPiYlZFRkZ2crzPGeJ7ZmRI0c2u7u7m/z9/XULFy78yrrP+lpMnjz5G09PT8P+/fubexOTK2tra+u5EiGEOLEBTGO7hPt/Nvf/4GYN4Km4dJunB9DUxWdwx/rWsrX+89+Ye4JHj+h/TH00ZIc39BfP8/UAPGC+tu0A1lt2HQKwBsD9PM+/D2AUgKth7tkFgCgAQQCUPM+fAOADYC+AxxhjRV2c61aYe6O7kssYy+hsh6enZ3ZFRcX8pKSk3/n7+3+Rn58f6+bmNhEAYmJivjh//vyS5OTkW8rKyrKbm5u/2bRp06cPPPDA/g8//DAPAE6cOBG4cOHC7+Pj46ceOXLk7e3btz8cEhKSNmfOnKtPnjx54MKFC4UnTpz4GuZkHwCQn58/Izk5+eGamppP6urqPtm0adO7Dz744OYPPvjgWG/O2ZOamhqEh4dPWbp0qf/x48fHHj16dNe+ffs+AnBjaGioT1VV1fdXXXXV3pCQkEVFRUUL8vPzv+/tzPvu2vb29pYD+GXcuHHHExMTZ544cUJcUVHxy+HDh3Nh/pleIiYmZnJhYeFbKSkpX6vV6nvPnDlz55EjRz4YNWrUJbf2KikpWTR79uz0EydO7Gpubj586NChjwF0MiDpogkTJkwqKCj4eN68eT8GBwfffvz48at/+eWXr9zd3T9ljN3h7+//TWFh4f08z0sYY3qO4zhPT8/UGTNm5DDGhLi4uN+dPHny9eTk5Ge9vLzeKC4uXpqbm/utSqU6zRj7k/W1CAkJmXLttdeOqq6uNvTqBXRhtE4vIcTlDWT1BiuNHvJPtwNv3df5/ozXgNzj7XUBAO9km/9NGg/8+RZz76ytxlZA1cUXjko50GiTEFvLKnfgUAmw8Qhw8M3+XUt/CYIwpB8rVqyYt2LFCkMfj5GtWLHi2hUrVjy1YsUKsc32G1esWFG5YsUKw4oVK4QVK1Z8ad2/YsWKJMu2oytWrAhbsWKFYsWKFe+vWLHi+IoVK9zseU0AOKlUasrIyHi/s/3e3t4tKSkpOTb1RSqVSrto0aJvBEFARETEoejo6Cab/QoAwuLFi7+wbpswYcJ7o0aNan/d1Gq1JiEh4YhtDGq1WrNo0aLvenvOuLi4cts41Wq1JjU19b+CIGDixInL5XK58NprrwVY948ZM6YgPj6+yLL/ZW9vb+GNN95QWPePGzcux9fXV+jp9eqp7QkTJvxxxIgRphUrVsis+yMiIua7ubkJf/vb30I6xhoTE/PfsLAwzYoVKzhr/fDw8CPh4eEam+vv+Ho+qFAohBUrVnh2F2tMTMx3Y8aM0dq2PW7cuDVjx45tEQQBfn5+vm5ubsJNN930hCAIiI2NvU0ulwuPPfbYWEEQEBISUpKYmHjAts2oqKgd1td+4sSJyyUSifCnP/0psjf/1xobGwUrVy3n5+c7PAYqU5nKVB608qYjgkF1iyBg6cAe3HWCIf3PvTqv5unPBOHFby7d3twmmCS/EYTC8ovb73hHEJ76vPN2bnlL0Dz5cfv2lqw9ghBwtyAIgtD2l/8KguJmQQi4WzD63yUIHrcIgvwmwTD5MXu8bl3+ThyywxsGgjGmZYz9BGAugN8BAM/zyQA+A3A3zMMcAgGoAXxiOcwyyATvMsZKGGOtME9oGwdzL7A9+el0Os7Hx2dvZzubmprclUrlcetzQRBMSqWyoa2tLdS6zd3dvdFmf6tlW4l1G8dxTTqdTmzbroeHxxmbYwQPD4/Gtra20b09Z09UKhWeffbZSutzsVjcptfr3QHAZDJFenl56Z588sn2PxPd3NxO2KNtQRAm1tbWco899phGoVAICoVCOHfu3CYAKCwsjOvYll6vD1apVA2WoTAAAJlMdtn0U9vX02g01mk0GgDodkKcXq8PUavV9bZtu7m55Tc1NbkDwIULF2pGjx59vLi4+GEAaGtreyomJqbs7bffPgkALS0tI/bt2zfFeh0KhUI4c+ZMUmtra/u6XJ6enggJCSnu8UWD+XVz9bKnp2ef6lOZylSmsrOVB7ROr5W7FBW/m92r88pk0su3e8jBXTcTeOFboEUD1ZEyYNU+4I65nbfz27mQfbHDvHJEXTMUb+UAdyUDAOS/zwSK3wcO/R2iw28BD6QC6VMh3vBSj7H1ptwVlxve0IEbzJPRAOAqAEcYY6stzyt5nv8IwOeW5wUA2gB0ti5Ip2uF8Dx/G4AV3Zx/B2NscSfbq6RSqVBXVxcP4NOOO1UqVVtLS8s463OO40QqlcrT3d39TMe6fdHS0tKewHIcx6nVarW7u/vZ3pxTLBY3GwyGYJv9bhKJRIpeEolExfX19dI333xTYU18DQbDuJ6O62Xb+f7+/qaXX35ZwhjrcbS/RCIpa2pqmsLzPGdNTrVa7Wh7xCKRSEobGxsvadtgMIxXqVTt3/EoFIq/Hz9+/N/XXXdd7NmzZydMnTr1BZt9tbGxsUe2bduW2NU5OI6DbVI93CkUip4rEUKIU7PDR354AAxTxgysjfcZcM97gP/dgK8K+IBdXJ2htAqIeRTIfxcI8QPSpgJ/vBZIfgFo0wHXzwRetkyNUcjMDyulHJBLAT/Py05pT0M26bVM/JEAkFqeyy27tJ39wud5/noAhQCOw3xddwCYD+ANS5XdAF7heT4VwAaYV3m4D8ABAGCMaXie/wTAozzPrwdwAeblzfIs7V6GMfYVgK/6em2CIAjjxo3btHfvXnbttdfu8vf3/+b48eMxnp6eE7Kzs78dOXJk9uHDh6//zW9+c31lZeXq2NjYr0pLS6VhYWEDWjotPz9/wrXXXnt/ZWXlpxMmTPi4pKREFhYW9joA9HROmUyWW1JSknjzzTdnlpSUbBo/fvyqgoKCXv/pWV9f/65Go/nTzz//vD4/P39xfn5+cllZWao97qZVVVX1D61W+/znn3+eu2XLllu+/fbb0jlz5kwQi8V3bd68+Q8d63Mc91ppaelvfvjhh8927dp13/Hjx28vLy+f2HFMb39wHPdaWVnZjf/73/++3759+2+LiorSz5w5s2ju3LnfWOvk5eV9olKp3j98+PAWHx8fYeHChdb/o/Dx8Xn18OHDH6anpz/l5+f3TnFxsQhAuqenp092djY/0PhcUU1NDby8vBwdBiGEDBphoDmvUg48fwNGjOjlpLGXupi37aMCfnq6830hfkDz15due+Jq86O/57OzoTy84Q6Ye17XwbzubpvlEQqYe1l5nreduT4S5iXJ6gGcA3APgFsYYxsAgDG2E8CDAP4OoAFAPgAtgLts2ngCQC6AwwDKLefKZIwZ7X1xtbW16QEBAet27NjxyWeffWY8fvz4QYPBMB8A8vPzbw8ICFi/adOmb/fv39/a0tKyYMGCBfd+8MEHxwZyznHjxv3y66+//uXAgQOa2tra6xYsWPDEBx98cLg35zx69OjzQUFBh3/++edVBQUFTf7+/galUqnt7blLS0trJk6ceNvJkycnf/XVV43nz5//OiIiYsNArsfq/PnzLWPHjp1+4cIF39WrVxfLZDLT8ePH97e1tc3trP6xY8cOxcXF/XH//v03fPPNN5q6uro3wsPD94vF4gH/nI8dO3Zk6tSp9x07dmzRypUrW4uLi7+Jj49fs3Tp0rusdQRBMI0aNWrjqVOn/KKjo3czxtpfx0OHDn0UFRX1x0OHDj37/ffftx04cKCltLT0Y5j/f5NO+Pn5OToEQggZVANep1cuAa6biYqKwV8LdyjjhAH/+UCcgaenp2bmzJlZ69atu8HRsQxF4eHheV5eXp4HDhxwtaUAXP4NXlZWRis4EEJc1+ajMF77F4ib+rnIrrsUePFG4KnrUFNTMxxu6NPlt9BDdngDIYMpMTHx+ZCQkP8WFhYWi8XiJ0tLS2MmT578N0fHRfpOq+31Fw6EEOKUBjyP7f5FAACDYXivcklJL3E4juNCpFJpp5P0JkyYcHb//v0h9j5nXV3d/EOHDr1sMBg4Ly8v/Zw5c1YuXrz4md4cK5PJjOhkaNCYMWOaCwoKep4+SuyKenkJIa6vn1mvmwi4bY75Vr8AmpubERAQYMe4nAslvcNEQ0ODvOdajiEIQinsd8OZXsnPz0/u77FarVbccy1ypZSVlfX/XvKEEOIE+j0U1c3NvIKCxXDvJBjKE9kIIaRHHh4ejg6BEEIGVb/X6U2IAsZevDFpWVmZnSJyTpT0EkKcmlTa6+WiCSHEOfWnp9dDDjx36dx1iURip4CcEyW9hBCnVldX5+gQCCFkUPVrcEOAJ5A84ZJNPj4+donHWdGYXkKIUxvOkzIIIcOARAyR1gB49uHukzoD8Oz1ly37UFlZCaVSaecAnQclvYQQp1ZbWzusP8QJIS4uaTzO/+deBPn24UY8MgkwL/ayzd7e3nYMzPlQ0ksIcWp6vd7RIRBCyODhOIgWTAYCAwfc1HAf3kBjegkhTm24L8FDCHF9NGHXPijpJYQ4teG+BA8hxPXRhF37oKSXEOLUaDwvIcTV0YRd+6AxvYQQp+bmRh9jhBAnUF4DNLZ1X8dLAYy8fNwtTdi1D/ptQQhxavX19fD19XV0GIQQ0rXmNgjhD4KT9XBzCL0BKPgXEHLpSg00Ydc+aHgDIcSpBdphRjMhhAwqvRHgADS1df8wmoC//3zZ4TRh1z4o6SWEOLXq6mpHh0AIIT0SenMrYb0R+M9GoPnSYRA0Ydc+KOklhDg1k8nk6BAIIaRHHLieKwHmew5/uuWSTTSe1z4o6SWEODUa3kAIcSmtWuAvKwGbP+hpwq59UNJLCHFq586dc3QIhBDSIwG9GN5g1dgGrDnQ/rS+vt7+AQ1DlPQSQpyaWq12dAiEENKjXg9vAIBmDfDKf9uf0jda9kFJLyGEEELIUHP0jPkBmrBrL5T0EkKcWmNjo6NDIISQHvVpeAMAaA3msb0Y4ITd2iZg6XLA4xYglAFfb+++/ttZQOA9gOftwD3/ArQ2awTf/g4w8h5AfRsQ9RDw7w39j8sBKOklhDi1oKAgR4dACCE94rg+DG8AzBPZfvwFuFDfu+ENL31rfnT00EeA1A2o/Bj46nHgQR7IK+28jXUHgeU/AJteAk5/CJyqBF60afOZ64DTK4DGr4CfnwGe+wbYX9y363IgSnrJkBYZGXloypQp5Y6Owx5iY2O/GzduXIOj43A1FRUVjg6BEEJ61Kt1ei87CMC/1vR/wm6LBli5B/jzrYDSHUgaD1wdD3yxrfP6n20F7k0BYkMAbyXw/A2XLp8WGwJY7yrHceYbbhQ7z2cwrYHh5HieFwNYDuAuAHIA6wHczxjrdAAQz/NLADwJYBIAMYBjAJ5ljO2w7PcB8BOAcZb2qgB8AuA1xphgqfN7ALcBmAjgHGMscpAuz25iYmJuLigo+OaDDz4YzRhzyCrfeXl5NznivK5OJKK/3QkhQ1+fJrJZafXAP3KgZnP7d9LCc4BYBETZfCM2ORTYlt95/bxS4Jp4m7pjgMp6oKYJ8FWZty1bYU6E23TAlDBgydT+xeYA9NvC+T0N4BoAMwBY71P4RTf1vQH8E0AkAD8AXwNYw/P8aMv+FgAPAhjFGFMDWABzgnufTRvnAPwNwGt2uoYhLyMjw93RMZDOjRgxwtEhEELI4DGaIPvp1/4d26wBPBWXbvP0MN/yuDf1rWXb+u/fDzR9Bex4Dbhu5sWeXydASa/zYwD+yhg7xRhrAPBHAGk8z4/ptDJjXzHGfmSM1TPGDIyxDwC0AZhm2a9ljOUxxmxGrsMEINqmjf8xxlYCuOLDDiZMmLDex8dHJ5PJBF9fX/2cOXM2WHq74e/vLx03btwBpVJplMvlgp+fnz4tLe0/HMcFFRUVfWMymfDII4+clclkQmpq6sruzhMZGXlo4sSJFdHR0ccUCoXp/PnzBQAQHx//zMiRI5vd3d1N/v7+uoULF37F8zzHcZybSqXSZWRkfNSxnSlTppwDgJiYmJ/Gjh3bat0XGxurmjBhwg4fHx+9h4eHKTIysu6GG264BgDGjh2bIpfLhZdeeikEACZOnPgmx3HC0qVL/woAfn5+QSKRSFi2bFlqd9fR1NTk8uXS0tI+1acylalMZUeU+zyRzapZA/HKPZ23n/EaBK/bAa/bgeU/Qlj+o7nsdTsMaS8DSjnQ2HrJsZqqOkDl3nmcSjnQ2HZxe6Ml2e1YXyxG0+RgoKwG+GCdw1/bjuWu0PAGJ8bzvCeAEAD7rdsYY8U8zzfCPHzhdC/amATAF+ZhDrbbswGkwDzE4SyAFQOI81YA73dTJZcxltGbtlQq1fF58+Y9r9fr9164cOG+3bt3r/D29n6LMfZoQEDA32praydcffXVcV9//fXRefPmTZFIJOGCIJyLiYm5paCg4Jt//vOfvR7ecPz48YAFCxasnzt37rTy8nKPuLi43508efL15OTkZ728vN4oLi5empub+61KpTotCMKfxo8fv/XUqVM3wtIrrlQqlXq9flJGRkanPeJtbW17JBJJUGpq6qyTJ08e1Wg032/fvn3lH//4x5FFRUVb3N3djUeOHLkfwJ9aWloW+/r66isqKjIAPDVy5Mj7pVKpMS4ubmMPr5fLl/39/R0eA5WpTGUqd1uua+7f8AYAUMrhxhZB1ln72X+62OpL35rLL90MwJLgtWgAgwmqiibAcoz8xHkgdnTnccaGAIdPQ3Xj/7d35+FRVXcfwL9nssxkJjuEBAgJkLCrAVlkXxI0QGKrvrWtxSpqe3ndrbVV36ql1VdtbV+11iq30mqpK621TSJBIBGRRfbIIgEiEMIyhJAQZpKZSTLn/eNOcAiTPZNhZr6f55mHu5x7729ymDu/Offcc6dpC0uOAImxF7o2XFK+sQkoO3X5/J3bwaTXvzWPyt/y5qgat3WtUlW1H4B/APitoigH3dcpipLrakGdCOB6AF0eJFBRlHegdaPotk2bNj3oNqsOGTLkp2azORsAhBA2h8MRUl1dfZ2iKF9/+umnOwHs7OqxBg8ebCssLLzNNWtLTU39xbhx43bm5+c/51q2YsSIEQ8cPnx4EYBfSCmfPHjw4ObFixdnLl26tGjIkCFPmc1mmZ2d/VzLfev1+r4Oh2P0nXfeefOyZcu2uuK/Qa/XN5SVld0vpXxqyJAhZWazOVcI8YTJZBo+bty4V48ePXqvqqqirq7uW8nJyV8ritKNcWwCQ2Njo69DICLyHkM4TkwYiCFd2dZkAG66BnjqPeCNe4Bdh4F/bwU2Puu5/G2zgEV/BBbOBPrHAc+sABbN0dadrgGK9gC544GIcGDNl8C7nwPv/KSLb6z3sXuDf2tuy49psTwWQJuDl6qqOgBAMbQb3x73VEZRlCZFUTZDS6pf7VakPWTcuHFvJiYm1hmNRqfRaJQVFRXDbTZbDADs3r37ieTk5I927dq1ZPny5ZYRI0aYb7755hu6eqyoqKhq93mr1dp369at44xGo2x+HT16dHpdXV00AOzfv/+Lvn37VpWWli4BgKqqqttHjhy5TVGUupb7TklJGQcA77777ormfUVERDQ5nU6d1WodBgCRkZGFx48fHzl06NBpJpNJl5aW9uiZM2fEnj17ZlRWVo5KTEz8pKvvLZBYLBZfh0BE1K4udW+ICAd+9m3owrvRb/ZPinbTWb87gFteBF5TtBZdACivBCJ/oP0LAPOuBn5+AzDnKSB1MZCaAPxKazmGEMBrhUDyj4G424BH3gJeuhP49qSux9bL2NLrxxRFqVFVtRzA1QB2AYCqqkOhtfJ+2dp2rv6+awH8S1GURzpwqFAAw7oap6qqC9F294j1iqLMb28/o0ePvvHQoUO3z58//7HExMSXVVW1DRkypBSupF9K2QjgvwBgypQp/SsrK9ds3779PWhdNLrSHHjRGcpoNJ4dM2bMl+vWrZvW2gYJCQlvHzhw4N6rr756otls7rdgwYIfeSpXXl5eAgB33HHHda+++qrH0b1rampeP3HixAMZGRlPx8XFlb/55pv24uLio5s3b15itVr1I0aMULvwngJOcnJy+4WIiHys0+P0Nlt8Hfp2JFtzdWu4RHwU8NFjntelJACWFhdiH/6W9mopIQZY90wHArl8Men1fyqAR1VVLQZQBeA3AFYpinLEY2FVHQlgDYA3FUV5wsP6yQBMADYCcACYBuBB13Gay4RC+78TBkCoqmoAAEVRbJ6OqSjK2wDe7uL7u0Cn0yXodDpEREQc3r59u2PixIkPnzhxYnhMTIwZAKZOnXp3XFycIzo6+j2r1Vqj0+nq4Epca2pqSp1OJzZt2jRDUZR3u3L8+Pj4Z0pKSl7Pycl5NCEh4aWysjIdgJyYmJj4/Px8FQCOHz/+S4vFcn9ERERBenq65Y033sjztC+73X46LS1tb3Fx8Ts333zzjStWrPh89uzZAx0Oh3LVVVctf/311w8dO3bsq9jYWNtXX301Oysr6w0AiI6OXrt79+4fp6am2l944YVWf9gEk4qKCqSlpfk6DCKiNkkpO9erNzQEuHUWEGPCqbIynud6ALs3+L/nAeQB2AptNIUQALc2r1RVdaGqqu7Xfx8FMBDAQ6qqWtxeC13rw6ENR3YaQDW0Fto/AFjito8noI34oAIY6ppuZfyTnrN37943Bg8evPU///nPe/v372+02+0PpqSk7G9eb7PZBm7ZsuWPH374oaWsrMzqdDrTxo0bdysAnDhxYveIESM2r1ix4m2j0Sivu+66FZ09/q5du/48fPjwn+/atet/Pvjgg/odO3ZYy8vL/wKgf3OZs2fP1gwaNGjf119/nZCWlvZhW/szGAxTQkNDdxUVFa01GAxy165d5TU1NXeGhYVdOC8mJCTsdjgcGD58uAoAVqv1zzabDf3799/b2fgDVViY/wyXQ0TBq9M3soXqtK4GAGJjY3s8nmAkuvSEECLyFwH/AbdYLIiMjPR1GERErau2QCbdCeHoRE+7zCuBtb8CAJjNZiQmJnopuIDT6q8LtvQSkV8zm82+DoGIqF2dupHNpNceAezCG3Z7BpNeCkoZGRm/1uv10tMrOzv7fV/HRx0XFxfn6xCIiNrVqe4NibHArDEXZnnDbs/gjWwUlEpKSp4C8JSv46Duczgcvg6BiKjnmAxaK6/baA+8YbdnsKWXiPya1Wr1dQhERO3qcPeGEAHcMuOiRbxht2cw6SUiv8bLfkTkDzo0Tq8hDLh/AaC/OMmNj4/3UlTBhUkvEfm1iooKX4dARNS28FDIEB0QHtr2K0QHPJBzyea8YbdnsE8vEfk1vV7v6xCIiNpmMuD07t8iKb5P2+X0oUDEpec03rDbM5j0EpFfi46O9nUIRETtCo+NBGJNXdqW3Rt6Brs3EJFfq6ys9HUIRETtqq6u9nUIQY9JLxH5tT592rlcSER0GeAT1XyPSS8R+bW6ujpfh0BE1K6zZ8/6OoSgxz69ROTX6uvrfR0CEVG7GhoaLllmcUj884BEkwRuGiYQa+jEU9uo05j0EpFf4zi9ROQPPJ2rfrXRiT/ulHBKoLJOh0evYdLrTezeQER+jeP0EpE/aHmuqmuQeL1EwtYENEl09Hlt1A1MeonIr0VERPg6BCKidkVGRl40/7d9Tia6vYxJLxH5NaPR6OsQiIjaFRr6TY9SKSWe3SxhvbSbL3kRk14i8mtVVVW+DoGIqF01NTUXpj85IlFt810swYpJLxH5tYSEBF+HQETUrqSkpAvTv97khIWtvL2OSS8R+bXa2lpfh0BE1K4zZ84AAPadkdh52sfBBCkmvUTk1+x2u69DICJql9PpBAA8v8UJR9Ol6/mgHe9j0kuXBSGEzMrKutvbx5k7d+4Pk5KS6g0Gg8zIyNjp7eOR93GcXiLqqLP1Ejd+1ATTS41IXdqId75ytln+xW1OJP2pETF/aMSdhU2wN2rjLdgbJe4qbELq0kZEvdyIcX9rxMqv295XUlISquolVpRqD6NoKTw8vMvvizqGSW+QmzFjxsspKSnnDAaDDAkJueRjOG3atDd0Op3U6/UXXiNHjjza1ePNmzdvuhBCzp8/f0L3Iu+asrKyF/v37/+lzWYTJSUl47q7v2nTpr3Rp08fR0/ERl3DcXqJqKPuXetEeAhgvicEb+eE4O7VTuw943ngsFWHnXh+ixNrvxuCI0oIvq6R+OVGLbFtdAKDooB13w/BuQdC8PQ0Hb6b58SRc60PQnbixAm8urP1xPjcuXPde3PULia9QS40NNScmpr6t/Hjx/+1tTJxcXENdrtdNL/279+f2psxdkVubq7HwVtra2ujTSbT9t6Opy3XX3+9yM3N1fs6Dn9lMpl8HQIR+QGr65G/T0/TITJcYHqywLfSBZbv85yIvrVX4q4rBcb0FYgzCDw5RYc392hJrSlcYMm0EAyOEdAJgdw0HYbEANvNrSe9BlM0XtyuPYzCE7b0eh+T3iBXXFz87Pr16+/X6/Vf9tQ+MzMzH0hOTrZERETIhIQE+9SpU//evG7dunXrAaCoqGirXq+XEyZMWNO8rq6ubtbAgQOtBoNBJicnn8/Ozp7fvC43N1c/ceLEVX379nUYjUY5aNCg2muvvXZh8/rRo0cfGjVq1OExY8YcNJlMzqNHj25tGZfRaJTV1dVhX3zxxd16vV7OmTPnZwAwa9as5wYMGGA1Go2yX79+9unTp7/avM38+fMnDB06tDIyMtIZEREhU1JSzjUfNysr68dbtmy5q7q6Oqy5FTwzM/OhzMzMh1q2mk+YMOHTIUOGXBhbSwghJ0+e/M8BAwZYV61a5bTZbLfm5OT0GTt27Nb4+PgGk8nkHDp0aGV2dnZm8zYzZsx4uV+/fnaDwSCjoqKaRo8efajrtRQ4+EVBRB1xoBoI0QHD47951G9GgsDeM57L762SyEi4uKy5DqiqvzSxNVslDlQDY/q0/hjhfx8JR2MbPSBCQkLafxPULUx6qV21tbVh0dHRTbGxsY0jR448Om/evBmtlZ03b96Mzz///OXU1NT3s7KyDBkZGfeUlJT8YObMmb8HgFmzZs0AgMzMzIl2u11s27ZtbvO25eXluVdeeWXW3Llzo00mU82BAwcuJMsnTpwoOnny5LQJEyZcm5WVFZ6SkvLPTZs2LV+wYMHg5jIHDx4cHB8fX5CZmalPSUmZ1TK2uro6ERsb23TNNde8ZrfbRXFx8Qtz5sx5dNu2bY+OGjXq51lZWaFXXXXV4h07dtydmZl5HwBIKUMHDRq0dNasWQlZWVnxcXFxX2/btu2t3NzciLVr1/550qRJy9xbwouKil7q6N/1yJEjC6688src7OzscIPB8EFZWdl2i8WScs0114ydM2dOVFxc3N4dO3aszM3NjcjJyemzcePGB6644orHbTabmDlzZlL//v1f7OixAll1dbWvQyAiP2BpAGJa/EaO0QPnHZ5bZy0Obb17WQA436JDW0OTxMICJ24fIzCylaRXSonf7Qpvc5iy+vr69t4CdROTXmpTVFTU8szMzOxZs2aFT5kyZZxOp2vYunXr2pycHI+Do1ZVVT2RmJh4fsOGDXfl5+fb16xZs2zEiBGbKyoqbm/vWGlpaUsLCws35+fnn+/fv/8ys9kcB2iX//ft2zd1+PDhjxUWFq7Ly8tr2LBhwx0mk8lRW1v78+btk5OTq9evX/9QXl5eQ0FBQYeeWHDs2LGHR40aVbx27dpX8/LymtasWfNmenr6brPZfB8AFBYWbl63bt0TBQUFVfn5+dXJyckLampqQhwOx9z29t2e9PT0v61atao4Ly+vwel09i8tLU1NT0//9sqVK/fm5+dbBwwYkGWxWMLr6+sXAVorQH19/aQFCxYMLSgoqFy7du2r7RwC58+fD/hp90d7Xg7xcJrTnPbN9Oz3GiF+5/k1/d1GiAYrah0Xb1trByJ0TR73aQxxotb+zfLmbeGwXChzrvY8fvix1k/4uUl1Hvdz/vx5OCVwsv6bJ7JdSiIyKtKrf59gmm5NWzVAhMLCwnVus7tzc3OvPHToUF1dXd3tAH7XsrzNZhsYGRl50cUig8FwwGKxjG/vWGFhYRcu1+t0uhq73S4AoKmpaYTdbsemTZteMRqNrzSXaWpqgsPhGNI8bzKZWrlI1brz58/HHDt2LNNoNF74qe90OjFw4MBKAFiwYMHwI0eOrDx58mRqfX19iE6n/U5sbGwc0souOyw8PHx387TD4ZgCAJ999tkm98fqut7j6LVr11ZlZmb+pLy8/LGSkpJDAwYMqE9LS/vr+vXr72vrGFFRUQE/7T5k2eUQD6c5zWnfTH/6fbTJ6jCh0dmEg9USw+K0bUsqJTKSvkmF3Pd5Zb8QlFRKfHektryo3IlEIzA4QSsjpcRDG4ww1wEf36RDRFjbcX4n1YL3j0ShwWMXB4H6unoAEZfN39Ofp1vDll7qLKcQAgA8XsMxGAzHrVZrH/dlNpttWGRkZPNPsMbOHjAkJKQ0PDwc06ZNW1RXVyeaX3a7XWzZsmW+W9G2x4vxIDIysjYjI6PAfb82m02UlZX1A4Dy8vL/1NfXx06ePHm8zWYTc+bMGeDaVAcAQoimlvvU6XRVTqcTubm5Fz6BNpstsWU5923Dw8O3AsDs2bNHucficDjE+vXr7weAoqKilw4dOpSUlZWlHzZs2CsbNmy4Nzs7e05n33OgaWjgY42IqH2mcIGbhgk8tcEJq0Niw3GJfx+S+OFoz6nQbWMElu2W2HdGotom8cwmiUVXfPPVd/caJ746K5F3ow4RYa335W22eKQVIW0Ua2q65OuEehhbeoPc9ddfHyalNEopDQCQm5sbAwBCiNq8vDw5e/bsJREREfmhoaHbm5qahpSXl+cbjcYmo9H4N0/769OnzzMlJSWfTZ8+fWlsbOwDNpvtltLS0injx49/GQBCQ0NLhRCw2+0zAGzrSIx5eXny6quv3nDw4MGXs7Ozj69atWpNTk5OYl1d3V0Gg2HlypUruzzebkpKyv/t2LHjf7Oysu41Go2qlDLcbrffIKXUrVmzZnlDQ4MxNDTUERISciQnJyexvLx8tfv2YWFhh61Wa1hOTs7AgoKC4wCg1+vX6PV6VFVVvXH99df/oK6uTjl8+PCIfv36tdr59OOPP943atSoI2VlZavnz59/w8qVK7fn5OSkWiyWuyMjI190Op19rVbroqioqD8UFBQcy8zMNAOAECLoMz6O00tEHfWnuTrcucqJfn9qQp8I4LVrdRjTV8tEy2slRv+1CfvuCEFKtMC8ITr8fBIw54Mm1DcC/zVM4FdTtQT56DmJpSUS+hAg6bVvktWl1+qwsJUkelxqHGYOAj454jm2mJiYHn2vdCkmvUHu7Nmzr23cuPGu5vmCgoIaAMjOzp4B4POampoFO3fufNJms+n0er0zKSnp5KRJkxYUFBSYPe2vsLBwfWZm5sOlpaVPnz17VomMjGy46qqrVnz22Wc/AYD8/PzqiRMnfrJt27bfGY3G/xszZszqrVu3XtdenAMGDMgKCwv7YOfOnQUGgyE8PDzcmZiYaE5PT/+kO++/uLj4+dmzZxtKS0ufP3v27CtCCPTp06c2PT19CQAMHjx48d69ez9YvXp1jclkahw9evSfdTrdmObtTSbTiykpKfd/+umnx4xGo5g8efKDRUVFf5g5c+bv9+zZ8+DOnTsbU1NTK9LT03fV1ta2OdTb0KFDJ5w8efKjrVu3bjIYDGEGg6EpKSmpIjIy8vcAQsvLy+86ffr0Tw0Gg4iOjnZMnTp1aWFh4efdef+BoKKiAmlpab4Og4j8QHyEwEc3hHhclxItYHnw4rTo4Qk6PDzh0iQ2NUZAPtK5FOrUqVN4YvJQbDjeBKuH5gptnF6Pt8tQDxFStj6mHBH5vYD/gJvNZiQmXtJ7hIjoslJVVYX4+HiM/EsTDrS47hcigMfGWvFMFlt7e0CrnUjYp5eI/FpoKC9YEdHlr7GxEUIIPDFZIDLs0vU6necWaOo5THqJyK/V1NT4OgQionZZLNpQZ98bqUOoh+yL4/R6H5NeIvJrSUlJvg6BiKhdzTfdhocIPDRewNCiYbcjQ25R9zDpJSK/duZMp4dnJiLqdRUVFRem7x17afplrbP2ZjhBiUkvEfk1p7PTwzMTEfW6sLBvOvL2NQp8Z7i4aNxeDizgfUx6icivsXsDEfmD+Pj4i+Yfv0aHcLcuDlGR7N7gbUx6icivnThxwtchEBG1y2y+eHj70X0FxvbTpiWA2tra3g8qyDDpJSK/Fh0d7esQiIjaFRcXd8myF2aFICUK6G8CJg3k8IvexodTEAW2gP+AV1ZWIiGBTzEiIv/Gc1mP4cMpiCgw8ZIgEQUCnsu8jy29RIEt4D/g9fX1iIiI8HUYRETdwnNZj2FLLxEFplOnTvk6BCKibuO5zPuY9BKRX9PpeBojIv/Hc5n38S9MRH6tb9++vg6BiKjbeC7zPia9ROTXeEmQiAIBz2Xex6SXiPxabGysr0MgIuo2nsu8j0kvEfm1xsZGX4dARNRtPJd5H5NeIvJrFovF1yEQEXUbz2Xex3F6iQJbwH/A7XY79Hq9r8MgIuoWnst6DMfpJaLAVFFR4esQiIi6jecy72PSS0R+LSwszNchEBF1G89l3sekl4j8Wnx8vK9DICLqNp7LvI9JLxH5NbPZ7OsQiIi6jecy7wv1dQBE5D1CiAYAIR0piiC46c0PsB4uD6yHy0Ow1cPl+n4v17ha45BShntawdEbiAhCiG1Sygm+jiPYsR4uD6yHy0Ow1YMQwiqlNPk6jpYCqR7YvYGIiIiIAh6TXiIiIiIKeEx6iQgAVF8HQABYD5cL1sPlIdjq4UNfB9CKgKkH9uklIiIiooDHll4iIiIiCnhMeomIiIgo4DHpJSIiIqKAx6SXKAgIIf5XCHFYCFErhDgthPiHECKljfKPCCHKhBDnhRAHhRD39Ga8gU4IoRNCbBRCSCFEchvl5gkh9goh6oUQe4QQ1/VmnIGuI/UghFgghCgSQpwRQlQLIdYLIWb0dqyBrKOfB7fyd7vKPtEb8QWLTpyX+gkh3hJCVLm+U3YJIQb0ZqxdxaSXKDgsBzBWShkNYDCAcgDveSoohPgWgF8BWCiljAJwG4AXhBDX9lKsweAnAOraKiCEGArtbu7nAMS4/v2XEGKw16MLHu3WA4A4AK8ASAeQAOAdACuFEIO8HFsw6Ug9AACEEKkAfgpgt1cjCk4dOS8ZAKwF4AAwAkAsgIUALN4Oricw6SUKAlLK/VLKc65ZAcAJ7YTlSTqAEinlZte2mwB8CSDD64EGASHEcAD3AHiknaK3A9gupfy7lNIhpXwbwA7XcuqmjtaDlPJtKeW/pJQ1UspGKeVrAOoBBMQTqnytE5+HZssA/ALAWa8FFYQ6eV6KBXCPlPKMlNIppdwrpaz1dow9gUkvUZAQQvxACHEO2i/yBwEsaaXoewCihRDTXJe7ZgAYDqCwdyINXEIIHYC/APgZgJp2imcA2N5i2Q7wx0e3dbIeWm57FYA+APb0fGTBpbP1IIRYDKBOSvm+l0MLKp2shzkA9gFY6uresF8I8bCXQ+wxTHqJgoSU8h0pZQyA/tAS3tYuD54G8A8AxdAuYRUD+KWUkl/y3fcggFNSyo4MQh8F4FyLZTUAons6qCDUmXq4QAjRD9pn47dSyoNeiSy4dLgeXPcgPAHgbq9HFXw683noC+A6ACXQvktuBfA/QoiFXoyvx4T6OgAi6l1SylNCiD8D+FoIkSKlbHmZ8EkAtwAYC+ArAKMB/EcIUS+lXNa70QYOIUQ6tL6IHb0sfh5aX153sQD84jLi5aoL9dC83QAAqwF8AuBxL4QWVLpQD28AeEZKedx7UQWfLp6XjkspX3bNbxNC/B3AtwG87YUQexRbeomCUygAEwBPd9yOB/AvKeU+qdkL4CMAub0YXyCaDu1GqD1CiDPQuioAwJetjI5RAuDqFsvGuZZT13W2HuC6eXA9gJVSyvskH2XaEzpbD9cCeNY1isYZANMAPC6EWN874QasztbDLgCe/v/7xWeCjyEmCnCu/lr3APhASnnaNRTNK9BacodJKRtblH8cwCIAuVLKg0KIUQDyAbwppXy6V4MPIEIII4B4t0XJADYBmAhgv5TS0qJ8GrQuKHdBu6T+HWitXWOklEd6I+ZA1IV6GAlgDbT//xwiq4d0oR5aDqG1AtoPkd9LKc3ejDWQdaEeUqFdAfwZgNcBXAHt83GfP/S1ZksvUXBYAO2XvBXAF9CGpZkrpWwUQiwUQrif2F4A8C8Aq13LV0Fr6X2+l2MOKFLKOillRfMLwCnXqlNSSkvLepBSlgG4CVo/xlrXvzcy4e2eztYDgEcBDATwkBDC4vbyiz6Ml6sufB4qWpS3A6hlwts9XaiHo9C+T34E7bz0DwBL/CHhBdjSS0RERERBgC29RERERBTwmPQSERERUcBj0ktEREREAY9JLxEREREFPCa9RERERBTwmPQSERERUcDjY4iJiIiIvEAIIaGNi/6SlPIXbssTALwD7fG/HwI4DcAspXypg/vdAuAO1xMzm5cVAZgKYJuUcnqPvQkXVVWHQXtgzj8URbm1jXKfApgMoPnBR8cVRRmhqqoewJ8AzIX2QIxDAP5HUZSVru3aXN8T2NJLRERE1EVCiDghhGzx8BKLEOJBV5EM94TX5XEAB6WUcQAeA3AbgKVu+0wTQliFEP3dli0UQpwQQgwC8DsAv3bfoZQyE8B/e+EtNnsVwNYOlr1PUZRI12uEa1kogGMAZgGIAfAkgA9UVR3cwfXdxpZeIiIioq4bC+CslLJPyxVCiJda2WYugOakeBGAj6WU9c0rpZRlQoh8AA8BeFQIMQXAHwFkSymPCSEqAbwuhOgvpTzZU2+kNaqqfh9ADYCNANK7sg9FUawAlrgtyldV9TCA8QCOtLe+K8dsiUkvERERUdeNBbCvIwWFEOEAKgFEA8gTQhx2zf/FQ/HfACgSQiyH1gXiv6WUWwBASmkTQmwHcB2AtzoTrKqq+QBa6/7wuaIouS3KR0NrVc4CcFcHD/OcqqrPAygF8AtFUT71EEcigOEA9rZc15H1XcGkl4iIiKjrxqGDSa+U0uFqtS2WUiYCgKvVttRD2R2uvrtfAPiNlPL9FkW+ApDR2WBbJrUd8DSAZYqiHFNVtSPlH4X293AA+D6APFVVxyqKUtZcQFXVMABvA3hLUZT9LXfQ3vquYtJLRERE1HVjAaQLIb7ntuwvUsqH2yhf4jYfC+B8y0JCCB2AJgBOaK2+LZ0H0N/D8h6jqupYaF0xxnV0G0VRvnCbfUtV1VsALADwimufOgDLoSXF93k4Zpvru4NJLxEREVEXCCH0AEYBmCKl3NbBzcbi4qS3GkCUh3K/h5YQHwSwEJd2gYiC1s+2U1RVXQlgRiur1yuKMt9tfjaAwQDKXa28kQBCVFUdrSjK1R08pAQgXMcWAJYBSASwQFGUhhaxtbm+u5j0EhEREXXNFdCSut2d2CYDWktmsy+h9V29MDKCEGIxgBsBXAMt8VwihPirlFK6bTcKwN87G3CLpLY9KoD33OYfgZYE3+2xsKrGQot5HbQhy74HYCa0G/IA4DVocc9VFKXewy7aW98t4uK/HxERERF1hBDiRwDulVJ6vPzvGqd3mJTykNuyUwCulVLuds0/DGCklFJxzc8F8D6A2VLK3UKIEAAHAPxUSvmRq4wewEkAV0gpT7jtexGAH3ljnF4AUFV1CYB093F6XS3H6xVFeVZV1QQAHwMYCa1rxn4ATyqKslpV1VRoozDY8c0YvgCwWFGUt9tb3xPxM+klIiIi6gIhxB+hjY1rc1ssASRLKc+1THqFEEkAjgKIlFI2uJb1BbALwDAAqQA+B3CblPJjt+PcC+BWKeUU1/zNAG6RUt7UIp5F8GLS6++Y9BIRERF5gRDCBq3l8g9SyifbKPcsgNOdeCLbFwDuklLucVu2GtqT0LZIKbO6FXiAYtJLRERERAGPjyEmIiIiooDHpJeIiIiIAh6TXiIiIiIKeEx6iYiIiCjgMeklIiIiooDHpJeIiIiIAh6TXiIiIiIKeEx6iYiIiCjg/T90JZ0OuVHJmAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 576x468 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train an XGBoost model\n",
    "model = xgb.XGBRegressor(**best_hyperparams).fit(X_train, y_train)\n",
    "\n",
    "# explain the model's predictions using SHAP\n",
    "# (same syntax works for LightGBM, CatBoost, scikit-learn, transformers, Spark, etc.)\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# visualize the first prediction's explanation\n",
    "shap.plots.waterfall(shap_values[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6586311700421369"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=0.658631170042137>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "R_squared(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important parts:\n",
    "- Regularization on the weights (L1 or L2)\n",
    "- Dropout\n",
    "- Layers (3 proposed in:\n",
    " @article{2021,\n",
    "   title={Airbnb Price Prediction Using Machine Learning and Sentiment Analysis},\n",
    "   ISBN={9783030840600},\n",
    "   ISSN={1611-3349},\n",
    "   url={http://dx.doi.org/10.1007/978-3-030-84060-0_11},\n",
    "   DOI={10.1007/978-3-030-84060-0_11},\n",
    "   journal={Machine Learning and Knowledge Extraction},\n",
    "   publisher={Springer International Publishing},\n",
    "   author={Rezazadeh Kalehbasti, Pouya and Nikolenko, Liubov and Rezaei, Hoormazd},\n",
    "   year={2021},\n",
    "   pages={173–184})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabnet\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 6000\n",
    "batch_size = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(ds):\n",
    "    features = tf.unstack(ds[\"features\"])\n",
    "    prices = ds[\"price\"]\n",
    "\n",
    "    x = dict(zip(col_names, features))\n",
    "    y = prices\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_col = [col for col in listings if np.isin(listings[col].unique(), [0, 1]).all()]\n",
    "num_col = [col for col in listings if ~np.isin(listings[col].unique(), [0, 1]).all()]\n",
    "col_names = bin_col + num_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.from_tensor_slices({\"features\": listings, \"price\": price[\"log_price\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.shuffle(6000, seed = 13)\n",
    "train_dataset = data.take(train_size)\n",
    "train_dataset = train_dataset.map(transform)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "test_dataset = data.take(len(listings)-train_size)\n",
    "test_dataset = test_dataset.map(transform)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "#for num in num_col:\n",
    " #   feature_columns.append(tf.feature_column.numeric_column(num))\n",
    "#for binary in bin_col:\n",
    " #   feature_columns.append(tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list(binary, [\"No\", \"Yes\"])))\n",
    " \n",
    "for col in col_names:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TabNet]: 34 features will be used for decision steps.\n"
     ]
    }
   ],
   "source": [
    "model = tabnet.TabNetRegression(feature_columns, num_regressors=1,\n",
    "                                output_dim=30,\n",
    "                                num_decision_steps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-29 00:05:11.136849: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - ETA: 0s - loss: 4.1425 - R_squared: -5.9133"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-29 00:05:13.757247: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 6s 218ms/step - loss: 4.1425 - R_squared: -5.9133 - val_loss: 0.6384 - val_R_squared: -0.0729\n",
      "Epoch 2/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.5667 - R_squared: 0.0345 - val_loss: 0.4834 - val_R_squared: 0.0635\n",
      "Epoch 3/600\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 0.4812 - R_squared: 0.1867 - val_loss: 0.5864 - val_R_squared: 0.2075\n",
      "Epoch 4/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.4306 - R_squared: 0.2637 - val_loss: 0.4241 - val_R_squared: 0.2280\n",
      "Epoch 5/600\n",
      "10/10 [==============================] - 1s 134ms/step - loss: 0.4324 - R_squared: 0.2732 - val_loss: 0.3950 - val_R_squared: 0.2662\n",
      "Epoch 6/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.4334 - R_squared: 0.2704 - val_loss: 0.3982 - val_R_squared: 0.3274\n",
      "Epoch 7/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.4261 - R_squared: 0.2859 - val_loss: 0.4009 - val_R_squared: 0.2876\n",
      "Epoch 8/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.4208 - R_squared: 0.2984 - val_loss: 0.3767 - val_R_squared: 0.3137\n",
      "Epoch 9/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.4147 - R_squared: 0.3097 - val_loss: 0.3916 - val_R_squared: 0.2949\n",
      "Epoch 10/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.4155 - R_squared: 0.2960 - val_loss: 0.3920 - val_R_squared: 0.3305\n",
      "Epoch 11/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.4126 - R_squared: 0.3157 - val_loss: 0.3718 - val_R_squared: 0.3639\n",
      "Epoch 12/600\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 0.4049 - R_squared: 0.3249 - val_loss: 0.4936 - val_R_squared: 0.3165\n",
      "Epoch 13/600\n",
      "10/10 [==============================] - 1s 149ms/step - loss: 0.3835 - R_squared: 0.3357 - val_loss: 0.3805 - val_R_squared: 0.3532\n",
      "Epoch 14/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.3809 - R_squared: 0.3558 - val_loss: 0.3451 - val_R_squared: 0.4112\n",
      "Epoch 15/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3880 - R_squared: 0.3431 - val_loss: 0.3885 - val_R_squared: 0.3376\n",
      "Epoch 16/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.4170 - R_squared: 0.3041 - val_loss: 0.4176 - val_R_squared: 0.2649\n",
      "Epoch 17/600\n",
      "10/10 [==============================] - 1s 129ms/step - loss: 0.4252 - R_squared: 0.2883 - val_loss: 0.3475 - val_R_squared: 0.3186\n",
      "Epoch 18/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.4088 - R_squared: 0.3187 - val_loss: 0.3939 - val_R_squared: 0.3451\n",
      "Epoch 19/600\n",
      "10/10 [==============================] - 1s 134ms/step - loss: 0.4037 - R_squared: 0.3312 - val_loss: 0.4326 - val_R_squared: 0.2657\n",
      "Epoch 20/600\n",
      "10/10 [==============================] - 1s 130ms/step - loss: 0.3831 - R_squared: 0.3470 - val_loss: 0.5362 - val_R_squared: 0.3404\n",
      "Epoch 21/600\n",
      "10/10 [==============================] - 2s 151ms/step - loss: 0.3964 - R_squared: 0.3391 - val_loss: 0.4088 - val_R_squared: 0.3108\n",
      "Epoch 22/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.3844 - R_squared: 0.3610 - val_loss: 0.4152 - val_R_squared: 0.3194\n",
      "Epoch 23/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.3881 - R_squared: 0.3522 - val_loss: 0.3152 - val_R_squared: 0.4153\n",
      "Epoch 24/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.3676 - R_squared: 0.3749 - val_loss: 0.4245 - val_R_squared: 0.3150\n",
      "Epoch 25/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.3677 - R_squared: 0.3866 - val_loss: 0.3910 - val_R_squared: 0.3263\n",
      "Epoch 26/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.3600 - R_squared: 0.3871 - val_loss: 0.4560 - val_R_squared: 0.3655\n",
      "Epoch 27/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.3721 - R_squared: 0.3902 - val_loss: 0.4864 - val_R_squared: 0.2972\n",
      "Epoch 28/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3734 - R_squared: 0.3811 - val_loss: 0.3716 - val_R_squared: 0.3375\n",
      "Epoch 29/600\n",
      "10/10 [==============================] - 1s 139ms/step - loss: 0.3694 - R_squared: 0.3839 - val_loss: 0.3740 - val_R_squared: 0.3704\n",
      "Epoch 30/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3652 - R_squared: 0.3940 - val_loss: 0.3970 - val_R_squared: 0.3750\n",
      "Epoch 31/600\n",
      "10/10 [==============================] - 1s 145ms/step - loss: 0.3631 - R_squared: 0.3971 - val_loss: 0.4034 - val_R_squared: 0.4135\n",
      "Epoch 32/600\n",
      "10/10 [==============================] - 1s 142ms/step - loss: 0.3660 - R_squared: 0.3893 - val_loss: 0.3379 - val_R_squared: 0.4564\n",
      "Epoch 33/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3612 - R_squared: 0.3863 - val_loss: 0.2962 - val_R_squared: 0.4390\n",
      "Epoch 34/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3597 - R_squared: 0.4018 - val_loss: 0.3796 - val_R_squared: 0.3672\n",
      "Epoch 35/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3581 - R_squared: 0.3882 - val_loss: 0.3269 - val_R_squared: 0.4716\n",
      "Epoch 36/600\n",
      "10/10 [==============================] - 1s 132ms/step - loss: 0.3552 - R_squared: 0.3958 - val_loss: 0.4374 - val_R_squared: 0.3244\n",
      "Epoch 37/600\n",
      "10/10 [==============================] - 1s 133ms/step - loss: 0.3601 - R_squared: 0.3920 - val_loss: 0.3942 - val_R_squared: 0.3669\n",
      "Epoch 38/600\n",
      "10/10 [==============================] - 1s 140ms/step - loss: 0.3677 - R_squared: 0.3926 - val_loss: 0.4799 - val_R_squared: 0.3159\n",
      "Epoch 39/600\n",
      "10/10 [==============================] - 1s 137ms/step - loss: 0.3622 - R_squared: 0.3953 - val_loss: 0.3627 - val_R_squared: 0.3645\n",
      "Epoch 40/600\n",
      "10/10 [==============================] - 2s 155ms/step - loss: 0.3579 - R_squared: 0.3883 - val_loss: 0.3631 - val_R_squared: 0.3747\n",
      "Epoch 41/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.3562 - R_squared: 0.3988 - val_loss: 0.3626 - val_R_squared: 0.3230\n",
      "Epoch 42/600\n",
      "10/10 [==============================] - 1s 131ms/step - loss: 0.3416 - R_squared: 0.4090 - val_loss: 0.3192 - val_R_squared: 0.3565\n",
      "Epoch 43/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.3643 - R_squared: 0.3904 - val_loss: 0.2930 - val_R_squared: 0.4247\n",
      "Epoch 44/600\n",
      "10/10 [==============================] - 1s 141ms/step - loss: 0.3662 - R_squared: 0.3828 - val_loss: 0.3838 - val_R_squared: 0.2776\n",
      "Epoch 45/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.3576 - R_squared: 0.3799 - val_loss: 0.3393 - val_R_squared: 0.3668\n",
      "Epoch 46/600\n",
      "10/10 [==============================] - 1s 135ms/step - loss: 0.3666 - R_squared: 0.3891 - val_loss: 0.4880 - val_R_squared: 0.2871\n",
      "Epoch 47/600\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 0.3581 - R_squared: 0.3917 - val_loss: 0.3426 - val_R_squared: 0.3766\n",
      "Epoch 48/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3515 - R_squared: 0.4086 - val_loss: 0.3317 - val_R_squared: 0.4206\n",
      "Epoch 49/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.3590 - R_squared: 0.4044 - val_loss: 0.3226 - val_R_squared: 0.4294\n",
      "Epoch 50/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3530 - R_squared: 0.4107 - val_loss: 0.3399 - val_R_squared: 0.4537\n",
      "Epoch 51/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.3507 - R_squared: 0.4026 - val_loss: 0.3749 - val_R_squared: 0.3823\n",
      "Epoch 52/600\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 0.3499 - R_squared: 0.4020 - val_loss: 0.3373 - val_R_squared: 0.4009\n",
      "Epoch 53/600\n",
      "10/10 [==============================] - 1s 150ms/step - loss: 0.3531 - R_squared: 0.4015 - val_loss: 0.3465 - val_R_squared: 0.4469\n",
      "Epoch 54/600\n",
      "10/10 [==============================] - 1s 131ms/step - loss: 0.3589 - R_squared: 0.3986 - val_loss: 0.3185 - val_R_squared: 0.3676\n",
      "Epoch 55/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.3595 - R_squared: 0.3968 - val_loss: 0.3093 - val_R_squared: 0.4494\n",
      "Epoch 56/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3712 - R_squared: 0.3809 - val_loss: 0.3301 - val_R_squared: 0.3955\n",
      "Epoch 57/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3646 - R_squared: 0.3682 - val_loss: 0.3419 - val_R_squared: 0.3715\n",
      "Epoch 58/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.3616 - R_squared: 0.3806 - val_loss: 0.3459 - val_R_squared: 0.3917\n",
      "Epoch 59/600\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 0.3706 - R_squared: 0.3744 - val_loss: 0.5079 - val_R_squared: 0.2373\n",
      "Epoch 60/600\n",
      "10/10 [==============================] - 1s 131ms/step - loss: 0.3704 - R_squared: 0.3676 - val_loss: 0.3879 - val_R_squared: 0.3648\n",
      "Epoch 61/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.3566 - R_squared: 0.3878 - val_loss: 0.3911 - val_R_squared: 0.3038\n",
      "Epoch 62/600\n",
      "10/10 [==============================] - 1s 139ms/step - loss: 0.3721 - R_squared: 0.3830 - val_loss: 0.4303 - val_R_squared: 0.3792\n",
      "Epoch 63/600\n",
      "10/10 [==============================] - 1s 131ms/step - loss: 0.3843 - R_squared: 0.3634 - val_loss: 0.3374 - val_R_squared: 0.3203\n",
      "Epoch 64/600\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 0.3898 - R_squared: 0.3374 - val_loss: 0.3718 - val_R_squared: 0.3402\n",
      "Epoch 65/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.3993 - R_squared: 0.3400 - val_loss: 0.4233 - val_R_squared: 0.3232\n",
      "Epoch 66/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3744 - R_squared: 0.3536 - val_loss: 0.3352 - val_R_squared: 0.3456\n",
      "Epoch 67/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.3922 - R_squared: 0.3445 - val_loss: 0.4138 - val_R_squared: 0.3023\n",
      "Epoch 68/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.3855 - R_squared: 0.3537 - val_loss: 0.3643 - val_R_squared: 0.3826\n",
      "Epoch 69/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.3849 - R_squared: 0.3583 - val_loss: 0.3567 - val_R_squared: 0.3450\n",
      "Epoch 70/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3748 - R_squared: 0.3703 - val_loss: 0.3783 - val_R_squared: 0.3534\n",
      "Epoch 71/600\n",
      "10/10 [==============================] - 1s 142ms/step - loss: 0.3791 - R_squared: 0.3586 - val_loss: 0.3674 - val_R_squared: 0.3765\n",
      "Epoch 72/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.3699 - R_squared: 0.3730 - val_loss: 0.5095 - val_R_squared: 0.2466\n",
      "Epoch 73/600\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 0.3741 - R_squared: 0.3780 - val_loss: 0.4543 - val_R_squared: 0.3045\n",
      "Epoch 74/600\n",
      "10/10 [==============================] - 1s 134ms/step - loss: 0.3820 - R_squared: 0.3648 - val_loss: 0.4624 - val_R_squared: 0.3716\n",
      "Epoch 75/600\n",
      "10/10 [==============================] - 1s 129ms/step - loss: 0.3808 - R_squared: 0.3617 - val_loss: 0.4522 - val_R_squared: 0.2925\n",
      "Epoch 76/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.3726 - R_squared: 0.3805 - val_loss: 0.3750 - val_R_squared: 0.3684\n",
      "Epoch 77/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3724 - R_squared: 0.3805 - val_loss: 0.3646 - val_R_squared: 0.4246\n",
      "Epoch 78/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3660 - R_squared: 0.3805 - val_loss: 0.3266 - val_R_squared: 0.4128\n",
      "Epoch 79/600\n",
      "10/10 [==============================] - 1s 141ms/step - loss: 0.3673 - R_squared: 0.3874 - val_loss: 0.4077 - val_R_squared: 0.2959\n",
      "Epoch 80/600\n",
      "10/10 [==============================] - 2s 164ms/step - loss: 0.3609 - R_squared: 0.3950 - val_loss: 0.3517 - val_R_squared: 0.4106\n",
      "Epoch 81/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.3450 - R_squared: 0.4160 - val_loss: 0.3560 - val_R_squared: 0.2919\n",
      "Epoch 82/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3616 - R_squared: 0.4039 - val_loss: 0.3416 - val_R_squared: 0.4031\n",
      "Epoch 83/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.3633 - R_squared: 0.3941 - val_loss: 0.3775 - val_R_squared: 0.3930\n",
      "Epoch 84/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.3587 - R_squared: 0.4030 - val_loss: 0.4270 - val_R_squared: 0.3971\n",
      "Epoch 85/600\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 0.3553 - R_squared: 0.4019 - val_loss: 0.3534 - val_R_squared: 0.3939\n",
      "Epoch 86/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3654 - R_squared: 0.3956 - val_loss: 0.3657 - val_R_squared: 0.3794\n",
      "Epoch 87/600\n",
      "10/10 [==============================] - 1s 129ms/step - loss: 0.3578 - R_squared: 0.3985 - val_loss: 0.3217 - val_R_squared: 0.4173\n",
      "Epoch 88/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.3555 - R_squared: 0.4052 - val_loss: 0.3955 - val_R_squared: 0.3587\n",
      "Epoch 89/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.3535 - R_squared: 0.4080 - val_loss: 0.3744 - val_R_squared: 0.3692\n",
      "Epoch 90/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3517 - R_squared: 0.4134 - val_loss: 0.3365 - val_R_squared: 0.3698\n",
      "Epoch 91/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3467 - R_squared: 0.4207 - val_loss: 0.4744 - val_R_squared: 0.3253\n",
      "Epoch 92/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3253 - R_squared: 0.4315 - val_loss: 0.3406 - val_R_squared: 0.3762\n",
      "Epoch 93/600\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3455 - R_squared: 0.424 - 1s 119ms/step - loss: 0.3455 - R_squared: 0.4242 - val_loss: 0.3499 - val_R_squared: 0.4048\n",
      "Epoch 94/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.3507 - R_squared: 0.4043 - val_loss: 0.3609 - val_R_squared: 0.4225\n",
      "Epoch 95/600\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 0.3495 - R_squared: 0.4105 - val_loss: 0.3522 - val_R_squared: 0.4415\n",
      "Epoch 96/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3502 - R_squared: 0.4147 - val_loss: 0.3331 - val_R_squared: 0.4481\n",
      "Epoch 97/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3423 - R_squared: 0.4209 - val_loss: 0.4111 - val_R_squared: 0.2988\n",
      "Epoch 98/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.3500 - R_squared: 0.4135 - val_loss: 0.3517 - val_R_squared: 0.4223\n",
      "Epoch 99/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3224 - R_squared: 0.4392 - val_loss: 0.3236 - val_R_squared: 0.3834\n",
      "Epoch 100/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3267 - R_squared: 0.4359 - val_loss: 0.3359 - val_R_squared: 0.4254\n",
      "Epoch 101/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3658 - R_squared: 0.3849 - val_loss: 0.3993 - val_R_squared: 0.2081\n",
      "Epoch 102/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.3830 - R_squared: 0.3538 - val_loss: 0.3966 - val_R_squared: 0.3410\n",
      "Epoch 103/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.3619 - R_squared: 0.3733 - val_loss: 0.3331 - val_R_squared: 0.4012\n",
      "Epoch 104/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.3795 - R_squared: 0.3664 - val_loss: 0.3241 - val_R_squared: 0.3118\n",
      "Epoch 105/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3718 - R_squared: 0.3752 - val_loss: 0.3610 - val_R_squared: 0.3815\n",
      "Epoch 106/600\n",
      "10/10 [==============================] - 1s 131ms/step - loss: 0.3726 - R_squared: 0.3816 - val_loss: 0.3509 - val_R_squared: 0.3535\n",
      "Epoch 107/600\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 0.3661 - R_squared: 0.3878 - val_loss: 0.3251 - val_R_squared: 0.4205\n",
      "Epoch 108/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3517 - R_squared: 0.4097 - val_loss: 0.3271 - val_R_squared: 0.4404\n",
      "Epoch 109/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.3509 - R_squared: 0.4123 - val_loss: 0.3312 - val_R_squared: 0.3642\n",
      "Epoch 110/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.3540 - R_squared: 0.4161 - val_loss: 0.3585 - val_R_squared: 0.3311\n",
      "Epoch 111/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3501 - R_squared: 0.4200 - val_loss: 0.4028 - val_R_squared: 0.3909\n",
      "Epoch 112/600\n",
      "10/10 [==============================] - 1s 131ms/step - loss: 0.3461 - R_squared: 0.4188 - val_loss: 0.3843 - val_R_squared: 0.3763\n",
      "Epoch 113/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3481 - R_squared: 0.4247 - val_loss: 0.4613 - val_R_squared: 0.3802\n",
      "Epoch 114/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3571 - R_squared: 0.4039 - val_loss: 0.3221 - val_R_squared: 0.3913\n",
      "Epoch 115/600\n",
      "10/10 [==============================] - 1s 134ms/step - loss: 0.3466 - R_squared: 0.4197 - val_loss: 0.3167 - val_R_squared: 0.4223\n",
      "Epoch 116/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.3414 - R_squared: 0.4210 - val_loss: 0.3808 - val_R_squared: 0.3968\n",
      "Epoch 117/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.3424 - R_squared: 0.4309 - val_loss: 0.3125 - val_R_squared: 0.4475\n",
      "Epoch 118/600\n",
      "10/10 [==============================] - 1s 137ms/step - loss: 0.3412 - R_squared: 0.4206 - val_loss: 0.4600 - val_R_squared: 0.3818\n",
      "Epoch 119/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.3248 - R_squared: 0.4387 - val_loss: 0.3272 - val_R_squared: 0.3963\n",
      "Epoch 120/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3374 - R_squared: 0.4394 - val_loss: 0.3545 - val_R_squared: 0.3935\n",
      "Epoch 121/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3453 - R_squared: 0.4244 - val_loss: 0.3197 - val_R_squared: 0.4305\n",
      "Epoch 122/600\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 0.3368 - R_squared: 0.4284 - val_loss: 0.3228 - val_R_squared: 0.4210\n",
      "Epoch 123/600\n",
      "10/10 [==============================] - 1s 150ms/step - loss: 0.3388 - R_squared: 0.4331 - val_loss: 0.3207 - val_R_squared: 0.4430\n",
      "Epoch 124/600\n",
      "10/10 [==============================] - 1s 130ms/step - loss: 0.3352 - R_squared: 0.4352 - val_loss: 0.4423 - val_R_squared: 0.3889\n",
      "Epoch 125/600\n",
      "10/10 [==============================] - 1s 131ms/step - loss: 0.3422 - R_squared: 0.4317 - val_loss: 0.3334 - val_R_squared: 0.4036\n",
      "Epoch 126/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3624 - R_squared: 0.4019 - val_loss: 0.4427 - val_R_squared: 0.2836\n",
      "Epoch 127/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.3608 - R_squared: 0.4015 - val_loss: 0.4013 - val_R_squared: 0.2855\n",
      "Epoch 128/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3444 - R_squared: 0.4287 - val_loss: 0.3634 - val_R_squared: 0.3889\n",
      "Epoch 129/600\n",
      "10/10 [==============================] - 1s 130ms/step - loss: 0.3357 - R_squared: 0.4385 - val_loss: 0.3072 - val_R_squared: 0.4024\n",
      "Epoch 130/600\n",
      "10/10 [==============================] - 1s 139ms/step - loss: 0.3409 - R_squared: 0.4325 - val_loss: 0.4786 - val_R_squared: 0.3295\n",
      "Epoch 131/600\n",
      "10/10 [==============================] - 2s 152ms/step - loss: 0.3307 - R_squared: 0.4326 - val_loss: 0.3333 - val_R_squared: 0.4389\n",
      "Epoch 132/600\n",
      "10/10 [==============================] - 1s 143ms/step - loss: 0.3424 - R_squared: 0.4250 - val_loss: 0.3948 - val_R_squared: 0.3729\n",
      "Epoch 133/600\n",
      "10/10 [==============================] - 1s 140ms/step - loss: 0.3349 - R_squared: 0.4394 - val_loss: 0.3633 - val_R_squared: 0.4350\n",
      "Epoch 134/600\n",
      "10/10 [==============================] - 1s 141ms/step - loss: 0.3433 - R_squared: 0.4399 - val_loss: 0.3539 - val_R_squared: 0.3920\n",
      "Epoch 135/600\n",
      "10/10 [==============================] - 1s 129ms/step - loss: 0.3399 - R_squared: 0.4289 - val_loss: 0.4579 - val_R_squared: 0.3047\n",
      "Epoch 136/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3440 - R_squared: 0.4267 - val_loss: 0.3039 - val_R_squared: 0.4300\n",
      "Epoch 137/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3436 - R_squared: 0.4225 - val_loss: 0.3625 - val_R_squared: 0.3334\n",
      "Epoch 138/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.3264 - R_squared: 0.4288 - val_loss: 0.2680 - val_R_squared: 0.4922\n",
      "Epoch 139/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3365 - R_squared: 0.4344 - val_loss: 0.3673 - val_R_squared: 0.2816\n",
      "Epoch 140/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3364 - R_squared: 0.4342 - val_loss: 0.3594 - val_R_squared: 0.3481\n",
      "Epoch 141/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3294 - R_squared: 0.4428 - val_loss: 0.3304 - val_R_squared: 0.4323\n",
      "Epoch 142/600\n",
      "10/10 [==============================] - 1s 141ms/step - loss: 0.3305 - R_squared: 0.4370 - val_loss: 0.3316 - val_R_squared: 0.4227\n",
      "Epoch 143/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.3517 - R_squared: 0.4180 - val_loss: 0.4416 - val_R_squared: 0.2837\n",
      "Epoch 144/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3722 - R_squared: 0.3739 - val_loss: 0.5296 - val_R_squared: 0.3441\n",
      "Epoch 145/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3585 - R_squared: 0.3943 - val_loss: 0.3775 - val_R_squared: 0.3518\n",
      "Epoch 146/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.3470 - R_squared: 0.4145 - val_loss: 0.3485 - val_R_squared: 0.4216\n",
      "Epoch 147/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3605 - R_squared: 0.3966 - val_loss: 0.4562 - val_R_squared: 0.3714\n",
      "Epoch 148/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3531 - R_squared: 0.4152 - val_loss: 0.5399 - val_R_squared: 0.3101\n",
      "Epoch 149/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3420 - R_squared: 0.4217 - val_loss: 0.3563 - val_R_squared: 0.3580\n",
      "Epoch 150/600\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 0.3355 - R_squared: 0.4232 - val_loss: 0.3361 - val_R_squared: 0.4116\n",
      "Epoch 151/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3205 - R_squared: 0.4414 - val_loss: 0.3646 - val_R_squared: 0.3760\n",
      "Epoch 152/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3352 - R_squared: 0.4303 - val_loss: 0.3302 - val_R_squared: 0.4229\n",
      "Epoch 153/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3459 - R_squared: 0.4249 - val_loss: 0.3144 - val_R_squared: 0.4058\n",
      "Epoch 154/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3249 - R_squared: 0.4394 - val_loss: 0.3810 - val_R_squared: 0.4143\n",
      "Epoch 155/600\n",
      "10/10 [==============================] - 2s 151ms/step - loss: 0.3474 - R_squared: 0.4161 - val_loss: 0.3333 - val_R_squared: 0.4135\n",
      "Epoch 156/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3510 - R_squared: 0.4170 - val_loss: 0.4898 - val_R_squared: 0.3321\n",
      "Epoch 157/600\n",
      "10/10 [==============================] - 1s 135ms/step - loss: 0.3439 - R_squared: 0.4207 - val_loss: 0.3462 - val_R_squared: 0.3584\n",
      "Epoch 158/600\n",
      "10/10 [==============================] - 1s 130ms/step - loss: 0.3521 - R_squared: 0.4162 - val_loss: 0.4497 - val_R_squared: 0.3749\n",
      "Epoch 159/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.3436 - R_squared: 0.4247 - val_loss: 0.2998 - val_R_squared: 0.4079\n",
      "Epoch 160/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3509 - R_squared: 0.4174 - val_loss: 0.3342 - val_R_squared: 0.3668\n",
      "Epoch 161/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3477 - R_squared: 0.4121 - val_loss: 0.3654 - val_R_squared: 0.3890\n",
      "Epoch 162/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3489 - R_squared: 0.4135 - val_loss: 0.3261 - val_R_squared: 0.4243\n",
      "Epoch 163/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.3441 - R_squared: 0.4269 - val_loss: 0.3098 - val_R_squared: 0.4027\n",
      "Epoch 164/600\n",
      "10/10 [==============================] - 1s 136ms/step - loss: 0.3459 - R_squared: 0.4258 - val_loss: 0.3622 - val_R_squared: 0.4180\n",
      "Epoch 165/600\n",
      "10/10 [==============================] - 2s 156ms/step - loss: 0.3396 - R_squared: 0.4292 - val_loss: 0.3189 - val_R_squared: 0.4058\n",
      "Epoch 166/600\n",
      "10/10 [==============================] - 2s 167ms/step - loss: 0.3207 - R_squared: 0.4483 - val_loss: 0.3011 - val_R_squared: 0.4626\n",
      "Epoch 167/600\n",
      "10/10 [==============================] - 1s 138ms/step - loss: 0.3340 - R_squared: 0.4363 - val_loss: 0.3687 - val_R_squared: 0.4098\n",
      "Epoch 168/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.3357 - R_squared: 0.4402 - val_loss: 0.3365 - val_R_squared: 0.4562\n",
      "Epoch 169/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3196 - R_squared: 0.4410 - val_loss: 0.4360 - val_R_squared: 0.3844\n",
      "Epoch 170/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3279 - R_squared: 0.4408 - val_loss: 0.3090 - val_R_squared: 0.3656\n",
      "Epoch 171/600\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 0.3372 - R_squared: 0.4331 - val_loss: 0.3222 - val_R_squared: 0.3483\n",
      "Epoch 172/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.3265 - R_squared: 0.4479 - val_loss: 0.4409 - val_R_squared: 0.3977\n",
      "Epoch 173/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3397 - R_squared: 0.4284 - val_loss: 0.3403 - val_R_squared: 0.4096\n",
      "Epoch 174/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.3372 - R_squared: 0.4316 - val_loss: 0.3746 - val_R_squared: 0.3658\n",
      "Epoch 175/600\n",
      "10/10 [==============================] - 1s 146ms/step - loss: 0.3478 - R_squared: 0.4204 - val_loss: 0.3662 - val_R_squared: 0.3798\n",
      "Epoch 176/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.3613 - R_squared: 0.3949 - val_loss: 0.3804 - val_R_squared: 0.3558\n",
      "Epoch 177/600\n",
      "10/10 [==============================] - 2s 158ms/step - loss: 0.3710 - R_squared: 0.3817 - val_loss: 0.3444 - val_R_squared: 0.3681\n",
      "Epoch 178/600\n",
      "10/10 [==============================] - 1s 131ms/step - loss: 0.3609 - R_squared: 0.4017 - val_loss: 0.5006 - val_R_squared: 0.3009\n",
      "Epoch 179/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.3633 - R_squared: 0.3908 - val_loss: 0.4509 - val_R_squared: 0.2882\n",
      "Epoch 180/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3344 - R_squared: 0.4282 - val_loss: 0.3662 - val_R_squared: 0.3713\n",
      "Epoch 181/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3401 - R_squared: 0.4218 - val_loss: 0.3615 - val_R_squared: 0.4201\n",
      "Epoch 182/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3276 - R_squared: 0.4462 - val_loss: 0.3175 - val_R_squared: 0.4234\n",
      "Epoch 183/600\n",
      "10/10 [==============================] - 1s 144ms/step - loss: 0.3317 - R_squared: 0.4403 - val_loss: 0.3752 - val_R_squared: 0.3502\n",
      "Epoch 184/600\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 0.3356 - R_squared: 0.4469 - val_loss: 0.3019 - val_R_squared: 0.4476\n",
      "Epoch 185/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3308 - R_squared: 0.4441 - val_loss: 0.3403 - val_R_squared: 0.3920\n",
      "Epoch 186/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3288 - R_squared: 0.4461 - val_loss: 0.3542 - val_R_squared: 0.4140\n",
      "Epoch 187/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3352 - R_squared: 0.4394 - val_loss: 0.4233 - val_R_squared: 0.3977\n",
      "Epoch 188/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3219 - R_squared: 0.4511 - val_loss: 0.3442 - val_R_squared: 0.4230\n",
      "Epoch 189/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.3398 - R_squared: 0.4236 - val_loss: 0.3330 - val_R_squared: 0.3632\n",
      "Epoch 190/600\n",
      "10/10 [==============================] - 2s 154ms/step - loss: 0.3275 - R_squared: 0.4304 - val_loss: 0.3567 - val_R_squared: 0.4282\n",
      "Epoch 191/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.3390 - R_squared: 0.4295 - val_loss: 0.2988 - val_R_squared: 0.4583\n",
      "Epoch 192/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.3453 - R_squared: 0.4222 - val_loss: 0.3554 - val_R_squared: 0.3880\n",
      "Epoch 193/600\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 0.3509 - R_squared: 0.4125 - val_loss: 0.3100 - val_R_squared: 0.3817\n",
      "Epoch 194/600\n",
      "10/10 [==============================] - 1s 129ms/step - loss: 0.3588 - R_squared: 0.3838 - val_loss: 0.3291 - val_R_squared: 0.3473\n",
      "Epoch 195/600\n",
      "10/10 [==============================] - 1s 141ms/step - loss: 0.3499 - R_squared: 0.3866 - val_loss: 0.4046 - val_R_squared: 0.3503\n",
      "Epoch 196/600\n",
      "10/10 [==============================] - 1s 130ms/step - loss: 0.3626 - R_squared: 0.3853 - val_loss: 0.3549 - val_R_squared: 0.4219\n",
      "Epoch 197/600\n",
      "10/10 [==============================] - 2s 141ms/step - loss: 0.3686 - R_squared: 0.3855 - val_loss: 0.3604 - val_R_squared: 0.3543\n",
      "Epoch 198/600\n",
      "10/10 [==============================] - 1s 138ms/step - loss: 0.3523 - R_squared: 0.4040 - val_loss: 0.3479 - val_R_squared: 0.4085\n",
      "Epoch 199/600\n",
      "10/10 [==============================] - 1s 145ms/step - loss: 0.3510 - R_squared: 0.4090 - val_loss: 0.3835 - val_R_squared: 0.3599\n",
      "Epoch 200/600\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 0.3531 - R_squared: 0.4055 - val_loss: 0.3196 - val_R_squared: 0.4307\n",
      "Epoch 201/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3547 - R_squared: 0.4076 - val_loss: 0.2987 - val_R_squared: 0.4199\n",
      "Epoch 202/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3570 - R_squared: 0.3990 - val_loss: 0.3643 - val_R_squared: 0.3744\n",
      "Epoch 203/600\n",
      "10/10 [==============================] - 1s 133ms/step - loss: 0.3297 - R_squared: 0.4260 - val_loss: 0.3472 - val_R_squared: 0.4095\n",
      "Epoch 204/600\n",
      "10/10 [==============================] - 1s 138ms/step - loss: 0.3414 - R_squared: 0.4196 - val_loss: 0.3235 - val_R_squared: 0.3399\n",
      "Epoch 205/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.3490 - R_squared: 0.4139 - val_loss: 0.3693 - val_R_squared: 0.3330\n",
      "Epoch 206/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3288 - R_squared: 0.4350 - val_loss: 0.4755 - val_R_squared: 0.3771\n",
      "Epoch 207/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3438 - R_squared: 0.4301 - val_loss: 0.4021 - val_R_squared: 0.3087\n",
      "Epoch 208/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3520 - R_squared: 0.4112 - val_loss: 0.3984 - val_R_squared: 0.3682\n",
      "Epoch 209/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3458 - R_squared: 0.4185 - val_loss: 0.4646 - val_R_squared: 0.2988\n",
      "Epoch 210/600\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3489 - R_squared: 0.418 - 1s 117ms/step - loss: 0.3489 - R_squared: 0.4188 - val_loss: 0.3635 - val_R_squared: 0.3954\n",
      "Epoch 211/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3229 - R_squared: 0.4380 - val_loss: 0.4768 - val_R_squared: 0.3582\n",
      "Epoch 212/600\n",
      "10/10 [==============================] - 1s 135ms/step - loss: 0.3461 - R_squared: 0.4253 - val_loss: 0.3428 - val_R_squared: 0.4362\n",
      "Epoch 213/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.3403 - R_squared: 0.4328 - val_loss: 0.3390 - val_R_squared: 0.4058\n",
      "Epoch 214/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3410 - R_squared: 0.4347 - val_loss: 0.3982 - val_R_squared: 0.3506\n",
      "Epoch 215/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3383 - R_squared: 0.4370 - val_loss: 0.3209 - val_R_squared: 0.4942\n",
      "Epoch 216/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3257 - R_squared: 0.4499 - val_loss: 0.4623 - val_R_squared: 0.3030\n",
      "Epoch 217/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3380 - R_squared: 0.4332 - val_loss: 0.3513 - val_R_squared: 0.4024\n",
      "Epoch 218/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3167 - R_squared: 0.4620 - val_loss: 0.3924 - val_R_squared: 0.3484\n",
      "Epoch 219/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3238 - R_squared: 0.4529 - val_loss: 0.3490 - val_R_squared: 0.3910\n",
      "Epoch 220/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.3322 - R_squared: 0.4382 - val_loss: 0.4485 - val_R_squared: 0.3098\n",
      "Epoch 221/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.3170 - R_squared: 0.4592 - val_loss: 0.3222 - val_R_squared: 0.4350\n",
      "Epoch 222/600\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.3322 - R_squared: 0.4359 - val_loss: 0.4409 - val_R_squared: 0.4045\n",
      "Epoch 223/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3347 - R_squared: 0.4455 - val_loss: 0.3431 - val_R_squared: 0.4284\n",
      "Epoch 224/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3045 - R_squared: 0.4720 - val_loss: 0.3434 - val_R_squared: 0.4401\n",
      "Epoch 225/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3200 - R_squared: 0.4645 - val_loss: 0.3909 - val_R_squared: 0.4218\n",
      "Epoch 226/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2984 - R_squared: 0.4770 - val_loss: 0.3377 - val_R_squared: 0.4853\n",
      "Epoch 227/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3104 - R_squared: 0.4738 - val_loss: 0.3761 - val_R_squared: 0.4225\n",
      "Epoch 228/600\n",
      "10/10 [==============================] - 1s 135ms/step - loss: 0.3139 - R_squared: 0.4767 - val_loss: 0.2998 - val_R_squared: 0.3973\n",
      "Epoch 229/600\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.3127 - R_squared: 0.4850 - val_loss: 0.2808 - val_R_squared: 0.4797\n",
      "Epoch 230/600\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.3067 - R_squared: 0.4888 - val_loss: 0.2899 - val_R_squared: 0.4807\n",
      "Epoch 231/600\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.3000 - R_squared: 0.4900 - val_loss: 0.3013 - val_R_squared: 0.4622\n",
      "Epoch 232/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3032 - R_squared: 0.4904 - val_loss: 0.3191 - val_R_squared: 0.4372\n",
      "Epoch 233/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3021 - R_squared: 0.4950 - val_loss: 0.2815 - val_R_squared: 0.4972\n",
      "Epoch 234/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3033 - R_squared: 0.4866 - val_loss: 0.3137 - val_R_squared: 0.4227\n",
      "Epoch 235/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3099 - R_squared: 0.4812 - val_loss: 0.4257 - val_R_squared: 0.3920\n",
      "Epoch 236/600\n",
      "10/10 [==============================] - 1s 131ms/step - loss: 0.3057 - R_squared: 0.4797 - val_loss: 0.3054 - val_R_squared: 0.4819\n",
      "Epoch 237/600\n",
      "10/10 [==============================] - 1s 133ms/step - loss: 0.3174 - R_squared: 0.4770 - val_loss: 0.2997 - val_R_squared: 0.4439\n",
      "Epoch 238/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.3164 - R_squared: 0.4687 - val_loss: 0.2953 - val_R_squared: 0.5094\n",
      "Epoch 239/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2943 - R_squared: 0.4859 - val_loss: 0.3146 - val_R_squared: 0.4656\n",
      "Epoch 240/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2922 - R_squared: 0.4996 - val_loss: 0.3053 - val_R_squared: 0.4857\n",
      "Epoch 241/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3115 - R_squared: 0.4733 - val_loss: 0.3110 - val_R_squared: 0.4257\n",
      "Epoch 242/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3087 - R_squared: 0.4889 - val_loss: 0.3801 - val_R_squared: 0.4230\n",
      "Epoch 243/600\n",
      "10/10 [==============================] - 1s 129ms/step - loss: 0.3098 - R_squared: 0.4875 - val_loss: 0.2905 - val_R_squared: 0.4597\n",
      "Epoch 244/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.3085 - R_squared: 0.4852 - val_loss: 0.4344 - val_R_squared: 0.3021\n",
      "Epoch 245/600\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 0.3080 - R_squared: 0.4803 - val_loss: 0.2783 - val_R_squared: 0.5105\n",
      "Epoch 246/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.3034 - R_squared: 0.4929 - val_loss: 0.3967 - val_R_squared: 0.4695\n",
      "Epoch 247/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2920 - R_squared: 0.5087 - val_loss: 0.4257 - val_R_squared: 0.4166\n",
      "Epoch 248/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3018 - R_squared: 0.4941 - val_loss: 0.3015 - val_R_squared: 0.4473\n",
      "Epoch 249/600\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2973 - R_squared: 0.5018 - val_loss: 0.2911 - val_R_squared: 0.4607\n",
      "Epoch 250/600\n",
      "10/10 [==============================] - 1s 129ms/step - loss: 0.2900 - R_squared: 0.5132 - val_loss: 0.2561 - val_R_squared: 0.5419\n",
      "Epoch 251/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2990 - R_squared: 0.4979 - val_loss: 0.2806 - val_R_squared: 0.5179\n",
      "Epoch 252/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.2937 - R_squared: 0.5035 - val_loss: 0.3821 - val_R_squared: 0.4682\n",
      "Epoch 253/600\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 0.2896 - R_squared: 0.5142 - val_loss: 0.2716 - val_R_squared: 0.5306\n",
      "Epoch 254/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2943 - R_squared: 0.5116 - val_loss: 0.2770 - val_R_squared: 0.4770\n",
      "Epoch 255/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2864 - R_squared: 0.5202 - val_loss: 0.2732 - val_R_squared: 0.5180\n",
      "Epoch 256/600\n",
      "10/10 [==============================] - 1s 139ms/step - loss: 0.2939 - R_squared: 0.5111 - val_loss: 0.2631 - val_R_squared: 0.4944\n",
      "Epoch 257/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2898 - R_squared: 0.5078 - val_loss: 0.2824 - val_R_squared: 0.4990\n",
      "Epoch 258/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2917 - R_squared: 0.5100 - val_loss: 0.3047 - val_R_squared: 0.5146\n",
      "Epoch 259/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2946 - R_squared: 0.5038 - val_loss: 0.4031 - val_R_squared: 0.4001\n",
      "Epoch 260/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2989 - R_squared: 0.5015 - val_loss: 0.2926 - val_R_squared: 0.4780\n",
      "Epoch 261/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3007 - R_squared: 0.4923 - val_loss: 0.3316 - val_R_squared: 0.4632\n",
      "Epoch 262/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3045 - R_squared: 0.4898 - val_loss: 0.2996 - val_R_squared: 0.4683\n",
      "Epoch 263/600\n",
      "10/10 [==============================] - 1s 132ms/step - loss: 0.3121 - R_squared: 0.4810 - val_loss: 0.2870 - val_R_squared: 0.4952\n",
      "Epoch 264/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2982 - R_squared: 0.4975 - val_loss: 0.3230 - val_R_squared: 0.4318\n",
      "Epoch 265/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3023 - R_squared: 0.4923 - val_loss: 0.2981 - val_R_squared: 0.4985\n",
      "Epoch 266/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2984 - R_squared: 0.5073 - val_loss: 0.2799 - val_R_squared: 0.5068\n",
      "Epoch 267/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2844 - R_squared: 0.5143 - val_loss: 0.4005 - val_R_squared: 0.3443\n",
      "Epoch 268/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3138 - R_squared: 0.4675 - val_loss: 0.3596 - val_R_squared: 0.4149\n",
      "Epoch 269/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3426 - R_squared: 0.4283 - val_loss: 0.3534 - val_R_squared: 0.3839\n",
      "Epoch 270/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.3323 - R_squared: 0.4450 - val_loss: 0.3374 - val_R_squared: 0.3915\n",
      "Epoch 271/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.3209 - R_squared: 0.4567 - val_loss: 0.3531 - val_R_squared: 0.3428\n",
      "Epoch 272/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3237 - R_squared: 0.4578 - val_loss: 0.4637 - val_R_squared: 0.3937\n",
      "Epoch 273/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.3236 - R_squared: 0.4555 - val_loss: 0.4006 - val_R_squared: 0.3967\n",
      "Epoch 274/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.3210 - R_squared: 0.4604 - val_loss: 0.3340 - val_R_squared: 0.3831\n",
      "Epoch 275/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.3191 - R_squared: 0.4625 - val_loss: 0.4317 - val_R_squared: 0.3089\n",
      "Epoch 276/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.3020 - R_squared: 0.4751 - val_loss: 0.3389 - val_R_squared: 0.4404\n",
      "Epoch 277/600\n",
      "10/10 [==============================] - 1s 138ms/step - loss: 0.3134 - R_squared: 0.4730 - val_loss: 0.3884 - val_R_squared: 0.4452\n",
      "Epoch 278/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.3088 - R_squared: 0.4881 - val_loss: 0.4241 - val_R_squared: 0.3617\n",
      "Epoch 279/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.3144 - R_squared: 0.4784 - val_loss: 0.3070 - val_R_squared: 0.4547\n",
      "Epoch 280/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.3052 - R_squared: 0.4957 - val_loss: 0.2874 - val_R_squared: 0.4822\n",
      "Epoch 281/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2821 - R_squared: 0.5172 - val_loss: 0.2747 - val_R_squared: 0.5152\n",
      "Epoch 282/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2913 - R_squared: 0.5097 - val_loss: 0.2825 - val_R_squared: 0.5617\n",
      "Epoch 283/600\n",
      "10/10 [==============================] - 1s 132ms/step - loss: 0.2766 - R_squared: 0.5260 - val_loss: 0.2764 - val_R_squared: 0.4897\n",
      "Epoch 284/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2735 - R_squared: 0.5169 - val_loss: 0.2997 - val_R_squared: 0.4818\n",
      "Epoch 285/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2846 - R_squared: 0.5204 - val_loss: 0.2518 - val_R_squared: 0.5363\n",
      "Epoch 286/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2901 - R_squared: 0.5103 - val_loss: 0.3187 - val_R_squared: 0.4720\n",
      "Epoch 287/600\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2911 - R_squared: 0.5042 - val_loss: 0.2566 - val_R_squared: 0.5163\n",
      "Epoch 288/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2907 - R_squared: 0.5098 - val_loss: 0.2515 - val_R_squared: 0.5740\n",
      "Epoch 289/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2854 - R_squared: 0.5187 - val_loss: 0.2795 - val_R_squared: 0.4389\n",
      "Epoch 290/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2666 - R_squared: 0.5388 - val_loss: 0.3009 - val_R_squared: 0.5155\n",
      "Epoch 291/600\n",
      "10/10 [==============================] - 1s 131ms/step - loss: 0.2842 - R_squared: 0.5286 - val_loss: 0.2994 - val_R_squared: 0.4825\n",
      "Epoch 292/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2889 - R_squared: 0.5151 - val_loss: 0.3248 - val_R_squared: 0.4740\n",
      "Epoch 293/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2820 - R_squared: 0.5231 - val_loss: 0.2795 - val_R_squared: 0.5293\n",
      "Epoch 294/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2794 - R_squared: 0.5227 - val_loss: 0.2981 - val_R_squared: 0.4976\n",
      "Epoch 295/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2794 - R_squared: 0.5331 - val_loss: 0.3973 - val_R_squared: 0.3895\n",
      "Epoch 296/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.2862 - R_squared: 0.5292 - val_loss: 0.3008 - val_R_squared: 0.4567\n",
      "Epoch 297/600\n",
      "10/10 [==============================] - 1s 131ms/step - loss: 0.2809 - R_squared: 0.5264 - val_loss: 0.3935 - val_R_squared: 0.3819\n",
      "Epoch 298/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2669 - R_squared: 0.5371 - val_loss: 0.3012 - val_R_squared: 0.5289\n",
      "Epoch 299/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2754 - R_squared: 0.5273 - val_loss: 0.3226 - val_R_squared: 0.5047\n",
      "Epoch 300/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2714 - R_squared: 0.5285 - val_loss: 0.2971 - val_R_squared: 0.5073\n",
      "Epoch 301/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.2734 - R_squared: 0.5340 - val_loss: 0.2871 - val_R_squared: 0.4948\n",
      "Epoch 302/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2854 - R_squared: 0.5216 - val_loss: 0.2745 - val_R_squared: 0.4964\n",
      "Epoch 303/600\n",
      "10/10 [==============================] - 1s 132ms/step - loss: 0.2758 - R_squared: 0.5328 - val_loss: 0.2574 - val_R_squared: 0.5682\n",
      "Epoch 304/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2770 - R_squared: 0.5285 - val_loss: 0.3098 - val_R_squared: 0.5041\n",
      "Epoch 305/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2762 - R_squared: 0.5378 - val_loss: 0.2690 - val_R_squared: 0.5678\n",
      "Epoch 306/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2758 - R_squared: 0.5421 - val_loss: 0.2960 - val_R_squared: 0.4966\n",
      "Epoch 307/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2726 - R_squared: 0.5443 - val_loss: 0.2554 - val_R_squared: 0.5122\n",
      "Epoch 308/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.2679 - R_squared: 0.5549 - val_loss: 0.2734 - val_R_squared: 0.5153\n",
      "Epoch 309/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2727 - R_squared: 0.5388 - val_loss: 0.2790 - val_R_squared: 0.5082\n",
      "Epoch 310/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2695 - R_squared: 0.5520 - val_loss: 0.3484 - val_R_squared: 0.4476\n",
      "Epoch 311/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2572 - R_squared: 0.5602 - val_loss: 0.2817 - val_R_squared: 0.5059\n",
      "Epoch 312/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2710 - R_squared: 0.5422 - val_loss: 0.2927 - val_R_squared: 0.4842\n",
      "Epoch 313/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2597 - R_squared: 0.5533 - val_loss: 0.2627 - val_R_squared: 0.5257\n",
      "Epoch 314/600\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 0.2738 - R_squared: 0.5326 - val_loss: 0.2370 - val_R_squared: 0.5612\n",
      "Epoch 315/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2627 - R_squared: 0.5585 - val_loss: 0.4061 - val_R_squared: 0.3835\n",
      "Epoch 316/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2711 - R_squared: 0.5513 - val_loss: 0.3322 - val_R_squared: 0.4616\n",
      "Epoch 317/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2664 - R_squared: 0.5463 - val_loss: 0.2977 - val_R_squared: 0.4425\n",
      "Epoch 318/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2666 - R_squared: 0.5516 - val_loss: 0.3441 - val_R_squared: 0.4876\n",
      "Epoch 319/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2669 - R_squared: 0.5512 - val_loss: 0.2709 - val_R_squared: 0.5515\n",
      "Epoch 320/600\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 0.2641 - R_squared: 0.5546 - val_loss: 0.2879 - val_R_squared: 0.4798\n",
      "Epoch 321/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2665 - R_squared: 0.5484 - val_loss: 0.2446 - val_R_squared: 0.5834\n",
      "Epoch 322/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2678 - R_squared: 0.5531 - val_loss: 0.2816 - val_R_squared: 0.4658\n",
      "Epoch 323/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2600 - R_squared: 0.5541 - val_loss: 0.3901 - val_R_squared: 0.4385\n",
      "Epoch 324/600\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2655 - R_squared: 0.5610 - val_loss: 0.2953 - val_R_squared: 0.5103\n",
      "Epoch 325/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.2718 - R_squared: 0.5529 - val_loss: 0.2804 - val_R_squared: 0.4665\n",
      "Epoch 326/600\n",
      "10/10 [==============================] - 1s 132ms/step - loss: 0.2538 - R_squared: 0.5633 - val_loss: 0.4020 - val_R_squared: 0.3912\n",
      "Epoch 327/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2661 - R_squared: 0.5472 - val_loss: 0.2628 - val_R_squared: 0.5404\n",
      "Epoch 328/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2600 - R_squared: 0.5550 - val_loss: 0.3170 - val_R_squared: 0.4770\n",
      "Epoch 329/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2634 - R_squared: 0.5640 - val_loss: 0.2532 - val_R_squared: 0.5419\n",
      "Epoch 330/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2647 - R_squared: 0.5549 - val_loss: 0.3003 - val_R_squared: 0.4398\n",
      "Epoch 331/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2673 - R_squared: 0.5524 - val_loss: 0.3037 - val_R_squared: 0.4963\n",
      "Epoch 332/600\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 0.2659 - R_squared: 0.5539 - val_loss: 0.3095 - val_R_squared: 0.4605\n",
      "Epoch 333/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2644 - R_squared: 0.5594 - val_loss: 0.2713 - val_R_squared: 0.5300\n",
      "Epoch 334/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2658 - R_squared: 0.5521 - val_loss: 0.3321 - val_R_squared: 0.4237\n",
      "Epoch 335/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2521 - R_squared: 0.5658 - val_loss: 0.2803 - val_R_squared: 0.5224\n",
      "Epoch 336/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2502 - R_squared: 0.5652 - val_loss: 0.2845 - val_R_squared: 0.4480\n",
      "Epoch 337/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2709 - R_squared: 0.5510 - val_loss: 0.3596 - val_R_squared: 0.3876\n",
      "Epoch 338/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2718 - R_squared: 0.5425 - val_loss: 0.3432 - val_R_squared: 0.4620\n",
      "Epoch 339/600\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 0.2639 - R_squared: 0.5578 - val_loss: 0.2880 - val_R_squared: 0.4347\n",
      "Epoch 340/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2710 - R_squared: 0.5470 - val_loss: 0.3294 - val_R_squared: 0.4792\n",
      "Epoch 341/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2719 - R_squared: 0.5459 - val_loss: 0.2545 - val_R_squared: 0.5513\n",
      "Epoch 342/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2661 - R_squared: 0.5532 - val_loss: 0.4265 - val_R_squared: 0.3932\n",
      "Epoch 343/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2723 - R_squared: 0.5462 - val_loss: 0.2912 - val_R_squared: 0.4698\n",
      "Epoch 344/600\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 0.2767 - R_squared: 0.5388 - val_loss: 0.2992 - val_R_squared: 0.4318\n",
      "Epoch 345/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2620 - R_squared: 0.5590 - val_loss: 0.2818 - val_R_squared: 0.4738\n",
      "Epoch 346/600\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 0.2707 - R_squared: 0.5517 - val_loss: 0.2700 - val_R_squared: 0.4726\n",
      "Epoch 347/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2520 - R_squared: 0.5706 - val_loss: 0.2844 - val_R_squared: 0.5108\n",
      "Epoch 348/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2635 - R_squared: 0.5555 - val_loss: 0.3297 - val_R_squared: 0.3950\n",
      "Epoch 349/600\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2656 - R_squared: 0.5535 - val_loss: 0.2593 - val_R_squared: 0.4891\n",
      "Epoch 350/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2569 - R_squared: 0.5659 - val_loss: 0.3703 - val_R_squared: 0.4608\n",
      "Epoch 351/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2705 - R_squared: 0.5419 - val_loss: 0.2665 - val_R_squared: 0.5174\n",
      "Epoch 352/600\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2679 - R_squared: 0.5506 - val_loss: 0.4232 - val_R_squared: 0.4010\n",
      "Epoch 353/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2597 - R_squared: 0.5671 - val_loss: 0.2738 - val_R_squared: 0.4853\n",
      "Epoch 354/600\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2612 - R_squared: 0.5647 - val_loss: 0.2880 - val_R_squared: 0.5246\n",
      "Epoch 355/600\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2604 - R_squared: 0.5558 - val_loss: 0.2404 - val_R_squared: 0.5056\n",
      "Epoch 356/600\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2541 - R_squared: 0.5729 - val_loss: 0.2914 - val_R_squared: 0.5243\n",
      "Epoch 357/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2620 - R_squared: 0.5620 - val_loss: 0.3170 - val_R_squared: 0.4642\n",
      "Epoch 358/600\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2435 - R_squared: 0.5747 - val_loss: 0.2627 - val_R_squared: 0.5150\n",
      "Epoch 359/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2660 - R_squared: 0.5500 - val_loss: 0.2196 - val_R_squared: 0.5762\n",
      "Epoch 360/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.2583 - R_squared: 0.5611 - val_loss: 0.3548 - val_R_squared: 0.4496\n",
      "Epoch 361/600\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2632 - R_squared: 0.5630 - val_loss: 0.3182 - val_R_squared: 0.4247\n",
      "Epoch 362/600\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2626 - R_squared: 0.5573 - val_loss: 0.2889 - val_R_squared: 0.5158\n",
      "Epoch 363/600\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2639 - R_squared: 0.5624 - val_loss: 0.2938 - val_R_squared: 0.4998\n",
      "Epoch 364/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2642 - R_squared: 0.5655 - val_loss: 0.2700 - val_R_squared: 0.5236\n",
      "Epoch 365/600\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2625 - R_squared: 0.5632 - val_loss: 0.2722 - val_R_squared: 0.5384\n",
      "Epoch 366/600\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2572 - R_squared: 0.5658 - val_loss: 0.3272 - val_R_squared: 0.4687\n",
      "Epoch 367/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2584 - R_squared: 0.5717 - val_loss: 0.4332 - val_R_squared: 0.4213\n",
      "Epoch 368/600\n",
      "10/10 [==============================] - 1s 133ms/step - loss: 0.2557 - R_squared: 0.5678 - val_loss: 0.3484 - val_R_squared: 0.5431\n",
      "Epoch 369/600\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2654 - R_squared: 0.5539 - val_loss: 0.3298 - val_R_squared: 0.4458\n",
      "Epoch 370/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2607 - R_squared: 0.5641 - val_loss: 0.3192 - val_R_squared: 0.4748\n",
      "Epoch 371/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2596 - R_squared: 0.5669 - val_loss: 0.2886 - val_R_squared: 0.5096\n",
      "Epoch 372/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2548 - R_squared: 0.5681 - val_loss: 0.2995 - val_R_squared: 0.4542\n",
      "Epoch 373/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2576 - R_squared: 0.5679 - val_loss: 0.2953 - val_R_squared: 0.5369\n",
      "Epoch 374/600\n",
      "10/10 [==============================] - 1s 129ms/step - loss: 0.2536 - R_squared: 0.5746 - val_loss: 0.4217 - val_R_squared: 0.4239\n",
      "Epoch 375/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2573 - R_squared: 0.5720 - val_loss: 0.3227 - val_R_squared: 0.5418\n",
      "Epoch 376/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2522 - R_squared: 0.5782 - val_loss: 0.3163 - val_R_squared: 0.4654\n",
      "Epoch 377/600\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2561 - R_squared: 0.5732 - val_loss: 0.3091 - val_R_squared: 0.4881\n",
      "Epoch 378/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2573 - R_squared: 0.5738 - val_loss: 0.2900 - val_R_squared: 0.4969\n",
      "Epoch 379/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2563 - R_squared: 0.5736 - val_loss: 0.2820 - val_R_squared: 0.5181\n",
      "Epoch 380/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2493 - R_squared: 0.5801 - val_loss: 0.2883 - val_R_squared: 0.5109\n",
      "Epoch 381/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2512 - R_squared: 0.5785 - val_loss: 0.2436 - val_R_squared: 0.4825\n",
      "Epoch 382/600\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2530 - R_squared: 0.5779 - val_loss: 0.2965 - val_R_squared: 0.4623\n",
      "Epoch 383/600\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2428 - R_squared: 0.5807 - val_loss: 0.3950 - val_R_squared: 0.4308\n",
      "Epoch 384/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2527 - R_squared: 0.5777 - val_loss: 0.2881 - val_R_squared: 0.4431\n",
      "Epoch 385/600\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2379 - R_squared: 0.5865 - val_loss: 0.2621 - val_R_squared: 0.5391\n",
      "Epoch 386/600\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2568 - R_squared: 0.5732 - val_loss: 0.2632 - val_R_squared: 0.5558\n",
      "Epoch 387/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2512 - R_squared: 0.5784 - val_loss: 0.2535 - val_R_squared: 0.5442\n",
      "Epoch 388/600\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2527 - R_squared: 0.5806 - val_loss: 0.3070 - val_R_squared: 0.5299\n",
      "Epoch 389/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2502 - R_squared: 0.5822 - val_loss: 0.3844 - val_R_squared: 0.4732\n",
      "Epoch 390/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2468 - R_squared: 0.5840 - val_loss: 0.2645 - val_R_squared: 0.5133\n",
      "Epoch 391/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2539 - R_squared: 0.5801 - val_loss: 0.2763 - val_R_squared: 0.5454\n",
      "Epoch 392/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2477 - R_squared: 0.5855 - val_loss: 0.2557 - val_R_squared: 0.5542\n",
      "Epoch 393/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2503 - R_squared: 0.5823 - val_loss: 0.2960 - val_R_squared: 0.5209\n",
      "Epoch 394/600\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 0.2486 - R_squared: 0.5854 - val_loss: 0.2619 - val_R_squared: 0.5067\n",
      "Epoch 395/600\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2506 - R_squared: 0.5824 - val_loss: 0.3231 - val_R_squared: 0.4982\n",
      "Epoch 396/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.2501 - R_squared: 0.5804 - val_loss: 0.2901 - val_R_squared: 0.5005\n",
      "Epoch 397/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2484 - R_squared: 0.5839 - val_loss: 0.2529 - val_R_squared: 0.5146\n",
      "Epoch 398/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2489 - R_squared: 0.5796 - val_loss: 0.2877 - val_R_squared: 0.5130\n",
      "Epoch 399/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2506 - R_squared: 0.5802 - val_loss: 0.4212 - val_R_squared: 0.3396\n",
      "Epoch 400/600\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 0.2534 - R_squared: 0.5768 - val_loss: 0.3143 - val_R_squared: 0.4735\n",
      "Epoch 401/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2543 - R_squared: 0.5739 - val_loss: 0.2974 - val_R_squared: 0.4741\n",
      "Epoch 402/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2334 - R_squared: 0.5921 - val_loss: 0.2463 - val_R_squared: 0.5490\n",
      "Epoch 403/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2555 - R_squared: 0.5765 - val_loss: 0.3008 - val_R_squared: 0.4266\n",
      "Epoch 404/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2449 - R_squared: 0.5836 - val_loss: 0.2645 - val_R_squared: 0.5315\n",
      "Epoch 405/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2489 - R_squared: 0.5785 - val_loss: 0.2628 - val_R_squared: 0.5305\n",
      "Epoch 406/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2437 - R_squared: 0.5823 - val_loss: 0.3298 - val_R_squared: 0.4590\n",
      "Epoch 407/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2492 - R_squared: 0.5770 - val_loss: 0.2951 - val_R_squared: 0.4787\n",
      "Epoch 408/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2479 - R_squared: 0.5854 - val_loss: 0.2702 - val_R_squared: 0.5292\n",
      "Epoch 409/600\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2499 - R_squared: 0.5831 - val_loss: 0.2376 - val_R_squared: 0.5338\n",
      "Epoch 410/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2539 - R_squared: 0.5768 - val_loss: 0.2564 - val_R_squared: 0.5356\n",
      "Epoch 411/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2375 - R_squared: 0.5927 - val_loss: 0.2719 - val_R_squared: 0.4911\n",
      "Epoch 412/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2450 - R_squared: 0.5904 - val_loss: 0.2771 - val_R_squared: 0.4969\n",
      "Epoch 413/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2411 - R_squared: 0.5922 - val_loss: 0.2643 - val_R_squared: 0.4815\n",
      "Epoch 414/600\n",
      "10/10 [==============================] - 1s 131ms/step - loss: 0.2480 - R_squared: 0.5848 - val_loss: 0.2549 - val_R_squared: 0.5284\n",
      "Epoch 415/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2483 - R_squared: 0.5789 - val_loss: 0.3306 - val_R_squared: 0.4790\n",
      "Epoch 416/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2400 - R_squared: 0.5988 - val_loss: 0.3919 - val_R_squared: 0.4641\n",
      "Epoch 417/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2462 - R_squared: 0.5912 - val_loss: 0.2717 - val_R_squared: 0.4638\n",
      "Epoch 418/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2450 - R_squared: 0.5901 - val_loss: 0.2935 - val_R_squared: 0.5203\n",
      "Epoch 419/600\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 0.2451 - R_squared: 0.5990 - val_loss: 0.2884 - val_R_squared: 0.5032\n",
      "Epoch 420/600\n",
      "10/10 [==============================] - 1s 145ms/step - loss: 0.2475 - R_squared: 0.5854 - val_loss: 0.2824 - val_R_squared: 0.5133\n",
      "Epoch 421/600\n",
      "10/10 [==============================] - 1s 148ms/step - loss: 0.2359 - R_squared: 0.6035 - val_loss: 0.2676 - val_R_squared: 0.5529\n",
      "Epoch 422/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2244 - R_squared: 0.6081 - val_loss: 0.2980 - val_R_squared: 0.5014\n",
      "Epoch 423/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2408 - R_squared: 0.6005 - val_loss: 0.2869 - val_R_squared: 0.4698\n",
      "Epoch 424/600\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 0.2456 - R_squared: 0.5889 - val_loss: 0.2307 - val_R_squared: 0.5725\n",
      "Epoch 425/600\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 0.2389 - R_squared: 0.5994 - val_loss: 0.2838 - val_R_squared: 0.5161\n",
      "Epoch 426/600\n",
      "10/10 [==============================] - 1s 144ms/step - loss: 0.2458 - R_squared: 0.5822 - val_loss: 0.3160 - val_R_squared: 0.4807\n",
      "Epoch 427/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2417 - R_squared: 0.5924 - val_loss: 0.2910 - val_R_squared: 0.5632\n",
      "Epoch 428/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2400 - R_squared: 0.6019 - val_loss: 0.2867 - val_R_squared: 0.4941\n",
      "Epoch 429/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2441 - R_squared: 0.5951 - val_loss: 0.2537 - val_R_squared: 0.5489\n",
      "Epoch 430/600\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 0.2393 - R_squared: 0.5974 - val_loss: 0.2955 - val_R_squared: 0.5086\n",
      "Epoch 431/600\n",
      "10/10 [==============================] - 2s 139ms/step - loss: 0.2377 - R_squared: 0.5984 - val_loss: 0.3633 - val_R_squared: 0.4725\n",
      "Epoch 432/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2242 - R_squared: 0.6131 - val_loss: 0.2813 - val_R_squared: 0.5789\n",
      "Epoch 433/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2453 - R_squared: 0.5908 - val_loss: 0.3027 - val_R_squared: 0.4431\n",
      "Epoch 434/600\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2411 - R_squared: 0.5947 - val_loss: 0.2805 - val_R_squared: 0.5049\n",
      "Epoch 435/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2442 - R_squared: 0.5909 - val_loss: 0.2598 - val_R_squared: 0.5109\n",
      "Epoch 436/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2374 - R_squared: 0.5971 - val_loss: 0.2933 - val_R_squared: 0.5363\n",
      "Epoch 437/600\n",
      "10/10 [==============================] - 1s 133ms/step - loss: 0.2413 - R_squared: 0.5960 - val_loss: 0.3049 - val_R_squared: 0.4838\n",
      "Epoch 438/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2391 - R_squared: 0.5967 - val_loss: 0.3014 - val_R_squared: 0.4716\n",
      "Epoch 439/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2491 - R_squared: 0.5910 - val_loss: 0.3617 - val_R_squared: 0.4965\n",
      "Epoch 440/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2274 - R_squared: 0.6001 - val_loss: 0.2523 - val_R_squared: 0.5799\n",
      "Epoch 441/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2385 - R_squared: 0.6011 - val_loss: 0.3003 - val_R_squared: 0.5250\n",
      "Epoch 442/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2393 - R_squared: 0.5940 - val_loss: 0.2631 - val_R_squared: 0.5827\n",
      "Epoch 443/600\n",
      "10/10 [==============================] - 1s 142ms/step - loss: 0.2376 - R_squared: 0.6018 - val_loss: 0.2865 - val_R_squared: 0.4687\n",
      "Epoch 444/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2429 - R_squared: 0.5961 - val_loss: 0.3082 - val_R_squared: 0.5352\n",
      "Epoch 445/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2350 - R_squared: 0.6040 - val_loss: 0.3947 - val_R_squared: 0.4337\n",
      "Epoch 446/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2396 - R_squared: 0.5967 - val_loss: 0.3304 - val_R_squared: 0.4665\n",
      "Epoch 447/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2372 - R_squared: 0.6018 - val_loss: 0.2333 - val_R_squared: 0.5719\n",
      "Epoch 448/600\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 0.2358 - R_squared: 0.6039 - val_loss: 0.2649 - val_R_squared: 0.5399\n",
      "Epoch 449/600\n",
      "10/10 [==============================] - 1s 150ms/step - loss: 0.2390 - R_squared: 0.5998 - val_loss: 0.2807 - val_R_squared: 0.4609\n",
      "Epoch 450/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2429 - R_squared: 0.5963 - val_loss: 0.3091 - val_R_squared: 0.4192\n",
      "Epoch 451/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2415 - R_squared: 0.5959 - val_loss: 0.2664 - val_R_squared: 0.5681\n",
      "Epoch 452/600\n",
      "10/10 [==============================] - 1s 133ms/step - loss: 0.2349 - R_squared: 0.6091 - val_loss: 0.2650 - val_R_squared: 0.5190\n",
      "Epoch 453/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2386 - R_squared: 0.5970 - val_loss: 0.2704 - val_R_squared: 0.5673\n",
      "Epoch 454/600\n",
      "10/10 [==============================] - 1s 140ms/step - loss: 0.2443 - R_squared: 0.5955 - val_loss: 0.2732 - val_R_squared: 0.5493\n",
      "Epoch 455/600\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 0.2343 - R_squared: 0.6047 - val_loss: 0.2677 - val_R_squared: 0.5344\n",
      "Epoch 456/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2249 - R_squared: 0.6129 - val_loss: 0.2890 - val_R_squared: 0.4913\n",
      "Epoch 457/600\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 0.2394 - R_squared: 0.6019 - val_loss: 0.3261 - val_R_squared: 0.4299\n",
      "Epoch 458/600\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 0.2241 - R_squared: 0.6115 - val_loss: 0.2608 - val_R_squared: 0.5510\n",
      "Epoch 459/600\n",
      "10/10 [==============================] - 1s 129ms/step - loss: 0.2397 - R_squared: 0.5969 - val_loss: 0.3806 - val_R_squared: 0.4336\n",
      "Epoch 460/600\n",
      "10/10 [==============================] - 1s 150ms/step - loss: 0.2407 - R_squared: 0.6017 - val_loss: 0.2652 - val_R_squared: 0.5649\n",
      "Epoch 461/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2325 - R_squared: 0.6065 - val_loss: 0.3232 - val_R_squared: 0.5439\n",
      "Epoch 462/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2312 - R_squared: 0.6090 - val_loss: 0.2692 - val_R_squared: 0.4892\n",
      "Epoch 463/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2371 - R_squared: 0.6041 - val_loss: 0.2826 - val_R_squared: 0.5149\n",
      "Epoch 464/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2336 - R_squared: 0.6084 - val_loss: 0.3652 - val_R_squared: 0.5517\n",
      "Epoch 465/600\n",
      "10/10 [==============================] - 1s 144ms/step - loss: 0.2369 - R_squared: 0.6044 - val_loss: 0.2720 - val_R_squared: 0.5305\n",
      "Epoch 466/600\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 0.2362 - R_squared: 0.6027 - val_loss: 0.2944 - val_R_squared: 0.4942\n",
      "Epoch 467/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2367 - R_squared: 0.5999 - val_loss: 0.2712 - val_R_squared: 0.5442\n",
      "Epoch 468/600\n",
      "10/10 [==============================] - 1s 129ms/step - loss: 0.2314 - R_squared: 0.6065 - val_loss: 0.2310 - val_R_squared: 0.5626\n",
      "Epoch 469/600\n",
      "10/10 [==============================] - 1s 142ms/step - loss: 0.2331 - R_squared: 0.6096 - val_loss: 0.2527 - val_R_squared: 0.5581\n",
      "Epoch 470/600\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 0.2359 - R_squared: 0.6002 - val_loss: 0.2901 - val_R_squared: 0.5657\n",
      "Epoch 471/600\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 0.2377 - R_squared: 0.6056 - val_loss: 0.2584 - val_R_squared: 0.5209\n",
      "Epoch 472/600\n",
      "10/10 [==============================] - 1s 136ms/step - loss: 0.2351 - R_squared: 0.6064 - val_loss: 0.3535 - val_R_squared: 0.5071\n",
      "Epoch 473/600\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 0.2327 - R_squared: 0.6213 - val_loss: 0.2733 - val_R_squared: 0.5826\n",
      "Epoch 474/600\n",
      "10/10 [==============================] - 1s 132ms/step - loss: 0.2264 - R_squared: 0.6200 - val_loss: 0.2570 - val_R_squared: 0.5316\n",
      "Epoch 475/600\n",
      "10/10 [==============================] - 2s 155ms/step - loss: 0.2326 - R_squared: 0.6089 - val_loss: 0.2681 - val_R_squared: 0.5124\n",
      "Epoch 476/600\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 0.2319 - R_squared: 0.6116 - val_loss: 0.2854 - val_R_squared: 0.4884\n",
      "Epoch 477/600\n",
      "10/10 [==============================] - 1s 145ms/step - loss: 0.2330 - R_squared: 0.6081 - val_loss: 0.2246 - val_R_squared: 0.5888\n",
      "Epoch 478/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2270 - R_squared: 0.6179 - val_loss: 0.3719 - val_R_squared: 0.4890\n",
      "Epoch 479/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2296 - R_squared: 0.6136 - val_loss: 0.2494 - val_R_squared: 0.5490\n",
      "Epoch 480/600\n",
      "10/10 [==============================] - 1s 129ms/step - loss: 0.2310 - R_squared: 0.6116 - val_loss: 0.3577 - val_R_squared: 0.4513\n",
      "Epoch 481/600\n",
      "10/10 [==============================] - 1s 142ms/step - loss: 0.2297 - R_squared: 0.6092 - val_loss: 0.2536 - val_R_squared: 0.5496\n",
      "Epoch 482/600\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 0.2307 - R_squared: 0.6120 - val_loss: 0.2440 - val_R_squared: 0.5660\n",
      "Epoch 483/600\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 0.2279 - R_squared: 0.6174 - val_loss: 0.2395 - val_R_squared: 0.5501\n",
      "Epoch 484/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2266 - R_squared: 0.6194 - val_loss: 0.2635 - val_R_squared: 0.5535\n",
      "Epoch 485/600\n",
      "10/10 [==============================] - 1s 129ms/step - loss: 0.2252 - R_squared: 0.6184 - val_loss: 0.2707 - val_R_squared: 0.5577\n",
      "Epoch 486/600\n",
      "10/10 [==============================] - 1s 150ms/step - loss: 0.2190 - R_squared: 0.6280 - val_loss: 0.2732 - val_R_squared: 0.5749\n",
      "Epoch 487/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2254 - R_squared: 0.6261 - val_loss: 0.2281 - val_R_squared: 0.5905\n",
      "Epoch 488/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2309 - R_squared: 0.6228 - val_loss: 0.2477 - val_R_squared: 0.5764\n",
      "Epoch 489/600\n",
      "10/10 [==============================] - 1s 132ms/step - loss: 0.2213 - R_squared: 0.6246 - val_loss: 0.2326 - val_R_squared: 0.5886\n",
      "Epoch 490/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.2266 - R_squared: 0.6227 - val_loss: 0.2968 - val_R_squared: 0.5513\n",
      "Epoch 491/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2307 - R_squared: 0.6182 - val_loss: 0.2340 - val_R_squared: 0.5812\n",
      "Epoch 492/600\n",
      "10/10 [==============================] - 1s 137ms/step - loss: 0.2269 - R_squared: 0.6274 - val_loss: 0.2601 - val_R_squared: 0.5181\n",
      "Epoch 493/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2251 - R_squared: 0.6183 - val_loss: 0.2445 - val_R_squared: 0.5619\n",
      "Epoch 494/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2245 - R_squared: 0.6303 - val_loss: 0.2583 - val_R_squared: 0.5111\n",
      "Epoch 495/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.2269 - R_squared: 0.6238 - val_loss: 0.2398 - val_R_squared: 0.6109\n",
      "Epoch 496/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.2191 - R_squared: 0.6365 - val_loss: 0.2480 - val_R_squared: 0.5684\n",
      "Epoch 497/600\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 0.2229 - R_squared: 0.6227 - val_loss: 0.2377 - val_R_squared: 0.5671\n",
      "Epoch 498/600\n",
      "10/10 [==============================] - 1s 133ms/step - loss: 0.2207 - R_squared: 0.6304 - val_loss: 0.2629 - val_R_squared: 0.5214\n",
      "Epoch 499/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2223 - R_squared: 0.6269 - val_loss: 0.2489 - val_R_squared: 0.5581\n",
      "Epoch 500/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2222 - R_squared: 0.6322 - val_loss: 0.3461 - val_R_squared: 0.5592\n",
      "Epoch 501/600\n",
      "10/10 [==============================] - 1s 131ms/step - loss: 0.2202 - R_squared: 0.6281 - val_loss: 0.3714 - val_R_squared: 0.5089\n",
      "Epoch 502/600\n",
      "10/10 [==============================] - 1s 138ms/step - loss: 0.2232 - R_squared: 0.6308 - val_loss: 0.2787 - val_R_squared: 0.5243\n",
      "Epoch 503/600\n",
      "10/10 [==============================] - 1s 132ms/step - loss: 0.2242 - R_squared: 0.6263 - val_loss: 0.2437 - val_R_squared: 0.6359\n",
      "Epoch 504/600\n",
      "10/10 [==============================] - 1s 141ms/step - loss: 0.2234 - R_squared: 0.6242 - val_loss: 0.2593 - val_R_squared: 0.5331\n",
      "Epoch 505/600\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 0.2339 - R_squared: 0.6117 - val_loss: 0.3573 - val_R_squared: 0.4902\n",
      "Epoch 506/600\n",
      "10/10 [==============================] - 1s 144ms/step - loss: 0.2335 - R_squared: 0.6144 - val_loss: 0.2669 - val_R_squared: 0.5773\n",
      "Epoch 507/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2234 - R_squared: 0.6212 - val_loss: 0.2253 - val_R_squared: 0.5781\n",
      "Epoch 508/600\n",
      "10/10 [==============================] - 1s 143ms/step - loss: 0.2273 - R_squared: 0.6210 - val_loss: 0.2709 - val_R_squared: 0.5836\n",
      "Epoch 509/600\n",
      "10/10 [==============================] - 1s 145ms/step - loss: 0.2222 - R_squared: 0.6277 - val_loss: 0.2338 - val_R_squared: 0.6174\n",
      "Epoch 510/600\n",
      "10/10 [==============================] - 1s 131ms/step - loss: 0.2224 - R_squared: 0.6242 - val_loss: 0.2490 - val_R_squared: 0.5417\n",
      "Epoch 511/600\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 0.2207 - R_squared: 0.6304 - val_loss: 0.2448 - val_R_squared: 0.5205\n",
      "Epoch 512/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.2208 - R_squared: 0.6340 - val_loss: 0.2596 - val_R_squared: 0.5386\n",
      "Epoch 513/600\n",
      "10/10 [==============================] - 1s 143ms/step - loss: 0.2244 - R_squared: 0.6218 - val_loss: 0.3356 - val_R_squared: 0.5174\n",
      "Epoch 514/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.2210 - R_squared: 0.6286 - val_loss: 0.2434 - val_R_squared: 0.5504\n",
      "Epoch 515/600\n",
      "10/10 [==============================] - 2s 160ms/step - loss: 0.2290 - R_squared: 0.6175 - val_loss: 0.3448 - val_R_squared: 0.4391\n",
      "Epoch 516/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2322 - R_squared: 0.6082 - val_loss: 0.2611 - val_R_squared: 0.5875\n",
      "Epoch 517/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2290 - R_squared: 0.6176 - val_loss: 0.2270 - val_R_squared: 0.5904\n",
      "Epoch 518/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2222 - R_squared: 0.6301 - val_loss: 0.2796 - val_R_squared: 0.4972\n",
      "Epoch 519/600\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.2256 - R_squared: 0.6241 - val_loss: 0.3544 - val_R_squared: 0.4603\n",
      "Epoch 520/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2205 - R_squared: 0.6292 - val_loss: 0.2421 - val_R_squared: 0.5523\n",
      "Epoch 521/600\n",
      "10/10 [==============================] - 1s 149ms/step - loss: 0.2213 - R_squared: 0.6320 - val_loss: 0.2258 - val_R_squared: 0.5764\n",
      "Epoch 522/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2205 - R_squared: 0.6352 - val_loss: 0.3802 - val_R_squared: 0.4633\n",
      "Epoch 523/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2194 - R_squared: 0.6288 - val_loss: 0.2481 - val_R_squared: 0.5635\n",
      "Epoch 524/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2189 - R_squared: 0.6334 - val_loss: 0.2398 - val_R_squared: 0.5643\n",
      "Epoch 525/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2205 - R_squared: 0.6266 - val_loss: 0.3629 - val_R_squared: 0.4457\n",
      "Epoch 526/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2191 - R_squared: 0.6310 - val_loss: 0.2137 - val_R_squared: 0.5564\n",
      "Epoch 527/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2152 - R_squared: 0.6396 - val_loss: 0.2152 - val_R_squared: 0.6138\n",
      "Epoch 528/600\n",
      "10/10 [==============================] - 1s 145ms/step - loss: 0.2235 - R_squared: 0.6270 - val_loss: 0.2585 - val_R_squared: 0.5033\n",
      "Epoch 529/600\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 0.2052 - R_squared: 0.6470 - val_loss: 0.2860 - val_R_squared: 0.5444\n",
      "Epoch 530/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2203 - R_squared: 0.6319 - val_loss: 0.2309 - val_R_squared: 0.5415\n",
      "Epoch 531/600\n",
      "10/10 [==============================] - 1s 134ms/step - loss: 0.2185 - R_squared: 0.6361 - val_loss: 0.3410 - val_R_squared: 0.5153\n",
      "Epoch 532/600\n",
      "10/10 [==============================] - 1s 147ms/step - loss: 0.2124 - R_squared: 0.6395 - val_loss: 0.2290 - val_R_squared: 0.6310\n",
      "Epoch 533/600\n",
      "10/10 [==============================] - 1s 139ms/step - loss: 0.2152 - R_squared: 0.6424 - val_loss: 0.3179 - val_R_squared: 0.5365\n",
      "Epoch 534/600\n",
      "10/10 [==============================] - 1s 132ms/step - loss: 0.2104 - R_squared: 0.6441 - val_loss: 0.2340 - val_R_squared: 0.5722\n",
      "Epoch 535/600\n",
      "10/10 [==============================] - 1s 145ms/step - loss: 0.2131 - R_squared: 0.6447 - val_loss: 0.2346 - val_R_squared: 0.5523\n",
      "Epoch 536/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2069 - R_squared: 0.6462 - val_loss: 0.2259 - val_R_squared: 0.5863\n",
      "Epoch 537/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2155 - R_squared: 0.6414 - val_loss: 0.3226 - val_R_squared: 0.5433\n",
      "Epoch 538/600\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.2141 - R_squared: 0.6413 - val_loss: 0.2500 - val_R_squared: 0.5609\n",
      "Epoch 539/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2374 - R_squared: 0.6045 - val_loss: 0.2283 - val_R_squared: 0.5554\n",
      "Epoch 540/600\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 0.2704 - R_squared: 0.5496 - val_loss: 0.3553 - val_R_squared: 0.4843\n",
      "Epoch 541/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2550 - R_squared: 0.5741 - val_loss: 0.2949 - val_R_squared: 0.5093\n",
      "Epoch 542/600\n",
      "10/10 [==============================] - 1s 144ms/step - loss: 0.2504 - R_squared: 0.5754 - val_loss: 0.2919 - val_R_squared: 0.5167\n",
      "Epoch 543/600\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 0.2279 - R_squared: 0.6033 - val_loss: 0.2359 - val_R_squared: 0.5450\n",
      "Epoch 544/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2433 - R_squared: 0.5914 - val_loss: 0.2877 - val_R_squared: 0.4712\n",
      "Epoch 545/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2300 - R_squared: 0.6090 - val_loss: 0.2412 - val_R_squared: 0.5412\n",
      "Epoch 546/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2340 - R_squared: 0.6098 - val_loss: 0.3459 - val_R_squared: 0.4605\n",
      "Epoch 547/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2221 - R_squared: 0.6197 - val_loss: 0.2706 - val_R_squared: 0.5635\n",
      "Epoch 548/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2299 - R_squared: 0.6153 - val_loss: 0.2508 - val_R_squared: 0.5558\n",
      "Epoch 549/600\n",
      "10/10 [==============================] - 1s 139ms/step - loss: 0.2392 - R_squared: 0.6008 - val_loss: 0.2354 - val_R_squared: 0.5981\n",
      "Epoch 550/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2200 - R_squared: 0.6166 - val_loss: 0.3282 - val_R_squared: 0.5151\n",
      "Epoch 551/600\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.2297 - R_squared: 0.6142 - val_loss: 0.3055 - val_R_squared: 0.5533\n",
      "Epoch 552/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2287 - R_squared: 0.6187 - val_loss: 0.2539 - val_R_squared: 0.6113\n",
      "Epoch 553/600\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 0.2216 - R_squared: 0.6248 - val_loss: 0.2618 - val_R_squared: 0.5846\n",
      "Epoch 554/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.2204 - R_squared: 0.6336 - val_loss: 0.2742 - val_R_squared: 0.5301\n",
      "Epoch 555/600\n",
      "10/10 [==============================] - 2s 164ms/step - loss: 0.2081 - R_squared: 0.6320 - val_loss: 0.2436 - val_R_squared: 0.5674\n",
      "Epoch 556/600\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 0.2196 - R_squared: 0.6329 - val_loss: 0.2626 - val_R_squared: 0.5738\n",
      "Epoch 557/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.2256 - R_squared: 0.6237 - val_loss: 0.2548 - val_R_squared: 0.5595\n",
      "Epoch 558/600\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 0.2237 - R_squared: 0.6220 - val_loss: 0.2577 - val_R_squared: 0.5961\n",
      "Epoch 559/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2226 - R_squared: 0.6308 - val_loss: 0.2424 - val_R_squared: 0.5549\n",
      "Epoch 560/600\n",
      "10/10 [==============================] - 1s 143ms/step - loss: 0.2256 - R_squared: 0.6297 - val_loss: 0.2619 - val_R_squared: 0.5616\n",
      "Epoch 561/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2144 - R_squared: 0.6406 - val_loss: 0.2253 - val_R_squared: 0.5157\n",
      "Epoch 562/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2228 - R_squared: 0.6230 - val_loss: 0.2534 - val_R_squared: 0.5054\n",
      "Epoch 563/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2236 - R_squared: 0.6312 - val_loss: 0.2665 - val_R_squared: 0.4933\n",
      "Epoch 564/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.2167 - R_squared: 0.6364 - val_loss: 0.2687 - val_R_squared: 0.5563\n",
      "Epoch 565/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2205 - R_squared: 0.6253 - val_loss: 0.2812 - val_R_squared: 0.5042\n",
      "Epoch 566/600\n",
      "10/10 [==============================] - 1s 135ms/step - loss: 0.2239 - R_squared: 0.6289 - val_loss: 0.2571 - val_R_squared: 0.5767\n",
      "Epoch 567/600\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 0.2248 - R_squared: 0.6211 - val_loss: 0.2399 - val_R_squared: 0.5963\n",
      "Epoch 568/600\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 0.2124 - R_squared: 0.6304 - val_loss: 0.2302 - val_R_squared: 0.6202\n",
      "Epoch 569/600\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.2216 - R_squared: 0.6322 - val_loss: 0.2411 - val_R_squared: 0.6432\n",
      "Epoch 570/600\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.2244 - R_squared: 0.6258 - val_loss: 0.2647 - val_R_squared: 0.5640\n",
      "Epoch 571/600\n",
      "10/10 [==============================] - 1s 131ms/step - loss: 0.2153 - R_squared: 0.6389 - val_loss: 0.2224 - val_R_squared: 0.6332\n",
      "Epoch 572/600\n",
      "10/10 [==============================] - 1s 145ms/step - loss: 0.2145 - R_squared: 0.6374 - val_loss: 0.2489 - val_R_squared: 0.5493\n",
      "Epoch 573/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2128 - R_squared: 0.6434 - val_loss: 0.2300 - val_R_squared: 0.5716\n",
      "Epoch 574/600\n",
      "10/10 [==============================] - 1s 152ms/step - loss: 0.2103 - R_squared: 0.6445 - val_loss: 0.2485 - val_R_squared: 0.5906\n",
      "Epoch 575/600\n",
      "10/10 [==============================] - 2s 158ms/step - loss: 0.2156 - R_squared: 0.6457 - val_loss: 0.2479 - val_R_squared: 0.5508\n",
      "Epoch 576/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2127 - R_squared: 0.6403 - val_loss: 0.2650 - val_R_squared: 0.5047\n",
      "Epoch 577/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2147 - R_squared: 0.6410 - val_loss: 0.2447 - val_R_squared: 0.5689\n",
      "Epoch 578/600\n",
      "10/10 [==============================] - 1s 138ms/step - loss: 0.2180 - R_squared: 0.6403 - val_loss: 0.2355 - val_R_squared: 0.5632\n",
      "Epoch 579/600\n",
      "10/10 [==============================] - 1s 130ms/step - loss: 0.2108 - R_squared: 0.6481 - val_loss: 0.3122 - val_R_squared: 0.5266\n",
      "Epoch 580/600\n",
      "10/10 [==============================] - 1s 147ms/step - loss: 0.2125 - R_squared: 0.6444 - val_loss: 0.3250 - val_R_squared: 0.5500\n",
      "Epoch 581/600\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 0.2104 - R_squared: 0.6480 - val_loss: 0.2292 - val_R_squared: 0.5923\n",
      "Epoch 582/600\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.2131 - R_squared: 0.6452 - val_loss: 0.3248 - val_R_squared: 0.5380\n",
      "Epoch 583/600\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.2102 - R_squared: 0.6469 - val_loss: 0.2567 - val_R_squared: 0.5891\n",
      "Epoch 584/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.2128 - R_squared: 0.6436 - val_loss: 0.2325 - val_R_squared: 0.5690\n",
      "Epoch 585/600\n",
      "10/10 [==============================] - 2s 169ms/step - loss: 0.2057 - R_squared: 0.6524 - val_loss: 0.2998 - val_R_squared: 0.5752\n",
      "Epoch 586/600\n",
      "10/10 [==============================] - 1s 135ms/step - loss: 0.2137 - R_squared: 0.6473 - val_loss: 0.2195 - val_R_squared: 0.5903\n",
      "Epoch 587/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.1904 - R_squared: 0.6701 - val_loss: 0.2295 - val_R_squared: 0.6210\n",
      "Epoch 588/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2070 - R_squared: 0.6530 - val_loss: 0.2287 - val_R_squared: 0.5986\n",
      "Epoch 589/600\n",
      "10/10 [==============================] - 1s 132ms/step - loss: 0.2120 - R_squared: 0.6461 - val_loss: 0.2314 - val_R_squared: 0.6066\n",
      "Epoch 590/600\n",
      "10/10 [==============================] - 2s 185ms/step - loss: 0.2094 - R_squared: 0.6491 - val_loss: 0.3399 - val_R_squared: 0.5014\n",
      "Epoch 591/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2073 - R_squared: 0.6577 - val_loss: 0.2199 - val_R_squared: 0.6417\n",
      "Epoch 592/600\n",
      "10/10 [==============================] - 2s 160ms/step - loss: 0.2113 - R_squared: 0.6537 - val_loss: 0.2165 - val_R_squared: 0.6232\n",
      "Epoch 593/600\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.1973 - R_squared: 0.6598 - val_loss: 0.3407 - val_R_squared: 0.4962\n",
      "Epoch 594/600\n",
      "10/10 [==============================] - 1s 140ms/step - loss: 0.1970 - R_squared: 0.6655 - val_loss: 0.2727 - val_R_squared: 0.5336\n",
      "Epoch 595/600\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 0.2060 - R_squared: 0.6580 - val_loss: 0.2218 - val_R_squared: 0.5854\n",
      "Epoch 596/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2077 - R_squared: 0.6568 - val_loss: 0.2465 - val_R_squared: 0.6221\n",
      "Epoch 597/600\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.2053 - R_squared: 0.6585 - val_loss: 0.2205 - val_R_squared: 0.5532\n",
      "Epoch 598/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2062 - R_squared: 0.6525 - val_loss: 0.2076 - val_R_squared: 0.6040\n",
      "Epoch 599/600\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.2106 - R_squared: 0.6446 - val_loss: 0.2705 - val_R_squared: 0.5303\n",
      "Epoch 600/600\n",
      "10/10 [==============================] - 1s 149ms/step - loss: 0.2074 - R_squared: 0.6542 - val_loss: 0.2454 - val_R_squared: 0.5870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29ef43e20>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr = 0.001\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(0.01, decay_steps=400, decay_rate=0.9, staircase=False)\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "model.compile(optimizer, loss='mse', metrics=[R_squared])\n",
    "\n",
    "model.fit(train_dataset, epochs=600, validation_data=test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tab_net_regressor_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "tab_net_7 (TabNet)           multiple                  83440     \n",
      "_________________________________________________________________\n",
      "regressor (Dense)            multiple                  10        \n",
      "=================================================================\n",
      "Total params: 83,450\n",
      "Trainable params: 83,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"tab_net_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_features_5 (DenseFeatu multiple                  0         \n",
      "_________________________________________________________________\n",
      "input_gn (GroupNormalization multiple                  48        \n",
      "_________________________________________________________________\n",
      "transform_block_83 (Transfor multiple                  3328      \n",
      "_________________________________________________________________\n",
      "transform_block_84 (Transfor multiple                  8448      \n",
      "_________________________________________________________________\n",
      "transform_block_85 (Transfor multiple                  8448      \n",
      "_________________________________________________________________\n",
      "transform_block_86 (Transfor multiple                  8448      \n",
      "_________________________________________________________________\n",
      "transform_block_87 (Transfor multiple                  8448      \n",
      "_________________________________________________________________\n",
      "transform_block_88 (Transfor multiple                  8448      \n",
      "_________________________________________________________________\n",
      "transform_block_89 (Transfor multiple                  8448      \n",
      "_________________________________________________________________\n",
      "transform_block_90 (Transfor multiple                  8448      \n",
      "_________________________________________________________________\n",
      "transform_block_91 (Transfor multiple                  8448      \n",
      "_________________________________________________________________\n",
      "transform_block_92 (Transfor multiple                  8448      \n",
      "_________________________________________________________________\n",
      "transform_block_93 (Transfor multiple                  1344      \n",
      "_________________________________________________________________\n",
      "transform_block_94 (Transfor multiple                  1344      \n",
      "_________________________________________________________________\n",
      "transform_block_95 (Transfor multiple                  1344      \n",
      "=================================================================\n",
      "Total params: 83,440\n",
      "Trainable params: 83,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TabNet]: 34 features will be used for decision steps.\n"
     ]
    }
   ],
   "source": [
    "model = tabnet.TabNet(feature_columns, \n",
    "                                output_dim=30,\n",
    "                                num_decision_steps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(listings.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tabnet.modeles'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/97/j215pw6x7sq158bvx1ktlhf80000gn/T/ipykernel_12375/1403850569.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtabnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTabNetClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tabnet.modeles'"
     ]
    }
   ],
   "source": [
    "from tabnet.modeles.classify import TabNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_addons'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/97/j215pw6x7sq158bvx1ktlhf80000gn/T/ipykernel_12375/451453972.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtabnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTabNetClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tabnet/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtabnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTabNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtabnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTabNetClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tabnet/models/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m from tabnet.models.transformers import (\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mFeatureTransformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mAttentiveTransformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.9/site-packages/tabnet/models/transformers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparsemax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtabnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_addons'"
     ]
    }
   ],
   "source": [
    "from tabnet.models.classify import TabNetClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tabnet' has no attribute 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/97/j215pw6x7sq158bvx1ktlhf80000gn/T/ipykernel_12375/3875963320.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtabnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tabnet' has no attribute 'models'"
     ]
    }
   ],
   "source": [
    "tabnet.models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a055630542d55aea1ab1dec3e56eff509a7fc93a2c500b722ed821d49412af4a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('tensorflow_m1': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
