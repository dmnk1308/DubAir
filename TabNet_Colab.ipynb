{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "mount_file_id": "1ipU8uzOiP2IL-R69OvAhadzPoOI0fc2Y",
      "authorship_tag": "ABX9TyPxCr1Ao1PfmGaSFm/FHNK/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmnk1308/DubAir/blob/main/TabNet_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "n8_1RKI27Hms"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Optional, Union, Tuple\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "class GLUBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, units: Optional[int] = None,\n",
        "                 virtual_batch_size: Optional[int] = 128, \n",
        "                 momentum: Optional[float] = 0.02):\n",
        "        super(GLUBlock, self).__init__()\n",
        "        self.units = units\n",
        "        self.virtual_batch_size = virtual_batch_size\n",
        "        self.momentum = momentum\n",
        "        \n",
        "    def build(self, input_shape: tf.TensorShape):\n",
        "        if self.units is None:\n",
        "            self.units = input_shape[-1]\n",
        "            \n",
        "        self.fc_outout = tf.keras.layers.Dense(self.units, \n",
        "                                               use_bias=False)\n",
        "        self.bn_outout = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n",
        "                                                            momentum=self.momentum)\n",
        "        \n",
        "        self.fc_gate = tf.keras.layers.Dense(self.units, \n",
        "                                             use_bias=False)\n",
        "        self.bn_gate = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n",
        "                                                          momentum=self.momentum)\n",
        "        \n",
        "    def call(self, inputs: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None):\n",
        "        output = self.bn_outout(self.fc_outout(inputs), \n",
        "                                training=training)\n",
        "        gate = self.bn_gate(self.fc_gate(inputs), \n",
        "                            training=training)\n",
        "    \n",
        "        return output * tf.keras.activations.sigmoid(gate) # GLU\n",
        "\n",
        "class FeatureTransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, units: Optional[int] = None, virtual_batch_size: Optional[int]=128, \n",
        "                 momentum: Optional[float] = 0.02, skip=False):\n",
        "        super(FeatureTransformerBlock, self).__init__()\n",
        "        self.units = units\n",
        "        self.virtual_batch_size = virtual_batch_size\n",
        "        self.momentum = momentum\n",
        "        self.skip = skip\n",
        "        \n",
        "    def build(self, input_shape: tf.TensorShape):\n",
        "        if self.units is None:\n",
        "            self.units = input_shape[-1]\n",
        "        \n",
        "        self.initial = GLUBlock(units = self.units, \n",
        "                                virtual_batch_size=self.virtual_batch_size, \n",
        "                                momentum=self.momentum)\n",
        "        self.residual =  GLUBlock(units = self.units, \n",
        "                                  virtual_batch_size=self.virtual_batch_size, \n",
        "                                  momentum=self.momentum)\n",
        "        \n",
        "    def call(self, inputs: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None):\n",
        "        initial = self.initial(inputs, training=training)\n",
        "        \n",
        "        if self.skip == True:\n",
        "            initial += inputs\n",
        "\n",
        "        residual = self.residual(initial, training=training) # skip\n",
        "        \n",
        "        return (initial + residual) * np.sqrt(0.5)\n",
        "\n",
        "class AttentiveTransformer(tf.keras.layers.Layer):\n",
        "    def __init__(self, units: Optional[int] = None, virtual_batch_size: Optional[int] = 128, \n",
        "                 momentum: Optional[float] = 0.02):\n",
        "        super(AttentiveTransformer, self).__init__()\n",
        "        self.units = units\n",
        "        self.virtual_batch_size = virtual_batch_size\n",
        "        self.momentum = momentum\n",
        "        \n",
        "    def build(self, input_shape: tf.TensorShape):\n",
        "        if self.units is None:\n",
        "            self.units = input_shape[-1]\n",
        "            \n",
        "        self.fc = tf.keras.layers.Dense(self.units, \n",
        "                                        use_bias=False)\n",
        "        self.bn = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n",
        "                                                     momentum=self.momentum)\n",
        "        \n",
        "    def call(self, inputs: Union[tf.Tensor, np.ndarray], priors: Optional[Union[tf.Tensor, np.ndarray]] = None, training: Optional[bool] = None) -> tf.Tensor:\n",
        "        feature = self.bn(self.fc(inputs), \n",
        "                          training=training)\n",
        "        if priors is None:\n",
        "            output = feature\n",
        "        else:\n",
        "            output = feature * priors\n",
        "        \n",
        "        return tfa.activations.sparsemax(output)\n",
        "\n",
        "class TabNetStep(tf.keras.layers.Layer):\n",
        "    def __init__(self, units: Optional[int] = None, virtual_batch_size: Optional[int]=128, \n",
        "                 momentum: Optional[float] =0.02):\n",
        "        super(TabNetStep, self).__init__()\n",
        "        self.units = units\n",
        "        self.virtual_batch_size = virtual_batch_size\n",
        "        self.momentum = momentum\n",
        "        \n",
        "    def build(self, input_shape: tf.TensorShape):\n",
        "        if self.units is None:\n",
        "            self.units = input_shape[-1]\n",
        "        \n",
        "        self.unique = FeatureTransformerBlock(units = self.units, \n",
        "                                              virtual_batch_size=self.virtual_batch_size, \n",
        "                                              momentum=self.momentum,\n",
        "                                              skip=True)\n",
        "        self.attention = AttentiveTransformer(units = input_shape[-1], \n",
        "                                              virtual_batch_size=self.virtual_batch_size, \n",
        "                                              momentum=self.momentum)\n",
        "        \n",
        "    def call(self, inputs, shared, priors, training=None) -> Tuple[tf.Tensor]:  \n",
        "        split = self.unique(shared, training=training)\n",
        "        keys = self.attention(split, priors, training=training)\n",
        "        masked = keys * inputs\n",
        "        \n",
        "        return split, masked, keys\n",
        "\n",
        "class TabNetEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, units: int =1, \n",
        "                 n_steps: int = 3, \n",
        "                 n_features: int = 8,\n",
        "                 outputs: int = 1, \n",
        "                 gamma: float = 1.3,\n",
        "                 epsilon: float = 1e-8, \n",
        "                 sparsity: float = 1e-5, \n",
        "                 virtual_batch_size: Optional[int]=128, \n",
        "                 momentum: Optional[float] =0.02):\n",
        "        super(TabNetEncoder, self).__init__()\n",
        "        \n",
        "        self.units = units\n",
        "        self.n_steps = n_steps\n",
        "        self.n_features = n_features\n",
        "        self.virtual_batch_size = virtual_batch_size\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.momentum = momentum\n",
        "        self.sparsity = sparsity\n",
        "        \n",
        "    def build(self, input_shape: tf.TensorShape):            \n",
        "        self.bn = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n",
        "                                                     momentum=self.momentum)\n",
        "        self.shared_block = FeatureTransformerBlock(units = self.n_features, \n",
        "                                                    virtual_batch_size=self.virtual_batch_size, \n",
        "                                                    momentum=self.momentum)        \n",
        "        self.initial_step = TabNetStep(units = self.n_features, \n",
        "                                       virtual_batch_size=self.virtual_batch_size, \n",
        "                                       momentum=self.momentum)\n",
        "        self.steps = [TabNetStep(units = self.n_features, \n",
        "                                 virtual_batch_size=self.virtual_batch_size, \n",
        "                                 momentum=self.momentum) for _ in range(self.n_steps)]\n",
        "        self.final = tf.keras.layers.Dense(units = self.units, \n",
        "                                           use_bias=False)\n",
        "    \n",
        "\n",
        "    def call(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> Tuple[tf.Tensor]:        \n",
        "        entropy_loss = 0.\n",
        "        encoded = 0.\n",
        "        output = 0.\n",
        "        importance = 0.\n",
        "        prior = tf.reduce_mean(tf.ones_like(X), axis=0)\n",
        "        \n",
        "        B = prior * self.bn(X, training=training)\n",
        "        shared = self.shared_block(B, training=training)\n",
        "        _, masked, keys = self.initial_step(B, shared, prior, training=training)\n",
        "\n",
        "        for step in self.steps:\n",
        "            entropy_loss += tf.reduce_mean(tf.reduce_sum(-keys * tf.math.log(keys + self.epsilon), axis=-1)) / tf.cast(self.n_steps, tf.float32)\n",
        "            prior *= (self.gamma - tf.reduce_mean(keys, axis=0))\n",
        "            importance += keys\n",
        "            \n",
        "            shared = self.shared_block(masked, training=training)\n",
        "            split, masked, keys = step(B, shared, prior, training=training)\n",
        "            features = tf.keras.activations.relu(split)\n",
        "            \n",
        "            output += features\n",
        "            encoded += split\n",
        "            \n",
        "        self.add_loss(self.sparsity * entropy_loss)\n",
        "          \n",
        "        prediction = self.final(output)\n",
        "        return prediction, encoded, importance\n",
        "\n",
        "class TabNetRegression(tf.keras.Model):\n",
        "    def __init__(self, outputs: int = 1, \n",
        "                 n_steps: int = 3, \n",
        "                 n_features: int = 8,\n",
        "                 gamma: float = 1.3, \n",
        "                 epsilon: float = 1e-8, \n",
        "                 sparsity: float = 1e-5, \n",
        "                 feature_column: Optional[tf.keras.layers.DenseFeatures] = None, \n",
        "                 pretrained_encoder: Optional[tf.keras.layers.Layer] = None,\n",
        "                 virtual_batch_size: Optional[int] = 128, \n",
        "                 momentum: Optional[float] = 0.02):\n",
        "        super(TabNetRegression, self).__init__()\n",
        "        \n",
        "        self.outputs = outputs\n",
        "        self.n_steps = n_steps\n",
        "        self.n_features = n_features\n",
        "        self.feature_column = feature_column\n",
        "        self.pretrained_encoder = pretrained_encoder\n",
        "        self.virtual_batch_size = virtual_batch_size\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.momentum = momentum\n",
        "        self.sparsity = sparsity\n",
        "        \n",
        "        if feature_column is None:\n",
        "            self.feature = tf.keras.layers.Lambda(identity)\n",
        "        else:\n",
        "            self.feature = feature_column\n",
        "            \n",
        "        if pretrained_encoder is None:\n",
        "            self.encoder = TabNetEncoder(units=outputs, \n",
        "                                        n_steps=n_steps, \n",
        "                                        n_features = n_features,\n",
        "                                        outputs=outputs, \n",
        "                                        gamma=gamma, \n",
        "                                        epsilon=epsilon, \n",
        "                                        sparsity=sparsity,\n",
        "                                        virtual_batch_size=self.virtual_batch_size, \n",
        "                                        momentum=momentum)\n",
        "        else:\n",
        "            self.encoder = pretrained_encoder\n",
        "\n",
        "    def forward(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> Tuple[tf.Tensor]:\n",
        "        X = self.feature(X)\n",
        "        output, encoded, importance = self.encoder(X)\n",
        "          \n",
        "        prediction = output\n",
        "        return prediction, encoded, importance\n",
        "    \n",
        "    def call(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
        "        prediction, _, _ = self.forward(X)\n",
        "        return prediction\n",
        "    \n",
        "    def transform(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
        "        _, encoded, _ = self.forward(X)\n",
        "        return encoded\n",
        "    \n",
        "    def explain(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
        "        _, _, importance = self.forward(X)\n",
        "        return importance    \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(listings.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA8F5TFv_joF",
        "outputId": "c1c3c055-2689-455a-a8c6-3a59739fa505"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "listings = pd.read_csv(\"/content/drive/MyDrive/Dubair/colab.csv\")\n",
        "listings.drop(listings.columns[0], axis = 1, inplace=True)\n",
        "price = listings[\"log_price\"]\n",
        "listings = listings.drop(\"log_price\", axis = 1)\n",
        "\n",
        "#X_train, X_test, y_train, y_test = train_test_split(listings, price[\"log_price\"], random_state = 123, test_size = 0.2)\n",
        "model_score = []\n",
        "\n",
        "listings.rename(columns = {\"property_type_Private room in residential home\": \"property_type_private_room_residential_home\",\"property_type_Entire rental unit\": \"property_type_entire_rental_units\"}, inplace = True)\n",
        "listings[\"property_type_private_room_residential_home\"] = listings[\"property_type_private_room_residential_home\"].values.astype(np.int8)\n",
        "listings[\"property_type_entire_rental_units\"] = listings[\"property_type_entire_rental_units\"].values.astype(np.int8)\n",
        "\n",
        "def R_squared(y, y_pred):\n",
        "  residual = tf.reduce_sum(tf.square(tf.subtract(y, y_pred)))\n",
        "  total = tf.reduce_sum(tf.square(tf.subtract(y, tf.reduce_mean(y))))\n",
        "  r2 = tf.subtract(1.0, tf.math.divide(residual, total))\n",
        "  return r2\n",
        "\n",
        "\n",
        "train_size = 6000\n",
        "batch_size = 600\n",
        "\n",
        "def transform(ds):\n",
        "    features = tf.unstack(ds[\"features\"])\n",
        "    prices = ds[\"price\"]\n",
        "\n",
        "    x = dict(zip(col_names, features))\n",
        "    y = prices\n",
        "    return x, y\n",
        "\n",
        "bin_col = [col for col in listings if np.isin(listings[col].unique(), [0, 1]).all()]\n",
        "num_col = [col for col in listings if ~np.isin(listings[col].unique(), [0, 1]).all()]\n",
        "col_names = bin_col + num_col\n",
        "\n",
        "data = tf.data.Dataset.from_tensor_slices({\"features\": listings, \"price\": price})\n",
        "\n",
        "\n",
        "data = data.shuffle(6000, seed = 13)\n",
        "train_dataset = data.take(train_size)\n",
        "train_dataset = train_dataset.map(transform)\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "test_dataset = data.take(len(listings)-train_size)\n",
        "test_dataset = test_dataset.map(transform)\n",
        "test_dataset = test_dataset.batch(batch_size)\n",
        "\n",
        "feature_columns = []\n",
        "\n",
        "for col in col_names:\n",
        "    feature_columns.append(tf.feature_column.numeric_column(col))\n",
        "\n",
        "feature_column = tf.keras.layers.DenseFeatures(feature_columns, trainable = True)\n",
        "\n",
        "model = TabNetRegression(n_features = 24, feature_column =feature_column, virtual_batch_size=600)\n",
        "\n",
        "\n",
        "lr = tf.keras.optimizers.schedules.ExponentialDecay(0.01, decay_steps=400, decay_rate=0.9, staircase=False)\n",
        "optimizer = tf.keras.optimizers.Adam(lr)\n",
        "model.compile(optimizer, loss='mse', metrics=[R_squared])\n",
        "\n",
        "model.fit(train_dataset, epochs=600, validation_data=test_dataset, verbose=1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uBDN3dyp7S0M",
        "outputId": "f0537bac-1b71-4f2f-9017-e3c383142f83"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/600\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tab_net_regression_3/tab_net_encoder_3/tab_net_step_3/attentive_transformer_3/dense_24/kernel:0', 'tab_net_regression_3/tab_net_encoder_3/tab_net_step_3/attentive_transformer_3/batch_normalization_24/gamma:0', 'tab_net_regression_3/tab_net_encoder_3/tab_net_step_3/attentive_transformer_3/batch_normalization_24/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tab_net_regression_3/tab_net_encoder_3/tab_net_step_3/attentive_transformer_3/dense_24/kernel:0', 'tab_net_regression_3/tab_net_encoder_3/tab_net_step_3/attentive_transformer_3/batch_normalization_24/gamma:0', 'tab_net_regression_3/tab_net_encoder_3/tab_net_step_3/attentive_transformer_3/batch_normalization_24/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "10/10 [==============================] - ETA: 0s - loss: 18.9622 - R_squared: -30.0886"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-329da3e73322>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mR_squared\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:  Input to reshape is a tensor with 5760 values, but the requested shape requires a multiple of 14400\n\t [[node tab_net_regression_3/tab_net_encoder_3/batch_normalization/Reshape\n (defined at /usr/local/lib/python3.7/dist-packages/keras/layers/normalization/batch_normalization.py:760)\n]] [Op:__inference_test_function_70034]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node tab_net_regression_3/tab_net_encoder_3/batch_normalization/Reshape:\nIn[0] tab_net_regression_3/dense_features_3/concat (defined at /usr/local/lib/python3.7/dist-packages/keras/feature_column/base_feature_layer.py:117)\t\nIn[1] tab_net_regression_3/tab_net_encoder_3/batch_normalization/concat_1 (defined at /usr/local/lib/python3.7/dist-packages/keras/layers/normalization/batch_normalization.py:757)\n\nOperation defined at: (most recent call last)\n>>>   File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n>>>     \"__main__\", mod_spec)\n>>> \n>>>   File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n>>>     handler_func(fileobj, events)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 452, in _handle_events\n>>>     self._handle_recv()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 481, in _handle_recv\n>>>     self._run_callback(callback, msg)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 431, in _run_callback\n>>>     callback(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n>>>     return self.dispatch_shell(stream, msg)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n>>>     handler(stream, idents, msg)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n>>>     user_expressions, allow_stdin)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n>>>     interactivity=interactivity, compiler=compiler, result=result)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2828, in run_ast_nodes\n>>>     if self.run_code(code, result):\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"<ipython-input-12-329da3e73322>\", line 61, in <module>\n>>>     model.fit(train_dataset, epochs=600, validation_data=test_dataset, verbose=1)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1263, in fit\n>>>     _use_cached_eval_dataset=True)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1537, in evaluate\n>>>     tmp_logs = self.test_function(iterator)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1366, in test_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1356, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1349, in run_step\n>>>     outputs = model.test_step(data)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1303, in test_step\n>>>     y_pred = self(x, training=False)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"<ipython-input-5-a72e9bf17789>\", line 237, in call\n>>>     prediction, _, _ = self.forward(X)\n>>> \n>>>   File \"<ipython-input-5-a72e9bf17789>\", line 231, in forward\n>>>     output, encoded, importance = self.encoder(X)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"<ipython-input-5-a72e9bf17789>\", line 166, in call\n>>>     B = prior * self.bn(X, training=training)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/layers/normalization/batch_normalization.py\", line 760, in call\n>>>     inputs = tf.reshape(inputs, expanded_shape)\n>>> "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "58JSWie17nrj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}