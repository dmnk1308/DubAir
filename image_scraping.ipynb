{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import webbrowser\n",
    "import requests\n",
    "import pandas as pd\n",
    "import cv2 \n",
    "import urllib.request as urli\n",
    "import os \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import time \n",
    "\n",
    "\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_listing = \"http://data.insideairbnb.com/ireland/leinster/dublin/2021-11-07/data/listings.csv.gz\"\n",
    "listings = pd.read_csv(url_listing)\n",
    "prefix = \"https://airbnb.de\"\n",
    "suffix = \"/photos\"\n",
    "urls = listings[\"listing_url\"]\n",
    "ids = listings[\"id\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webbrowser.open(urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6976 [00:00<?, ?it/s]/var/folders/97/j215pw6x7sq158bvx1ktlhf80000gn/T/ipykernel_88378/3013026069.py:5: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(\"/Users/dmnk/Documents/Dubair_Trash/chromedriver 2\", options = options)\n",
      "/var/folders/97/j215pw6x7sq158bvx1ktlhf80000gn/T/ipykernel_88378/3013026069.py:9: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  body = driver.find_element_by_tag_name(\"body\")\n",
      " 23%|██▎       | 1626/6976 [2:19:50<9:10:03,  6.17s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ups - fail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 1644/6976 [2:21:21<8:15:35,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ups - fail\n"
     ]
    }
   ],
   "source": [
    "id_dict = dict()\n",
    "for id, url in tqdm(zip(ids, urls), total = len(ids)):    \n",
    "    id_list = []\n",
    "    try:\n",
    "        driver = webdriver.Chrome(\"/Users/dmnk/Documents/Dubair_Trash/chromedriver 2\", options = options)\n",
    "        \n",
    "        driver.get(url+suffix)\n",
    "\n",
    "        body = driver.find_element_by_tag_name(\"body\")\n",
    "        for i in range(40):\n",
    "            body.send_keys(Keys.TAB)\n",
    "        \n",
    "        time.sleep(1)\n",
    "\n",
    "        for i in range(40):\n",
    "            body.send_keys(Keys.TAB)            \n",
    "\n",
    "        html = driver.page_source\n",
    "        html_content = BeautifulSoup(html, \"html.parser\")\n",
    "        gallery_items = html_content.findAll(\"img\", {\"class\": \"_6tbg2q\"})\n",
    "    except:\n",
    "        continue\n",
    "    for i, item in enumerate(gallery_items[5:]):\n",
    "        try:\n",
    "            img_link = item.get(\"src\")\n",
    "            img_title = item.get(\"alt\")\n",
    "            path_name = \"/Volumes/TOSHIBA EXT/images_new/\"+str(id)+\"_\"+str(i)+\".png\"\n",
    "            if os.path.exists(path_name):\n",
    "                id_list.append(img_title)\n",
    "                continue\n",
    "            else:\n",
    "                urli.urlretrieve(img_link, path_name)\n",
    "                id_list.append(img_title)\n",
    "            \n",
    "        except:\n",
    "            print(\"ups - fail\")\n",
    "            continue\n",
    "\n",
    "    id_dict[id] = id_list\n",
    "meta_images = open(\"/Volumes/TOSHIBA EXT/images/data.pkl\", \"wb\")\n",
    "pickle.dump(id_dict, meta_images)\n",
    "meta_images.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('unable to open database file')).History will not be written to the database.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device: 'labels_raw.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels_raw.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m      4\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(id_dict, fp)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device: 'labels_raw.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('labels_raw.json', 'w') as fp:\n",
    "    json.dump(id_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resize images to same shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_path = \"data/images_resized/\"\n",
    "\n",
    "dir = os.listdir(\"images\")\n",
    "\n",
    "img_names = pd.Series(dir).str.split(\".\").str[0].values\n",
    "\n",
    "for img_dir, resized_img_dir in tqdm(zip(dir, img_names), total = len(dir)):\n",
    "  img_dir_tmp = \"images/\"+ img_dir\n",
    "  img_tmp = cv2.imread(img_dir_tmp)\n",
    "  img_tmp = cv2.resize(img_tmp, dsize=(256, 256))\n",
    "  path_tmp = resized_path + resized_img_dir + \".png\"\n",
    "  cv2.imwrite(path_tmp, img_tmp)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
