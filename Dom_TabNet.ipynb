{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from load_data import *\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import tabnet\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns \n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(ds):\n",
    "    features = tf.unstack(ds[\"features\"])\n",
    "    prices = ds[\"price\"]\n",
    "\n",
    "    x = dict(zip(col_names, features))\n",
    "    y = prices\n",
    "    return x, y\n",
    "\n",
    "def R_squared(y, y_pred):\n",
    "  residual = tf.reduce_sum(tf.square(tf.subtract(y, y_pred)))\n",
    "  total = tf.reduce_sum(tf.square(tf.subtract(y, tf.reduce_mean(y))))\n",
    "  r2 = tf.subtract(1.0, tf.math.divide(residual, total))\n",
    "  \n",
    "  return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Fit and Transform data...\n",
      "------------------------------\n",
      "58 amenities have been removed due to close zero-variance.\n",
      "Text, OpenStreet and image data loaded.\n",
      "44 binary variables have been removed due to close zero-variance.\n",
      "Imputation done. No NaN's are left in the data.\n",
      "PCA's built and correlated features dropped.\n",
      "Due to insignificant t-tests we drop:\n",
      "['host_is_superhost', 'Extra pillows and blankets', 'Luggage dropoff allowed', 'Free_parking', 'host_name_sounds_west', 'host_name_sounds_rare', 'neighbourhood_cleansed_Dn Laoghaire-Rathdown']\n",
      "------------------------------\n",
      "Transform data...\n",
      "------------------------------\n",
      "58 amenities have been removed due to close zero-variance.\n",
      "Text, OpenStreet and image data loaded.\n",
      "44 binary variables have been removed due to close zero-variance.\n",
      "Imputation done. No NaN's are left in the data.\n",
      "PCA's built and correlated features dropped.\n",
      "Due to insignificant t-tests we drop:\n",
      "['host_is_superhost', 'Extra pillows and blankets', 'Luggage dropoff allowed', 'Free_parking', 'host_name_sounds_west', 'host_name_sounds_rare', 'neighbourhood_cleansed_Dn Laoghaire-Rathdown']\n",
      "------------------------------\n",
      "Transform data...\n",
      "------------------------------\n",
      "58 amenities have been removed due to close zero-variance.\n",
      "Text, OpenStreet and image data loaded.\n",
      "44 binary variables have been removed due to close zero-variance.\n",
      "Imputation done. No NaN's are left in the data.\n",
      "PCA's built and correlated features dropped.\n",
      "Due to insignificant t-tests we drop:\n",
      "['host_is_superhost', 'Extra pillows and blankets', 'Luggage dropoff allowed', 'Free_parking', 'host_name_sounds_west', 'host_name_sounds_rare', 'neighbourhood_cleansed_Dn Laoghaire-Rathdown']\n",
      "[TabNet]: 4 features will be used for decision steps.\n",
      "WARNING:tensorflow:Layer tab_net_regressor_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "11/11 [==============================] - 8s 705ms/step - loss: 5.0924 - R_squared: -10.2388 - mse: 5.0924 - mae: 1.5760 - val_loss: 0.5842 - val_R_squared: -0.5619 - val_mse: 0.5842 - val_mae: 0.5961\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, X_val, y_train, y_test, y_val = load_data(for_dendro = False)\n",
    "\n",
    "bin_col = [col for col in X_train if np.isin(X_train[col].unique(), [0, 1]).all()]\n",
    "num_col = [col for col in X_train if ~np.isin(X_train[col].unique(), [0, 1]).all()]\n",
    "col_names = bin_col + num_col\n",
    "\n",
    "train_size = int(X_train.shape[0] * 0.9)\n",
    "batch_size = int(X_train.shape[0] * 0.1)\n",
    "\n",
    "data_train = tf.data.Dataset.from_tensor_slices({\"features\": X_train, \"price\": y_train})\n",
    "data_train = data_train.shuffle(6000, seed = 13)\n",
    "train_dataset = data_train.take(len(X_train))\n",
    "train_dataset = train_dataset.map(transform)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "data_test = tf.data.Dataset.from_tensor_slices({\"features\": X_val, \"price\": y_val})\n",
    "test_dataset = data_test.take(len(X_val))\n",
    "test_dataset = test_dataset.map(transform)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "\n",
    "feature_columns = []\n",
    "\n",
    "for col in col_names:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(col))\n",
    "\n",
    "\n",
    "od = 110\n",
    "fd = 114\n",
    "nds = 2\n",
    "rf = 2.5\n",
    "epo = 1\n",
    "\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(0.01, decay_steps=100, decay_rate=0.95, staircase=False)\n",
    "#lr = 0.01\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "model = tabnet.TabNetRegression(feature_columns, num_regressors=1,\n",
    "                                output_dim=od, feature_dim=fd, num_groups=1,\n",
    "                                num_decision_steps=nds, relaxation_factor=rf)\n",
    "\n",
    "model.compile(optimizer, loss=['mse', \"mae\"] , metrics=[R_squared, \"mse\", \"mae\"])\n",
    "\n",
    "hist_model = model.fit(train_dataset, epochs=epo, \n",
    "                        validation_data=test_dataset, verbose=1)\n",
    "\n",
    "model.load_weights(\"TabNet_GS/best_model5.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving mask 1 of shape (1, 477, 66, 1)\n",
      "Saving aggregate mask of shape (1, 477, 66, 1)\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(train_dataset))\n",
    "_ = model(x)\n",
    "\n",
    "writer = tf.summary.create_file_writer(\"TabNet_GS_logs\")\n",
    "with writer.as_default():\n",
    "    for i, mask in enumerate(model.tabnet.feature_selection_masks):\n",
    "        print(\"Saving mask {} of shape {}\".format(i + 1, mask.shape))\n",
    "        tf.summary.image('mask_at_iter_{}'.format(i + 1), step=0, data=mask, max_outputs=1)\n",
    "        writer.flush()\n",
    "\n",
    "    agg_mask = model.tabnet.aggregate_feature_selection_mask\n",
    "    print(\"Saving aggregate mask of shape\", agg_mask.shape)\n",
    "    tf.summary.image(\"Aggregate Mask\", step=0, data=agg_mask, max_outputs=1)\n",
    "    writer.flush()\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "80af794f376859ceeba8c9f1eafc6102bfce8b61902df3bf9ce86f8d60c30a70"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('DL_tabnet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
